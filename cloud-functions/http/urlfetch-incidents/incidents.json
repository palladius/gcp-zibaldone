[{"begin": "2019-12-04T03:02:07Z", "created": "2019-12-04T03:02:09Z", "end": "2019-12-04T03:23:38Z", "external_desc": "We are investigating an issue with network connectivity in asia-southest1", "modified": "2019-12-04T03:23:38Z", "most-recent-update": {"created": "2019-12-04T03:23:38Z", "modified": "2019-12-04T03:23:38Z", "text": "The network connectivity issue with Google Compute Engine has been resolved for all affected projects as of Tuesday, 2019-12-03 18:24 US/Pacific.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-12-04T03:23:38Z"}, "number": 19011, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-12-04T03:23:38Z", "modified": "2019-12-04T03:23:38Z", "text": "The network connectivity issue with Google Compute Engine has been resolved for all affected projects as of Tuesday, 2019-12-03 18:24 US/Pacific.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-12-04T03:23:38Z"}, {"created": "2019-12-04T03:02:09Z", "modified": "2019-12-04T03:02:09Z", "text": "Description: We are experiencing an increased packet loss in and out of asia-southeast1, beginning at Tuesday, 2019-12-03 17:37 US/Pacific.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Tuesday, 2019-12-03 20:00 US/Pacific with current details.\n\n\nDiagnosis: The customer affected by this issue may see increased network packet loss.\n\nWorkaround: None at this time", "when": "2019-12-04T03:02:09Z"}], "uri": "/incident/compute/19011"}, {"begin": "2019-11-11T11:52:53Z", "created": "2019-11-11T12:15:10Z", "end": "2019-11-11T12:49:07Z", "external_desc": "We've received a report of an issue with Stackdriver Monitoring.", "modified": "2019-11-11T12:49:08Z", "most-recent-update": {"created": "2019-11-11T12:49:07Z", "modified": "2019-11-11T12:49:07Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:48 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:49:07Z"}, "number": 19008, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-11-11T12:49:07Z", "modified": "2019-11-11T12:49:07Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:48 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:49:07Z"}, {"created": "2019-11-11T12:15:11Z", "modified": "2019-11-11T12:15:11Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:15:11Z"}], "uri": "/incident/google-stackdriver/19008"}, {"begin": "2019-11-11T11:38:18Z", "created": "2019-11-11T12:13:09Z", "end": "2019-11-11T12:39:40Z", "external_desc": "We've received a report of an issue with Cloud Bigtable.", "modified": "2019-11-11T12:39:40Z", "most-recent-update": {"created": "2019-11-11T12:39:40Z", "modified": "2019-11-11T12:39:40Z", "text": "The issue with Cloud Bigtable has been resolved for all affected users as of Monday, 2019-11-11 04:39 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:39:40Z"}, "number": 19002, "public": true, "service_key": "cloud-bigtable", "service_name": "Google Cloud Bigtable", "severity": "medium", "updates": [{"created": "2019-11-11T12:39:40Z", "modified": "2019-11-11T12:39:40Z", "text": "The issue with Cloud Bigtable has been resolved for all affected users as of Monday, 2019-11-11 04:39 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:39:40Z"}, {"created": "2019-11-11T12:13:10Z", "modified": "2019-11-11T12:13:10Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:13:10Z"}], "uri": "/incident/cloud-bigtable/19002"}, {"begin": "2019-11-11T11:33:12Z", "created": "2019-11-11T12:06:19Z", "end": "2019-11-11T12:23:47Z", "external_desc": "We've received a report of an issue with Google Cloud Functions.", "modified": "2019-11-11T12:23:47Z", "most-recent-update": {"created": "2019-11-11T12:23:47Z", "modified": "2019-11-11T12:23:47Z", "text": "The issue with Google Cloud Functions has been resolved for all affected users as of Monday, 2019-11-11 04:23 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:23:47Z"}, "number": 19009, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-11-11T12:23:47Z", "modified": "2019-11-11T12:23:47Z", "text": "The issue with Google Cloud Functions has been resolved for all affected users as of Monday, 2019-11-11 04:23 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:23:47Z"}, {"created": "2019-11-11T12:06:20Z", "modified": "2019-11-11T12:06:20Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:06:20Z"}], "uri": "/incident/cloud-functions/19009"}, {"begin": "2019-11-11T11:23:55Z", "created": "2019-11-11T12:06:46Z", "end": "2019-11-11T12:27:17Z", "external_desc": "We've received a report of an issue with Cloud Memorystore.", "modified": "2019-11-11T12:27:17Z", "most-recent-update": {"created": "2019-11-11T12:27:17Z", "modified": "2019-11-11T12:27:17Z", "text": "The issue with Cloud Memorystore has been resolved for all affected users as of Monday, 2019-11-11 04:25 US/Pacific.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:27:17Z"}, "number": 19003, "public": true, "service_key": "cloud-memorystore", "service_name": "Cloud Memorystore", "severity": "medium", "updates": [{"created": "2019-11-11T12:27:17Z", "modified": "2019-11-11T12:27:17Z", "text": "The issue with Cloud Memorystore has been resolved for all affected users as of Monday, 2019-11-11 04:25 US/Pacific.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:27:17Z"}, {"created": "2019-11-11T12:06:47Z", "modified": "2019-11-11T12:06:47Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:06:47Z"}], "uri": "/incident/cloud-memorystore/19003"}, {"begin": "2019-11-11T11:15:16Z", "created": "2019-11-11T11:15:35Z", "end": "2019-11-11T13:36:34Z", "external_desc": "We've received a report of an issue with Cloud Firestore.", "modified": "2019-11-15T20:13:37Z", "most-recent-update": {"created": "2019-11-15T20:09:32Z", "modified": "2019-11-15T20:13:37Z", "text": "# ISSUE SUMMARY\r\nOn Monday 11 November, 2019, Google's internal key management system (KMS), suffered a failure which began to cause user facing impact in the us-east1, us-east4, and southamerica-east1 regions at 02:39 US/Pacific and recovered by 03:27. Some services took longer to fully recover and continued to experience issues outside of this period. Google's commitment to user privacy and data security means that KMS is a common dependency across many infrastructure components. Specific service impact is outlined below.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Monday 11 November, 2019 from 02:39 to 03:27 US/Pacific various services in us-east1, us-east4, southamerica-east1 experienced varying degrees of impact as detailed below. Some global services were also impacted during this period.\r\n\r\n##Cloud IAM\r\n\r\nCloud IAM saw an average error rate of 93.1% and 89.3% in us-east4 and southamerica-east1 respectively for the first 36 minutes of the incident. Cloud IAM in us-east1 saw an average error rate of 94.5% for the duration of the event.\r\n\r\n##Cloud Pub/Sub\r\n\r\nCloud Pub/Sub saw an average error rate of 15% for Publish operations during the impact period.\r\n\r\n##Google Compute Engine (GCE)\r\n\r\nGoogle Compute Engine API requests saw a 16% error rate during the impact period. Persistent Disk control plane operations: Device Creation, Grow, Deletion, Attachment, Snapshot upload/download, and Image Restores failed in the us-east1, us-east4, and southamerica-east1 regions. Existing instances were unaffected.\r\n\r\n##Google App Engine (GAE)\r\n\r\nGoogle App Engine in us-east1 served an average elevated error rate of 10% (peaking to 15%). Applications in us-east4 served an average of 10% errors for the first 35 minutes of the incident and applications in southamerica-east1 served an average of 3% errors for the first 35 minutes of the incident.\r\n\r\n##Cloud Functions\r\n\r\nCloud Functions HTTP triggering was degraded in us-east1, us-east4 and southamerica-east1 with peak error rates of 1%, 1.7%, and 8.5% respectively.\r\n\r\n##Cloud Run\r\n\r\nCloud Run experienced elevated error rates averaging 11% in us-east1 for the first 35 minutes of the incident. In all cases, the majority of serverless errors were due to permission checks--for example--if a user were an admin, or if a user were authorized to access a restricted endpoint. Errors were returned in the form of HTTP 5XX responses. The Serverless API served an average of 70% errors during the impact period due to requiring IAM permission checks.\r\n\r\n##Cloud Bigtable\r\n\r\nCloud Bigtable instances in us-east1, us-east4, and southamerica-east1 were inaccessible for the first 18 minutes of the incident.\r\n\r\n##Cloud Memorystore\r\n\r\nCloud Memorystore Delete Instance, Get Instance, List, and List Instance operations were unavailable during the impact period. Existing instances were unaffected.\r\n\r\n##Cloud DNS\r\n\r\nCloud DNS API had an average error rate of 27% for the first 25 minutes of the incident. Domain resolution was unaffected.\r\n\r\n##Cloud Console\r\n\r\nThe Cloud Console served an average of 4.7% errors for the impact duration.\r\n\r\n##Google Cloud Storage (GCS)\r\n\r\nBuckets in us-east1 served an average of 99% errors for the duration of the incident, buckets in us-east4 and southamerica-east1 served an average of 97% and 96% errors respectively for the first 36 minutes of the incident. Buckets in the US multiregion served an average of 1.7% errors from 02:39 to 05:54 US/Pacific for a duration of 3 hours 15 minutes.\r\n\r\n##Google BigQuery\r\n\r\nBigQuery Dataset, Table, and Job operations were unavailable in us-east1, us-east4, and southamerica-east1 for the duration of the incident.\r\n\r\n##Google Cloud Load Balancing\r\n\r\nHealth checks to new instances or instances that were live-migrated during the incident failed in us-east1-c and us-east1-d. Instances that fail health checks will generally not receive any traffic. Existing instances were unaffected unless they were migrated. 4% of Managed Instance Groups (MIGs) in us-east1-b failed to repair itself, while up to 85% MIG instance creations failed. This meant that instances were created successfully, but their health checks timed out and the instances were unable to receive traffic. Autohealing and MIG health checks failed for up to 60% of requests in us-east1-d and up to 25% in us-east1-c. Up to 6% of Internal Load Balancer\u2019s (ILB) backend health checks failed in us-east1-d and up to 3.5% in us-east1-c. Google Cloud HTTP(S) Load Balancing backends saw up to 0.5% health checks failed globally.\r\n\r\n##Stackdriver Logging\r\n\r\nStackdriver Logging saw an average error rate of 11% globally, and up to a 98% error rate in us-east1, us-east4, and southamerica-east1.\r\n\r\n##Stackdriver Monitoring\r\n\r\nStackdriver Monitoring was unavailable in us-east, us-east4, and southamerica-east1 from 02:44 to 03:03 US/Pacific.\r\n\r\n##G Suite\r\n\r\nThe impact on G Suite users was different from and generally lower than the impact on Google Cloud Platform users due to differences in architecture and provisioning of these services. Please see the G Suite Status Dashboard (https://www.google.com/appsstatus) for details on affected G Suite services.\r\n\r\n\r\n# ROOT CAUSE AND REMEDIATION\r\nAt Google, data security is a critical part of service design. To accomplish this, services depend on the KMS for performing cryptographic functions such as encrypting and decrypting the keys used for protecting user data. On Monday 11 November, 2019 at 00:11 US/Pacific a new version of the KMS began to roll out, starting with a single zone in the us-east1 region. This binary added a new feature which was incompatible with older versions of related components. Six minutes later, an alert notified Google engineers of an increased error rate in the KMS. Additional alerts went off at 00:34 while investigation was ongoing. At this time there was no impact to Google Cloud users.\r\n\r\nIn order to begin recovery, Google engineers started to roll forward a change in the affected region, bringing all tasks up to the same version. A roll forward was necessary due to the backwards-incompatible nature of the newest version of the KMS, as persisted keys could not be processed by other components. The roll forward began at 02:32, and resulted in a temporary increase of internal errors from the KMS as the ratio of tasks with the new incompatible version and the previous version changed. By 02:39 the error rate reached a point where external users began to see service degradation. KMS is a low level service and is a critical dependency for core services such as IAM and Storage. The dependency on these core services created distinct start and end times for impact across higher-level services. \r\n\r\nThe roll forward completed by 02:59 and brought the KMS back to a healthy state. From 02:59 onward, services which depend on the KMS began to recover. Most services returned to a healthy state by 03:27, however, some services which integrate with the KMS in multiple locations, such as GCS, saw a longer tail of errors until other locations were brought back to a healthy state.\r\n\r\n# PREVENTION AND FOLLOW-UP\r\nA key reliability principle for GCP is that regions provide failure domain isolation; as a result, we will be modifying our processes to ensure that multiple regions are not impacted by a rollout of this particular component simultaneously. Projects are already in progress to split and isolate shared dependencies for the regions impacted by this incident. We are making changes to our canary and rollout validation processes to improve detection speed of similar incidents in future.\r\n\r\nAn additional principle is the ability to mitigate incidents quickly, typically through rollbacks, and we will be introducing explicit backwards-compatibility testing for the components implicated in this outage to ensure we comply with that principle in future.", "when": "2019-11-15T20:09:32Z"}, "number": 19006, "public": true, "service_key": "cloud-datastore", "service_name": "Google Cloud Datastore", "severity": "high", "updates": [{"created": "2019-11-15T20:09:32Z", "modified": "2019-11-15T20:13:37Z", "text": "# ISSUE SUMMARY\r\nOn Monday 11 November, 2019, Google's internal key management system (KMS), suffered a failure which began to cause user facing impact in the us-east1, us-east4, and southamerica-east1 regions at 02:39 US/Pacific and recovered by 03:27. Some services took longer to fully recover and continued to experience issues outside of this period. Google's commitment to user privacy and data security means that KMS is a common dependency across many infrastructure components. Specific service impact is outlined below.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Monday 11 November, 2019 from 02:39 to 03:27 US/Pacific various services in us-east1, us-east4, southamerica-east1 experienced varying degrees of impact as detailed below. Some global services were also impacted during this period.\r\n\r\n##Cloud IAM\r\n\r\nCloud IAM saw an average error rate of 93.1% and 89.3% in us-east4 and southamerica-east1 respectively for the first 36 minutes of the incident. Cloud IAM in us-east1 saw an average error rate of 94.5% for the duration of the event.\r\n\r\n##Cloud Pub/Sub\r\n\r\nCloud Pub/Sub saw an average error rate of 15% for Publish operations during the impact period.\r\n\r\n##Google Compute Engine (GCE)\r\n\r\nGoogle Compute Engine API requests saw a 16% error rate during the impact period. Persistent Disk control plane operations: Device Creation, Grow, Deletion, Attachment, Snapshot upload/download, and Image Restores failed in the us-east1, us-east4, and southamerica-east1 regions. Existing instances were unaffected.\r\n\r\n##Google App Engine (GAE)\r\n\r\nGoogle App Engine in us-east1 served an average elevated error rate of 10% (peaking to 15%). Applications in us-east4 served an average of 10% errors for the first 35 minutes of the incident and applications in southamerica-east1 served an average of 3% errors for the first 35 minutes of the incident.\r\n\r\n##Cloud Functions\r\n\r\nCloud Functions HTTP triggering was degraded in us-east1, us-east4 and southamerica-east1 with peak error rates of 1%, 1.7%, and 8.5% respectively.\r\n\r\n##Cloud Run\r\n\r\nCloud Run experienced elevated error rates averaging 11% in us-east1 for the first 35 minutes of the incident. In all cases, the majority of serverless errors were due to permission checks--for example--if a user were an admin, or if a user were authorized to access a restricted endpoint. Errors were returned in the form of HTTP 5XX responses. The Serverless API served an average of 70% errors during the impact period due to requiring IAM permission checks.\r\n\r\n##Cloud Bigtable\r\n\r\nCloud Bigtable instances in us-east1, us-east4, and southamerica-east1 were inaccessible for the first 18 minutes of the incident.\r\n\r\n##Cloud Memorystore\r\n\r\nCloud Memorystore Delete Instance, Get Instance, List, and List Instance operations were unavailable during the impact period. Existing instances were unaffected.\r\n\r\n##Cloud DNS\r\n\r\nCloud DNS API had an average error rate of 27% for the first 25 minutes of the incident. Domain resolution was unaffected.\r\n\r\n##Cloud Console\r\n\r\nThe Cloud Console served an average of 4.7% errors for the impact duration.\r\n\r\n##Google Cloud Storage (GCS)\r\n\r\nBuckets in us-east1 served an average of 99% errors for the duration of the incident, buckets in us-east4 and southamerica-east1 served an average of 97% and 96% errors respectively for the first 36 minutes of the incident. Buckets in the US multiregion served an average of 1.7% errors from 02:39 to 05:54 US/Pacific for a duration of 3 hours 15 minutes.\r\n\r\n##Google BigQuery\r\n\r\nBigQuery Dataset, Table, and Job operations were unavailable in us-east1, us-east4, and southamerica-east1 for the duration of the incident.\r\n\r\n##Google Cloud Load Balancing\r\n\r\nHealth checks to new instances or instances that were live-migrated during the incident failed in us-east1-c and us-east1-d. Instances that fail health checks will generally not receive any traffic. Existing instances were unaffected unless they were migrated. 4% of Managed Instance Groups (MIGs) in us-east1-b failed to repair itself, while up to 85% MIG instance creations failed. This meant that instances were created successfully, but their health checks timed out and the instances were unable to receive traffic. Autohealing and MIG health checks failed for up to 60% of requests in us-east1-d and up to 25% in us-east1-c. Up to 6% of Internal Load Balancer\u2019s (ILB) backend health checks failed in us-east1-d and up to 3.5% in us-east1-c. Google Cloud HTTP(S) Load Balancing backends saw up to 0.5% health checks failed globally.\r\n\r\n##Stackdriver Logging\r\n\r\nStackdriver Logging saw an average error rate of 11% globally, and up to a 98% error rate in us-east1, us-east4, and southamerica-east1.\r\n\r\n##Stackdriver Monitoring\r\n\r\nStackdriver Monitoring was unavailable in us-east, us-east4, and southamerica-east1 from 02:44 to 03:03 US/Pacific.\r\n\r\n##G Suite\r\n\r\nThe impact on G Suite users was different from and generally lower than the impact on Google Cloud Platform users due to differences in architecture and provisioning of these services. Please see the G Suite Status Dashboard (https://www.google.com/appsstatus) for details on affected G Suite services.\r\n\r\n\r\n# ROOT CAUSE AND REMEDIATION\r\nAt Google, data security is a critical part of service design. To accomplish this, services depend on the KMS for performing cryptographic functions such as encrypting and decrypting the keys used for protecting user data. On Monday 11 November, 2019 at 00:11 US/Pacific a new version of the KMS began to roll out, starting with a single zone in the us-east1 region. This binary added a new feature which was incompatible with older versions of related components. Six minutes later, an alert notified Google engineers of an increased error rate in the KMS. Additional alerts went off at 00:34 while investigation was ongoing. At this time there was no impact to Google Cloud users.\r\n\r\nIn order to begin recovery, Google engineers started to roll forward a change in the affected region, bringing all tasks up to the same version. A roll forward was necessary due to the backwards-incompatible nature of the newest version of the KMS, as persisted keys could not be processed by other components. The roll forward began at 02:32, and resulted in a temporary increase of internal errors from the KMS as the ratio of tasks with the new incompatible version and the previous version changed. By 02:39 the error rate reached a point where external users began to see service degradation. KMS is a low level service and is a critical dependency for core services such as IAM and Storage. The dependency on these core services created distinct start and end times for impact across higher-level services. \r\n\r\nThe roll forward completed by 02:59 and brought the KMS back to a healthy state. From 02:59 onward, services which depend on the KMS began to recover. Most services returned to a healthy state by 03:27, however, some services which integrate with the KMS in multiple locations, such as GCS, saw a longer tail of errors until other locations were brought back to a healthy state.\r\n\r\n# PREVENTION AND FOLLOW-UP\r\nA key reliability principle for GCP is that regions provide failure domain isolation; as a result, we will be modifying our processes to ensure that multiple regions are not impacted by a rollout of this particular component simultaneously. Projects are already in progress to split and isolate shared dependencies for the regions impacted by this incident. We are making changes to our canary and rollout validation processes to improve detection speed of similar incidents in future.\r\n\r\nAn additional principle is the ability to mitigate incidents quickly, typically through rollbacks, and we will be introducing explicit backwards-compatibility testing for the components implicated in this outage to ensure we comply with that principle in future.", "when": "2019-11-15T20:09:32Z"}, {"created": "2019-11-11T20:15:02Z", "modified": "2019-11-11T20:15:02Z", "text": "We have updated our initial incident description below to reflect impact more accurately.", "when": "2019-11-11T20:15:02Z"}, {"created": "2019-11-11T13:48:45Z", "modified": "2019-11-11T13:48:45Z", "text": "We will publish analysis of this incident once we have completed our internal investigation.", "when": "2019-11-11T13:47:00Z"}, {"created": "2019-11-11T13:36:34Z", "modified": "2019-11-11T13:36:34Z", "text": "The issue with multiple products is resolved for most of projects and our Engineering Team is working on full resolution for Stackdriver Monitoring and Google Cloud Storage for US multi-region.\n\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-11-11T13:36:34Z"}, {"created": "2019-11-11T12:44:37Z", "modified": "2019-11-11T12:44:37Z", "text": "Description:  We are investigating an issue with an infrastructure component impacting multiple products. We believe we have identified the cause and are currently rolling out mitigation.\n\nWe will provide an update by Monday, 2019-11-11 05:30 US/Pacific with current details.\n\nDiagnosis: None at this time\n\nWorkaround: None at this time", "when": "2019-11-11T12:44:37Z"}, {"created": "2019-11-11T12:00:24Z", "modified": "2019-11-11T12:00:24Z", "text": "Description: Mitigation work is currently underway by our engineering team.\n\nWe will provide more information by Monday, 2019-11-11 05:00 US/Pacific.\n\nDiagnosis: None at this time\n\nWorkaround: None at this time", "when": "2019-11-11T12:00:24Z"}, {"created": "2019-11-11T11:15:36Z", "modified": "2019-11-11T20:14:33Z", "text": "Description: We are experiencing an issue with some Google Cloud APIs across us-east1, us-east4 and southamerica-east1, with some APIs impacted globally. This includes the APIs for Compute Engine, Cloud Storage, BigQuery, Dataflow, Dataproc, and Pub/Sub. App Engine applications in those regions are also impacted.\r\n\r\nOur engineering team continues to investigate the issue.\r\n\r\nWe will provide an update by Monday, 2019-11-11 03:45 US/Pacific with current details.\r\n\r\n\r\nDiagnosis: None at this time\r\n\r\nWorkaround: None at this time", "when": "2019-11-11T11:15:36Z"}], "uri": "/incident/cloud-datastore/19006"}, {"begin": "2019-11-11T11:15:08Z", "created": "2019-11-11T12:08:08Z", "end": "2019-11-11T12:45:27Z", "external_desc": "We've received a report of an issue with Cloud SQL.", "modified": "2019-11-11T12:45:28Z", "most-recent-update": {"created": "2019-11-11T12:45:27Z", "modified": "2019-11-11T12:45:27Z", "text": "The issue with Cloud SQL has been resolved for all affected users as of Monday, 2019-11-11 04:45 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:45:27Z"}, "number": 19005, "public": true, "service_key": "cloud-sql", "service_name": "Google Cloud SQL", "severity": "medium", "updates": [{"created": "2019-11-11T12:45:27Z", "modified": "2019-11-11T12:45:27Z", "text": "The issue with Cloud SQL has been resolved for all affected users as of Monday, 2019-11-11 04:45 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:45:27Z"}, {"created": "2019-11-11T12:08:09Z", "modified": "2019-11-11T12:08:09Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:08:09Z"}], "uri": "/incident/cloud-sql/19005"}, {"begin": "2019-11-11T11:07:43Z", "created": "2019-11-11T12:08:29Z", "end": "2019-11-11T12:47:33Z", "external_desc": "We've received a report of an issue with Google App Engine.", "modified": "2019-11-11T12:47:33Z", "most-recent-update": {"created": "2019-11-11T12:47:33Z", "modified": "2019-11-11T12:47:33Z", "text": "The issue with Google App Engine has been resolved for all affected users as of Monday, 2019-11-11 04:47 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:47:33Z"}, "number": 19013, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-11-11T12:47:33Z", "modified": "2019-11-11T12:47:33Z", "text": "The issue with Google App Engine has been resolved for all affected users as of Monday, 2019-11-11 04:47 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:47:33Z"}, {"created": "2019-11-11T12:08:30Z", "modified": "2019-11-11T12:08:30Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:08:30Z"}], "uri": "/incident/appengine/19013"}, {"begin": "2019-11-11T11:04:03Z", "created": "2019-11-11T12:07:29Z", "end": "2019-11-11T12:47:58Z", "external_desc": "We've received a report of an issue with Google Compute Engine.", "modified": "2019-11-11T12:47:58Z", "most-recent-update": {"created": "2019-11-11T12:47:58Z", "modified": "2019-11-11T12:47:58Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:45 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:47:58Z"}, "number": 19010, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-11-11T12:47:58Z", "modified": "2019-11-11T12:47:58Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:45 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:47:58Z"}, {"created": "2019-11-11T12:07:30Z", "modified": "2019-11-11T12:07:30Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:07:30Z"}], "uri": "/incident/compute/19010"}, {"begin": "2019-11-11T11:02:45Z", "created": "2019-11-11T12:08:51Z", "end": "2019-11-11T14:27:55Z", "external_desc": "We've received a report of an issue with Google Cloud Storage.", "modified": "2019-11-11T14:27:56Z", "most-recent-update": {"created": "2019-11-11T14:27:55Z", "modified": "2019-11-11T14:27:55Z", "text": "The issue with Google Cloud Storage is believed to be affecting a very small fraction of requests and our Engineering Team is working on it.\n\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-11-11T14:27:55Z"}, "number": 19009, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-11-11T14:27:55Z", "modified": "2019-11-11T14:27:55Z", "text": "The issue with Google Cloud Storage is believed to be affecting a very small fraction of requests and our Engineering Team is working on it.\n\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-11-11T14:27:55Z"}, {"created": "2019-11-11T13:37:10Z", "modified": "2019-11-11T13:37:10Z", "text": "Description: The issue with Cloud Storage is resolved for most users, except for those in the \"us\" multi-region, where we still see errors on less than one percent of requests.\n\nWe are continuing to investigate and will provide an update by Monday, 2019-11-11 06:30 US/Pacific with current details.", "when": "2019-11-11T13:37:10Z"}, {"created": "2019-11-11T12:08:51Z", "modified": "2019-11-11T12:08:51Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:08:51Z"}], "uri": "/incident/storage/19009"}, {"begin": "2019-11-11T10:52:47Z", "created": "2019-11-11T12:21:05Z", "end": "2019-11-11T12:45:32Z", "external_desc": "We've received a report of an issue with Google BigQuery.", "modified": "2019-11-11T12:45:32Z", "most-recent-update": {"created": "2019-11-11T12:45:32Z", "modified": "2019-11-11T12:45:32Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:40 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:45:32Z"}, "number": 19009, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-11-11T12:45:32Z", "modified": "2019-11-11T12:45:32Z", "text": "The issue has been resolved for all affected projects as of Monday, 2019-11-11 04:40 US/Pacific.\n\nWe will publish an analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:45:32Z"}, {"created": "2019-11-11T12:21:05Z", "modified": "2019-11-11T12:21:05Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:21:05Z"}], "uri": "/incident/bigquery/19009"}, {"begin": "2019-11-11T10:40:06Z", "created": "2019-11-11T11:46:25Z", "end": "2019-11-11T11:50:36Z", "external_desc": "We've received a report of an issue with Cloud Networking.", "modified": "2019-11-11T12:13:37Z", "most-recent-update": {"created": "2019-11-11T12:11:36Z", "modified": "2019-11-11T12:11:36Z", "text": "The issue with Cloud Networking has been resolved for all affected projects as of Monday, 2019-11-11 04:00 US/Pacific\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:11:36Z"}, "number": 19022, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-11-11T12:11:36Z", "modified": "2019-11-11T12:11:36Z", "text": "The issue with Cloud Networking has been resolved for all affected projects as of Monday, 2019-11-11 04:00 US/Pacific\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:11:36Z"}, {"created": "2019-11-11T11:46:26Z", "modified": "2019-11-11T11:46:26Z", "text": "Description: We believe the issue with Cloud Networking is partially resolved.\n\nWe will provide an update by Monday, 2019-11-11 04:30 US/Pacific with current details.", "when": "2019-11-11T11:46:26Z"}], "uri": "/incident/cloud-networking/19022"}, {"begin": "2019-11-11T10:09:50Z", "created": "2019-11-11T12:09:57Z", "end": "2019-11-11T12:32:29Z", "external_desc": "We've received a report of an issue with Cloud Firestore.", "modified": "2019-11-11T12:32:29Z", "most-recent-update": {"created": "2019-11-11T12:32:29Z", "modified": "2019-11-11T12:32:29Z", "text": "The issue with Cloud Datastore has been resolved for all affected users as of Monday, 2019-11-11 04:31 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:32:29Z"}, "number": 19007, "public": true, "service_key": "cloud-datastore", "service_name": "Google Cloud Datastore", "severity": "medium", "updates": [{"created": "2019-11-11T12:32:29Z", "modified": "2019-11-11T12:32:29Z", "text": "The issue with Cloud Datastore has been resolved for all affected users as of Monday, 2019-11-11 04:31 US/Pacific.\n\nWe will publish analysis of this incident once we have completed our internal investigation.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-11T12:32:29Z"}, {"created": "2019-11-11T12:09:58Z", "modified": "2019-11-11T12:09:58Z", "text": "Description: We are experiencing an issue with multiple products.\n\nOur engineering team continues to investigate the issue.\n\nFor regular status updates, please follow: https://status.cloud.google.com/incident/cloud-datastore/19006 where we will provide the next update by Monday, 2019-11-11 05:00 US/Pacific.", "when": "2019-11-11T12:09:58Z"}], "uri": "/incident/cloud-datastore/19007"}, {"begin": "2019-11-04T19:46:04Z", "created": "2019-11-07T16:49:40Z", "end": "2019-11-13T23:38:45Z", "external_desc": "We are investigating an issue with Google Kubernetes Engine where some nodes in recently upgraded clusters (see affected versions) may be experiencing elevated numbers of kernel panics", "modified": "2019-11-13T23:38:45Z", "most-recent-update": {"created": "2019-11-13T23:38:45Z", "modified": "2019-11-13T23:38:45Z", "text": "The issue with Google Kubernetes Engine clusters with node pools experiencing an elevated number of kernel panics has been resolved in a new release of GKE available as of Wednesday, 2019-11-11 16:00 US/Pacific.\n\nThe fix is contained in the following versions of GKE which is currently rolling out to node pools with auto upgrade enabled [1]. This should complete by Friday, 2019-11-15. Any customer on manual updates will need to manually upgrade their nodes to the following versions.:\n\n1.13.11-gke.14\n1.13.12-gke.8\n1.14.7-gke.23\n1.14.8-gke.12\n\nPlease note that this fix has downgraded the version of CoS to cos-73-11647-293-0 [2] as a temporary mitigation, we expect the next release of GKE to have an upgraded kernel and fix for the panics seen in the below releases.. \n\nAffected versions were: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7\n\nWe thank you for your patience while we've worked on resolving the issue.\n\n[1] - https://cloud.google.com/kubernetes-engine/versioning-and-upgrades#rollout_schedule\n[2] - https://cloud.google.com/container-optimized-os/docs/release-notes#cos-73-11647-293-0", "when": "2019-11-13T23:38:45Z"}, "number": 19012, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-11-13T23:38:45Z", "modified": "2019-11-13T23:38:45Z", "text": "The issue with Google Kubernetes Engine clusters with node pools experiencing an elevated number of kernel panics has been resolved in a new release of GKE available as of Wednesday, 2019-11-11 16:00 US/Pacific.\n\nThe fix is contained in the following versions of GKE which is currently rolling out to node pools with auto upgrade enabled [1]. This should complete by Friday, 2019-11-15. Any customer on manual updates will need to manually upgrade their nodes to the following versions.:\n\n1.13.11-gke.14\n1.13.12-gke.8\n1.14.7-gke.23\n1.14.8-gke.12\n\nPlease note that this fix has downgraded the version of CoS to cos-73-11647-293-0 [2] as a temporary mitigation, we expect the next release of GKE to have an upgraded kernel and fix for the panics seen in the below releases.. \n\nAffected versions were: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7\n\nWe thank you for your patience while we've worked on resolving the issue.\n\n[1] - https://cloud.google.com/kubernetes-engine/versioning-and-upgrades#rollout_schedule\n[2] - https://cloud.google.com/container-optimized-os/docs/release-notes#cos-73-11647-293-0", "when": "2019-11-13T23:38:45Z"}, {"created": "2019-11-12T02:53:24Z", "modified": "2019-11-12T02:53:24Z", "text": "Description: The following fixed versions are now available and should fix the kernel panic issue: 1.13.11-gke.14, 1.13.12-gke.8, 1.14.7-gke.23 and 1.14.8-gke.12.\n\nMitigation work is currently underway by our engineering team to roll out the fixed versions to clusters configured with node auto-update, and is expected to be complete by Wednesday, 2019-11-13. Clusters not configured with node auto-update can be manually upgraded.\n\nAt this time the following versions are still affected: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7.\n\n\n\nWe will provide more information as it becomes available or by Wednesday, 2019-11-13 17:00 US/Pacific at the latest.\n\nDiagnosis: Affected users may notice elevated levels of kernel panics on nodes running one of the affected versions listed above.\n\nWorkaround: Users seeing this issue can upgrade to a fixed release.", "when": "2019-11-12T02:53:24Z"}, {"created": "2019-11-09T01:10:53Z", "modified": "2019-11-09T01:10:53Z", "text": "Description: This issue was downgraded to an orange category Service Disruption as the number of projects actually affected is very low. \n\nMitigation work is currently underway by our engineering team and is expected to be complete by Wednesday, 2019-11-13.\n\nAt this time the following versions are still affected: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7.\n\nWe will provide more information as it becomes available or by Wednesday, 2019-11-13 17:00 US/Pacific at the latest.\n\nDiagnosis: Affected users may notice elevated levels of kernel panics on nodes upgraded to one of the affected versions listed above.\n\nWorkaround: Users seeing this issue can downgrade to a previous release (not listed in the affected versions above). \n\nUsers on a Release Channel affected by this issue should reach out to support for assistance with downgrading their nodes.", "when": "2019-11-09T01:10:53Z"}, {"created": "2019-11-07T22:49:06Z", "modified": "2019-11-07T22:49:06Z", "text": "Description: This issue was downgraded to an orange category Service Disruption as the number of projects actually affected is very low. \n\nMitigation work is currently underway by our engineering team and is expected to completed by early next week.\n\nAt this time the following versions are still affected: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7.\n\nWe will provide more information as it becomes available or by Friday, 2019-11-08 17:00 US/Pacific at the latest.\n\nDiagnosis: Affected users may notice elevated levels of kernel panics on nodes upgraded to one of the affected versions listed above.\n\nWorkaround: Users seeing this issue can downgrade to a previous release (not listed in the affected versions above). \n\nUsers on a Release Channel affected by this issue should reach out to support for assistance with downgrading their nodes.", "when": "2019-11-07T22:49:06Z"}, {"created": "2019-11-07T16:49:41Z", "modified": "2019-11-07T16:49:41Z", "text": "Description: Mitigation work is currently underway by our engineering team.\n\nThe mitigation is expected to complete by early next week. At this time the following versions are still affected: 1.13.11-gke.9, 1.14.7-gke.14, 1.13.12-gke.1, 1.14.8-gke.1, 1.13.11-gke.11, 1.13.12-gke.2, 1.14.7-gke.17, 1.14.8-gke.2, 1.13.12-gke.3, 1.14.8-gke.6, 1.13.11-gke.12, 1.13.12-gke.4, and 1.14.8-gke.7.\n\nWe will provide more information by Monday, 2019-11-11 15:00 US/Pacific.\n\nDiagnosis: Kernel panics following an upgrade in GKE version.\n\nWorkaround: Users seeing this issue may go to a previous patch release (not listed in the affected versions above).", "when": "2019-11-07T16:49:41Z"}], "uri": "/incident/container-engine/19012"}, {"begin": "2019-10-31T23:41:40Z", "created": "2019-11-01T12:29:28Z", "end": "2019-11-02T17:00:00Z", "external_desc": "We are experiencing an issue with Google Kubernetes Engine.", "modified": "2019-11-02T18:00:15Z", "most-recent-update": {"created": "2019-11-01T19:42:30Z", "modified": "2019-11-01T19:42:30Z", "text": "Our engineers have determined that Google Kubernetes Engine (GKE) was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:42:30Z"}, "number": 19011, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-11-01T19:42:30Z", "modified": "2019-11-01T19:42:30Z", "text": "Our engineers have determined that Google Kubernetes Engine (GKE) was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:42:30Z"}, {"created": "2019-11-01T15:53:45Z", "modified": "2019-11-01T15:53:45Z", "text": "The issue with creating new Google Kubernetes Engine clusters has been resolved for all affected projects as of Friday, 2019-11-01 08:50 US/Pacific.\n\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-11-01T15:53:45Z"}, {"created": "2019-11-01T14:17:00Z", "modified": "2019-11-01T14:17:00Z", "text": "Description: Mitigation work is currently underway by our engineering team.\n\nWe will provide more information by Friday, 2019-11-01 09:00 US/Pacific.\n\nDiagnosis: Cluster creation might fail with NetworkUnavailable status set to True. For existing clusters, new nodes might be created without network routes. As a result, autoscaler might not be able to add capacity to the cluster.\n\nWorkaround: None at this time", "when": "2019-11-01T14:17:00Z"}, {"created": "2019-11-01T12:52:31Z", "modified": "2019-11-01T12:52:31Z", "text": "Description: We are experiencing an issue with Google Kubernetes Engine beginning at Thursday, 2019-10-31 17:00 US/Pacific.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Friday, 2019-11-01 07:00 US/Pacific with current details.\n\nDiagnosis: Cluster creation might fail with NetworkUnavailable status set to True. For existing clusters, new nodes might be created without network routes. As a result, autoscaler might not be able to add capacity to the cluster.\n\nWorkaround: None at this time", "when": "2019-11-01T12:52:31Z"}, {"created": "2019-11-01T12:29:29Z", "modified": "2019-11-01T12:29:29Z", "text": "Description: We are experiencing an issue with Google Kubernetes Engine beginning at Thursday, 2019-10-31 17:00 US/Pacific.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Friday, 2019-11-01 06:00 US/Pacific with current details.\n\nDiagnosis: Cluster creation might fail with NetworkUnavailable status set to True. For existing clusters, adding a new node might fail. Autoscaler might not be able to increase the number of nodes.\n\nWorkaround: None at this time", "when": "2019-11-01T12:29:29Z"}], "uri": "/incident/container-engine/19011"}, {"begin": "2019-10-31T23:41:30Z", "created": "2019-11-01T15:02:21Z", "end": "2019-11-02T17:00:00Z", "external_desc": "Our engineers have determine issue to be linked to a single Google incident.", "modified": "2019-11-02T18:01:48Z", "most-recent-update": {"created": "2019-11-01T19:49:20Z", "modified": "2019-11-01T19:49:20Z", "text": "Our engineers have determined that Google Cloud infrastructure components was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:49:20Z"}, "number": 19001, "public": true, "service_key": "zall", "service_name": "Google Cloud infrastructure components", "severity": "high", "updates": [{"created": "2019-11-01T19:49:20Z", "modified": "2019-11-01T19:49:20Z", "text": "Our engineers have determined that Google Cloud infrastructure components was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:49:20Z"}, {"created": "2019-11-01T16:37:02Z", "modified": "2019-11-01T16:37:02Z", "text": "The issue with Google Cloud infrastructure components is now resolved and we are working to address individually affected services now. For further status updates, please visit [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008). \n\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-11-01T16:37:02Z"}, {"created": "2019-11-01T15:07:53Z", "modified": "2019-11-01T15:07:53Z", "text": "Description: Our engineers have determine issue to be linked to a single Google\nincident.  For regular status updates, please\nvisit [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008). We will provide an update by Friday, 2019-11-01 15:00 US/Pacific with further details.", "when": "2019-11-01T15:07:53Z"}, {"created": "2019-11-01T15:02:22Z", "modified": "2019-11-01T15:02:22Z", "text": "Description: Our engineers have determine issue to be linked to a single Google\nincident.  For regular status updates, please\nvisit [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008). We will provide an update by Friday, 2019-11-01 08:45 US/Pacific with further details.", "when": "2019-11-01T15:02:22Z"}], "uri": "/incident/zall/19001"}, {"begin": "2019-10-31T23:41:24Z", "created": "2019-11-01T12:24:06Z", "end": "2019-11-01T14:41:00Z", "external_desc": "We've received a report of issues with Cloud Memorystore", "modified": "2019-11-01T22:39:48Z", "most-recent-update": {"created": "2019-11-01T19:39:46Z", "modified": "2019-11-01T19:39:46Z", "text": "Our engineers have determined that Cloud Memorystore was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:39:46Z"}, "number": 19002, "public": true, "service_key": "cloud-memorystore", "service_name": "Cloud Memorystore", "severity": "medium", "updates": [{"created": "2019-11-01T19:39:46Z", "modified": "2019-11-01T19:39:46Z", "text": "Our engineers have determined that Cloud Memorystore was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:39:46Z"}, {"created": "2019-11-01T14:04:54Z", "modified": "2019-11-01T14:04:54Z", "text": "The Cloud Memorystore issue is believed to be affecting less than 1% of projects and our Engineering Team is working on it.\n\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-11-01T14:04:54Z"}, {"created": "2019-11-01T12:52:14Z", "modified": "2019-11-01T12:52:14Z", "text": "Description: We are experiencing an issue with Cloud Memorystore beginning at Thursday, 2019-10-31 17:00 US/Pacific.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Friday, 2019-11-01 07:00 US/Pacific with current details.\n\nDiagnosis: Creating new Cloud Memorystore instances might fail.", "when": "2019-11-01T12:52:14Z"}, {"created": "2019-11-01T12:24:07Z", "modified": "2019-11-01T12:24:07Z", "text": "Description: We are experiencing an issue with Cloud Memorystore beginning at Friday, 2019-10-31 17:00 US/Pacific.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Friday, 2019-11-01 06:00 US/Pacific with current details.\n\nDiagnosis: Creating new Cloud Memorystore instances might fail.", "when": "2019-11-01T12:24:07Z"}], "uri": "/incident/cloud-memorystore/19002"}, {"begin": "2019-10-31T23:41:00Z", "created": "2019-11-01T19:37:02Z", "end": "2019-11-01T15:15:00Z", "external_desc": "We've received a report of issues with Google Cloud App Engine.", "modified": "2019-11-01T22:34:15Z", "most-recent-update": {"created": "2019-11-01T19:37:02Z", "modified": "2019-11-01T19:37:02Z", "text": "Our engineers have determined that Google App Engine Flexible was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:37:02Z"}, "number": 19012, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-11-01T19:37:02Z", "modified": "2019-11-01T19:37:02Z", "text": "Our engineers have determined that Google App Engine Flexible was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:37:02Z"}], "uri": "/incident/appengine/19012"}, {"begin": "2019-10-31T23:41:00Z", "created": "2019-11-01T19:46:52Z", "end": "2019-11-01T21:14:00Z", "external_desc": "We've received a report of issues with Google Cloud Filestore.", "modified": "2019-11-01T23:03:14Z", "most-recent-update": {"created": "2019-11-01T19:46:52Z", "modified": "2019-11-01T19:46:52Z", "text": "Our engineers have determined that Google Cloud Filestore was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:46:52Z"}, "number": 19003, "public": true, "service_key": "cloud-filestore", "service_name": "Cloud Filestore", "severity": "medium", "updates": [{"created": "2019-11-01T19:46:52Z", "modified": "2019-11-01T19:46:52Z", "text": "Our engineers have determined that Google Cloud Filestore was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:46:52Z"}], "uri": "/incident/cloud-filestore/19003"}, {"begin": "2019-10-31T23:41:00Z", "created": "2019-11-01T19:41:00Z", "end": "2019-11-01T16:00:00Z", "external_desc": "We've received a report of issues with Google Machine Learning", "modified": "2019-11-01T20:52:29Z", "most-recent-update": {"created": "2019-11-01T19:41:00Z", "modified": "2019-11-01T19:41:00Z", "text": "Our engineers have determined that Cloud ML Engine was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:41:00Z"}, "number": 19002, "public": true, "service_key": "cloud-ml", "service_name": "Cloud Machine Learning", "severity": "medium", "updates": [{"created": "2019-11-01T19:41:00Z", "modified": "2019-11-01T19:41:00Z", "text": "Our engineers have determined that Cloud ML Engine was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:41:00Z"}], "uri": "/incident/cloud-ml/19002"}, {"begin": "2019-10-31T23:41:00Z", "created": "2019-11-01T19:38:45Z", "end": "2019-11-02T17:00:00Z", "external_desc": "We've received a report of issues with Google Cloud Networking.", "modified": "2019-11-02T18:02:32Z", "most-recent-update": {"created": "2019-11-01T19:38:45Z", "modified": "2019-11-01T19:38:45Z", "text": "Our engineers have determined that Google Cloud Networking was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:38:45Z"}, "number": 19021, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-11-01T19:38:45Z", "modified": "2019-11-01T19:38:45Z", "text": "Our engineers have determined that Google Cloud Networking was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:38:45Z"}], "uri": "/incident/cloud-networking/19021"}, {"begin": "2019-10-31T23:41:00Z", "created": "2019-11-01T19:43:26Z", "end": "2019-11-01T19:41:00Z", "external_desc": "We've received a report of issues with Cloud Composer", "modified": "2019-11-01T20:22:44Z", "most-recent-update": {"created": "2019-11-01T19:43:26Z", "modified": "2019-11-01T19:43:26Z", "text": "Our engineers have determined that Cloud Composer was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:43:26Z"}, "number": 19003, "public": true, "service_key": "composer", "service_name": "Google Cloud Composer", "severity": "medium", "updates": [{"created": "2019-11-01T19:43:26Z", "modified": "2019-11-01T19:43:26Z", "text": "Our engineers have determined that Cloud Composer was impacted by the same underlying issue as the Google Compute Engine (GCE) incident. The start & end times have been updated in this incident to reflect the approximate impact period. Please refer to [https://status.cloud.google.com/incident/compute/19008](https://status.cloud.google.com/incident/compute/19008) for the full impact details of the incident. No further updates will be provided here.", "when": "2019-11-01T19:43:26Z"}], "uri": "/incident/composer/19003"}, {"begin": "2019-10-31T23:30:48Z", "created": "2019-11-01T05:06:45Z", "end": "2019-11-02T21:00:57Z", "external_desc": "Some networking update/create/delete operations pending globally", "modified": "2019-11-09T00:17:43Z", "most-recent-update": {"created": "2019-11-09T00:13:09Z", "modified": "2019-11-09T00:15:23Z", "text": "# ISSUE SUMMARY\r\n\r\n\r\nOn Thursday 31 October, 2019, network administration operations on Google Compute Engine (GCE), such as creating/deleting firewall rules, routes, global load balancers, subnets, or new VPCs, were subject to elevated latency and errors. Specific service impact is outlined in detail below. \r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\n\r\nOn Thursday 31 October, 2019 from 16:30 to 18:00 US/Pacific and again from 20:24 to 23:08 Google Compute Engine experienced elevated latency and errors applying certain network administration operations. At 23:08, the issue was mitigated fully, and as a result, administrative operations began to succeed for most projects. However, projects which saw network administration operations fail during the incident were left stuck in a state where new operations could not be applied. The cleanup process for these stuck projects took until 2019-11-02 14:00. \r\n\r\n\r\nThe following services experienced up to a 100% error rate when submitting create, modify, and/or delete requests that relied on Google Compute Engine\u2019s global (and in some cases, regional) networking APIs between 2019-10-31 16:40 - 18:00 and 20:24 - 23:08 US/Pacific for a combined duration of 4 hours and 4 minutes:\r\n\r\n\r\n  -- Google Compute Engine\r\n\r\n  -- Google Kubernetes Engine\r\n\r\n  -- Google App Engine Flexible\r\n\r\n  -- Google Cloud Filestore\r\n\r\n  -- Google Cloud Machine Learning Engine\r\n\r\n  -- Google Cloud Memorystore\r\n\r\n  -- Google Cloud Composer\r\n\r\n  -- Google Cloud Data Fusion\r\n\r\n\r\n\r\n# ROOT CAUSE\r\n\r\nGoogle Compute Engine\u2019s networking stack consists of software which is made up of two components, a control plane and data plane. The data plane is where packets are processed and routed based on the configuration set up by the control plane. GCE\u2019s networking control plane has global components that are responsible for fanning-out network configurations that can affect an entire VPC network to downstream (regional/zonal) networking controllers. Each region and zone has their own control plane service, and each control plane service is sharded such that network programming is spread across multiple shards.\r\n\r\n\r\nA performance regression introduced in a recent release of the networking control software caused the service to begin accumulating a backlog of requests. The backlog eventually became significant enough that requests timed out, leaving some projects stuck in a state where further administrative operations could not be applied. The backlog was further exacerbated by the retry policy in the system sending the requests, which increased load still further. Manual intervention was required to clear the stuck projects, prolonging the incident.\r\n\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\n\r\nGoogle engineers were alerted to the problem on 2019-10-31 at 17:10 US/Pacific and immediately began investigating. From 17:10 to 18:00, engineers ruled out potential sources of the outage without finding a definitive root cause. The networking control plane performed an automatic failover at 17:57, dropping the error rate. This greatly reduced the number of stuck operations in the system and significantly mitigated user impact. However, after 18:59, the overload condition returned and error rates again increased. After further investigation from multiple teams, additional mitigation efforts began at 19:52, when Google engineers allotted additional resources to the overloaded components. At 22:16, as a further mitigation, Google engineers introduced a rate limit designed to throttle requests to the network programming distribution service. At 22:28, this service was restarted, allowing it to drop any pending requests from its queue. The rate limit coupled with the restart mitigated the issue of new operations becoming stuck, allowing the team to begin focusing on the cleanup of stuck projects.\r\n\r\n\r\nResolving the stuck projects required manual intervention, which was unique to each failed operation type. Engineers worked round the clock to address each operation type in turn; as each was processed, further operations of the same type (from the same project) also began to be processed. 80% of the stuck operations were processed by 2019-11-01 16:00, and all operations were fully processed by 2019-11-02 14:00.\r\n\r\n\r\nWe will be taking these immediate steps to prevent this class of error from recurring:\r\n\r\n\r\n  -- We are implementing continuous load testing as part of the deployment pipeline of the component which suffered the performance regression, so that such issues are identified before they reach production in future.\r\n\r\n  -- We have rate-limited the traffic between the impacted control plane components to avoid the congestion collapse experienced during this incident.\r\n\r\n  -- We are further sharding the global network programming distribution service to allow for graceful horizontal scaling under high traffic.\r\n\r\n  -- We are automating the steps taken to unstick administrative operations, to eliminate the need for manual cleanup after failures such as this one.\r\n\r\n  -- We are adding alerting to the network programming distribution service, to reduce response time in the event of a similar problem in the future.\r\n\r\n  -- We are changing the way the control plane processes requests to allow forward progress even when there is a significant backlog.\r\n\r\n\r\n\r\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\r\n\r\n\r\nIf you believe your application experienced an SLA violation as a result of this incident, please contact us (https://support.google.com/cloud/answer/6282346).", "when": "2019-11-09T00:13:09Z"}, "number": 19008, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "high", "updates": [{"created": "2019-11-09T00:13:09Z", "modified": "2019-11-09T00:15:23Z", "text": "# ISSUE SUMMARY\r\n\r\n\r\nOn Thursday 31 October, 2019, network administration operations on Google Compute Engine (GCE), such as creating/deleting firewall rules, routes, global load balancers, subnets, or new VPCs, were subject to elevated latency and errors. Specific service impact is outlined in detail below. \r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\n\r\nOn Thursday 31 October, 2019 from 16:30 to 18:00 US/Pacific and again from 20:24 to 23:08 Google Compute Engine experienced elevated latency and errors applying certain network administration operations. At 23:08, the issue was mitigated fully, and as a result, administrative operations began to succeed for most projects. However, projects which saw network administration operations fail during the incident were left stuck in a state where new operations could not be applied. The cleanup process for these stuck projects took until 2019-11-02 14:00. \r\n\r\n\r\nThe following services experienced up to a 100% error rate when submitting create, modify, and/or delete requests that relied on Google Compute Engine\u2019s global (and in some cases, regional) networking APIs between 2019-10-31 16:40 - 18:00 and 20:24 - 23:08 US/Pacific for a combined duration of 4 hours and 4 minutes:\r\n\r\n\r\n  -- Google Compute Engine\r\n\r\n  -- Google Kubernetes Engine\r\n\r\n  -- Google App Engine Flexible\r\n\r\n  -- Google Cloud Filestore\r\n\r\n  -- Google Cloud Machine Learning Engine\r\n\r\n  -- Google Cloud Memorystore\r\n\r\n  -- Google Cloud Composer\r\n\r\n  -- Google Cloud Data Fusion\r\n\r\n\r\n\r\n# ROOT CAUSE\r\n\r\nGoogle Compute Engine\u2019s networking stack consists of software which is made up of two components, a control plane and data plane. The data plane is where packets are processed and routed based on the configuration set up by the control plane. GCE\u2019s networking control plane has global components that are responsible for fanning-out network configurations that can affect an entire VPC network to downstream (regional/zonal) networking controllers. Each region and zone has their own control plane service, and each control plane service is sharded such that network programming is spread across multiple shards.\r\n\r\n\r\nA performance regression introduced in a recent release of the networking control software caused the service to begin accumulating a backlog of requests. The backlog eventually became significant enough that requests timed out, leaving some projects stuck in a state where further administrative operations could not be applied. The backlog was further exacerbated by the retry policy in the system sending the requests, which increased load still further. Manual intervention was required to clear the stuck projects, prolonging the incident.\r\n\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\n\r\nGoogle engineers were alerted to the problem on 2019-10-31 at 17:10 US/Pacific and immediately began investigating. From 17:10 to 18:00, engineers ruled out potential sources of the outage without finding a definitive root cause. The networking control plane performed an automatic failover at 17:57, dropping the error rate. This greatly reduced the number of stuck operations in the system and significantly mitigated user impact. However, after 18:59, the overload condition returned and error rates again increased. After further investigation from multiple teams, additional mitigation efforts began at 19:52, when Google engineers allotted additional resources to the overloaded components. At 22:16, as a further mitigation, Google engineers introduced a rate limit designed to throttle requests to the network programming distribution service. At 22:28, this service was restarted, allowing it to drop any pending requests from its queue. The rate limit coupled with the restart mitigated the issue of new operations becoming stuck, allowing the team to begin focusing on the cleanup of stuck projects.\r\n\r\n\r\nResolving the stuck projects required manual intervention, which was unique to each failed operation type. Engineers worked round the clock to address each operation type in turn; as each was processed, further operations of the same type (from the same project) also began to be processed. 80% of the stuck operations were processed by 2019-11-01 16:00, and all operations were fully processed by 2019-11-02 14:00.\r\n\r\n\r\nWe will be taking these immediate steps to prevent this class of error from recurring:\r\n\r\n\r\n  -- We are implementing continuous load testing as part of the deployment pipeline of the component which suffered the performance regression, so that such issues are identified before they reach production in future.\r\n\r\n  -- We have rate-limited the traffic between the impacted control plane components to avoid the congestion collapse experienced during this incident.\r\n\r\n  -- We are further sharding the global network programming distribution service to allow for graceful horizontal scaling under high traffic.\r\n\r\n  -- We are automating the steps taken to unstick administrative operations, to eliminate the need for manual cleanup after failures such as this one.\r\n\r\n  -- We are adding alerting to the network programming distribution service, to reduce response time in the event of a similar problem in the future.\r\n\r\n  -- We are changing the way the control plane processes requests to allow forward progress even when there is a significant backlog.\r\n\r\n\r\n\r\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\r\n\r\n\r\nIf you believe your application experienced an SLA violation as a result of this incident, please contact us (https://support.google.com/cloud/answer/6282346).", "when": "2019-11-09T00:13:09Z"}, {"created": "2019-11-02T17:51:57Z", "modified": "2019-11-02T17:51:57Z", "text": "Our engineers have made significant progress unsticking operations overnight and early this morning. At this point in time, the issue with Google Cloud Networking operations being stuck is believed to be affecting a very small number of remaining projects and our Engineering Team is actively working on unsticking the final stuck operations.\n\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you directly until this issue is fully resolved.\n\nNo further updates will be provided here.", "when": "2019-11-02T17:51:57Z"}, {"created": "2019-11-02T03:35:07Z", "modified": "2019-11-02T03:35:07Z", "text": "Description: Mitigation efforts have successfully mitigated most types of operations. At this time the backlog consists of mostly network and subnet deletion operations, and a small fraction of create subnet operations. This affects subnets created during the impact window. Subnets created outside of this window remain unaffected. \n\nMitigation efforts will continue overnight to unstick the remaining operations.\n\n\nWe will publish an analysis of this incident once we have completed our internal investigation. We thank you for your patience while we have worked on resolving the issue. We will provide more information by Saturday, 2019-11-02 11:00 US/Pacific.\n\n\nDiagnosis: Google Cloud Networking\n\n- Networking-related Compute API operations stuck pending if submitted during the above time.\n- The affected operations include:  deleting and creating subnets, creating networks. \nResubmitting similar requests may also enter a pending state as they are waiting for the previous operation to complete.\n\nGoogle Kubernetes Engine\n\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Cloud Networking. \n\n \n\nWorkaround: No workaround is available at the moment", "when": "2019-11-02T03:35:07Z"}, {"created": "2019-11-01T23:32:04Z", "modified": "2019-11-01T23:32:04Z", "text": "Description: Approximately 25% of global (and regional) route and subnet deletion operations remain stuck in a pending state. Mitigation work is still underway to unblock pending network operations globally. We expect the majority of mitigations to complete over the next several hours, with the long-tail going into tomorrow.\n\nPlease note, this will allow newer incoming operations of the same type to eventually process successfully. However, resubmitting similar requests may still get stuck in a running state as they are waiting for previously queued operations to complete.\n\nWe will publish an analysis of this incident once we have completed our internal investigation. We thank you for your patience while we have worked on resolving the issue. We will provide more information by Friday, 2019-11-01 20:30 US/Pacific.\n\nDiagnosis: Google Cloud Networking\n\n- Networking-related Compute API operations stuck pending if submitted during the above time.\n- The affected operations include: [deleting/creating] backend services, subnets, instance groups, routes and firewall rules.\n- Resubmitting similar requests may also enter a pending state as they are waiting for the previous operation to complete.\n- Our product team is working to unblock any pending operation\n\nGoogle Compute Engine\n\n- 40-80% of Compute Engine API operations may have become stuck pending if submitted during the above time.\n- Affected operations include any operation which would need to update Networking on affected projects\n\nGoogle Cloud Filestore\n\n- Impacts instance creation/deletion\n\nGoogle Kubernetes Engine\n\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Cloud Networking. \n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T23:32:04Z"}, {"created": "2019-11-01T21:38:24Z", "modified": "2019-11-01T21:38:24Z", "text": "Currently, the backlog of pending operations has been reduced by approximately 70%, and we expect the majority of mitigations to complete over the next several hours, with the long-tail going into tomorrow. Mitigation work is still underway to unblock pending network operations globally.\r\n\r\nTo determine whether you are affected by this incident, you may run the following command [1] \u201cgcloud compute operations list --filter=\"status!=DONE\u201d to view your project\u2019s pending operations. If you see global operations (or regional subnet operations) that are running for a long time (or significantly longer than usual), then you are likely still impacted.\r\n\r\nThe remaining 30% of stuck operations are currently either being processed successfully or marked as failed. This will allow newer incoming operations of the same type to be eventually processed successfully, however, resubmitting similar requests may also get stuck in a running state as they are waiting for the queued operations to complete.\r\n\r\nIf you have an operation that does not appear to be finishing, please wait for it to succeed or be marked as failed before retrying the operation.\r\n\r\nFor Context:\r\n40-80% of Cloud Networking operations submitted between 2019-10-31 16:41 US/Pacific and 2019-10-31 23:01 US/Pacific may have been affected. The exact percentage of failures is region dependent.\r\n\r\nWe will provide more information by Friday, 2019-11-01 16:30 US/Pacific.\r\n\r\n[1] https://cloud.google.com/sdk/gcloud/reference/compute/operations/list\r\n\r\nDiagnosis:\r\n\r\nAs we become aware of products which were impacted we will update this post to ensure transparency.\r\n\r\nGoogle Cloud Networking\r\nNetworking-related Compute API operations stuck pending if submitted during the above time.\r\nThe affected operations include: [deleting/creating] backend services, subnets, instance groups, routes and firewall rules.\r\nResubmitting similar requests may also enter a pending state as they are waiting for the previous operation to complete.\r\nOur product team is working to unblock any pending operation\r\n\r\nGoogle Compute Engine\r\n40-80% of Compute Engine API operations may have become stuck pending if submitted during the above time.\r\nAffected operations include any operation which would need to update Networking on affected projects\r\n\r\nGoogle Cloud DNS\r\nSome DNS updates may be stuck pending from during the above time.\r\n\r\nGoogle Cloud Filestore\r\nImpacts instance creation/deletion\r\n\r\nCloud Machine Learning\r\nOnline prediction jobs using Google Kubernetes Engine may have experienced failures during this time\r\nThe team is no longer seeing issues affecting Cloud Machine Learning and we feel the incident for this product is now resolved. \r\n\r\nCloud Composer\r\nCreate Environment operations during the affected time may have experienced failures.\r\nCustomers should no longer being seeing impact\r\n\r\nGoogle Kubernetes Engine\r\nCluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\r\nNew Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Cloud Networking. \r\n\r\nGoogle Cloud Memorystore\r\nThis issue is believed to have affected less than 1% of projects\r\nThe affected projects should find full resolution once the issue affecting Google Compute Engine is resolved. \r\n\r\nApp Engine Flexible \r\nNew deployments experienced elevated failure rates during the affected time. \r\nThe team is no longer seeing issues affecting new deployment creation", "when": "2019-11-01T21:38:24Z"}, {"created": "2019-11-01T19:22:56Z", "modified": "2019-11-01T19:22:56Z", "text": "Description: Mitigation work continues to unblock pending network operations globally. 40-80% of Cloud Networking operations submitted between Thursday, 2019-10-31 16:41 US/Pacific and Thursday, 2019-10-31 23:01 US/Pacific may have been affected. The exact amount of failures is region dependent.\n\nOur team has been able to reduce the number of pending operations by 60% at this time. We expect mitigation to continue over the next 4 hours and are working to clear the pending operations by largest type impacted.\n\nWe will provide more information by Friday, 2019-11-01 14:30 US/Pacific.\n\n\nDiagnosis: As we become aware of products which were impacted we will update this post to ensure transparency.\n\nGoogle Compute Engine\n- Networking-related Compute API operations pending to complete if submitted during the above time.\n- Resubmitting similar requests may fail as they are waiting for the above operations to complete.\n- The affected operations include: deleting backend services, subnets, instance groups, routes and firewall rules.\n- Some operations may still show as pending and are being mitigated at this time. We are currently working to address operations around subnet deletion as our next target group\n\nGoogle Kubernetes Engine\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Compute Engine. No further updates will be provided for Google Kubernetes Engine in this post.\n\nGoogle Cloud Memorystore\n- This issue is believed to have affected less than 1% of projects\n- The affected projects should find full resolution once the issue affecting Google Compute Engine is resolved. No further updates will be provided for Google Cloud Memorystore\n\nApp Engine Flexible \n- New deployments experienced elevated failure rates during the affected time. \n- The team is no longer seeing issues affecting new deployment creation and we feel the incident for this product is now resolved. No further updates will be provided for App Engine Flexible\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T19:22:56Z"}, {"created": "2019-11-01T17:13:29Z", "modified": "2019-11-01T17:13:29Z", "text": "Description: Mitigation work is currently underway by our product team to unblock stuck network operations globally. Network operations submitted between Thursday, 2019-10-31 16:41 US/Pacific and Thursday, 2019-10-31 23:01 US/Pacific may be affected. \n\nNew operations are succeeding as expected currently and we are currently working to clear a back log of pending operations in our system.\n\nWe will provide more information by Friday, 2019-11-01 12:30 US/Pacific.\n\nDiagnosis: Customer may have encountered errors across the below products if affected.\n\nGoogle Compute Engine\n- Networking-related Compute API operations pending to complete if submitted during the above time.\n- Resubmitting similar requests may fail as they are waiting for the above operations to complete.\n- The affected operations include: deleting backend services, subnets, instance groups, routes and firewall rules.\n- Some operations may still show as pending and are being mitigated at this time. We expect this current mitigation work to be completed no later than 2019-11-01 12:30 PDT\n\nGoogle Kubernetes Engine\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Compute Engine. No further updates will be provided for Google Kubernetes Engine in this post.\n\nGoogle Cloud Memorystore\n- This issue is believed to have affected less than 1% of projects\n- The affected projects should find full resolution once the issue affecting Google Compute Engine is resolved. No further updates will be provided for Google Cloud Memorystore\n\nApp Engine Flexible \n- New deployments experienced elevated failure rates during the affected time. \n- The team is no longer seeing issues affecting new deployment creation and we feel the incident for this product is now resolved. No further updates will be provided for App Engine Flexible \n\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T17:13:29Z"}, {"created": "2019-11-01T17:05:38Z", "modified": "2019-11-01T17:05:38Z", "text": "Description: Mitigation work is currently underway by our product team to unblock stuck network operations globally. Network operations submitted between Thursday, 2019-10-31 16:41 US/Pacific and Thursday, 2019-10-31 23:01 US/Pacific may be affected. \n\nNew operations are showing a reduction in failures currently and we are currently working to clear a back log of pending operations in our system.\n\nWe will provide more information by Friday, 2019-11-01 12:00 US/Pacific.\n\nDiagnosis: Customer may have encountered errors across the below products if affected.\n\nGoogle Compute Engine\n- Networking-related Compute API operations pending to complete if submitted during the above time.\n- The affected operations include: deleting backend services, subnets, instance groups, routes and firewall rules.\n- Some operations may still show as pending and are being mitigated at this time. We expect this current mitigation work to be completed no later than 2019-11-01 12:30 PDT\n\nGoogle Kubernetes Engine\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this are underway as part of the mitigation mentioned under Google Compute Engine. No further updates will be provided for Google Kubernetes Engine in this post.\n\nGoogle Cloud Memorystore\n- This issue is believed to have affected less than 1% of projects\n- The affected projects should find full resolution once the issue affecting Google Compute Engine is resolved. No further updates will be provided for Google Cloud Memorystore\n\nApp Engine Flexible \n- New deployments experienced elevated failure rates during the affected time. \n- The team is no longer seeing issues affecting new deployment creation and we feel the incident for this product is now resolved. No further updates will be provided for App Engine Flexible \n\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T17:05:38Z"}, {"created": "2019-11-01T16:06:42Z", "modified": "2019-11-01T16:06:42Z", "text": "Description: Mitigation work is currently underway by our product team to unblock stuck network operations globally. Network operations submitted between Thursday, 2019-10-31 16:41 US/Pacific and Thursday, 2019-10-31 23:01 US/Pacific may be affected. \n\nNew operations are showing a reduction in failures currently and we are currently working to clear a back log of pending operations in our system.\n\nWe will provide more information by Friday, 2019-11-01 12:00 US/Pacific.\n\nDiagnosis: Customer may have encountered errors across the below products if affected.\n\nGoogle Compute Engine\n- Networking-related Compute API operations pending to complete if submitted during the above time.\n- The affected operations include: deleting backend services, subnets, instance groups, routes and firewall rules.\n- Some operations may still show as pending and are being mitigated at this time. We expect this current mitigation work to be completed no later than 2019-11-01 12:30 PDT\n\nGoogle Kubernetes Engine\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on recovering from this can be found [https://status.cloud.google.com/incident/container-engine/19011](https://status.cloud.google.com/incident/container-engine/19011). No further updates will be provided for Google Kubernetes Engine in this post.\n\nGoogle Cloud Memorystore\n- This issue is believed to have affected less than 1% of projects\n- The affected projects should find full resolution once the issue affecting Google Compute Engine is resolved. No further updates will be provided for Google Cloud Memorystore\n\nApp Engine Flexible \n- New deployments experienced elevated failure rates during the affected time. \n- The team is no longer seeing issues affecting new deployment creation and we feel the incident for this product is now resolved. No further updates will be provided for App Engine Flexible \n\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T16:06:42Z"}, {"created": "2019-11-01T15:50:16Z", "modified": "2019-11-01T15:50:16Z", "text": "Description: Mitigation work is currently underway by our product team to unblock stuck network operations globally. Network operations submitted between Thursday, 2019-10-31 16:41 US/Pacific and Thursday, 2019-10-31 23:01 US/Pacific may be affected. \n\nNew operations are showing a reduction in failures currently and we are currently working to clear a back log of pending operations in our system.\n\nWe will provide more information by Friday, 2019-11-01 10:00 US/Pacific.\n\nDiagnosis: Customer may be seeing errors across the below products if affected.\n\nGoogle Compute Engine\n- Networking-related Compute API operations failing to complete if submitted during the above time.\n- This may include deleting backend services, subnets, instance groups, routes and firewall rules.\n\nGoogle Kubernetes Engine\n- Cluster operations including creation, update, auto scaling may have failed due to the networking API failures mentioned under Google Compute Engine\n- New Cluster operations are now succeeding and further updates on restoring this can be found [https://status.cloud.google.com/incident/container-engine/19011](https://status.cloud.google.com/incident/container-engine/19011)\n\nGoogle Cloud Memorystore\n- Create/Delete events failed during the above time\n\nApp Engine Flexible \n- Deployments seeing elevated failure rates\n\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T15:50:16Z"}, {"created": "2019-11-01T15:28:23Z", "modified": "2019-11-01T15:28:23Z", "text": "Description: Mitigation work is currently underway by our product team to address the ongoing issue with some network operations failing globally at this time. These reports started Thursday, 2019-10-31 16:41 US/Pacific. Operations are showing a reduction in failures currently and we are currently working to clear a back log of stuck operations in our system.\n\nWe will provide more information by Friday, 2019-11-01 09:30 US/Pacific.\n\nDiagnosis: Customer may experience errors with the below products if affected \n\nGoogle Compute Engine\n * Networking-related Compute API operations failing\n * This may include deleting backend services, subnets, instance groups, routes and firewall rules and more.\n\nGoogle Kubernetes Engine\n * Cluster operations including creation, update, auto scaling may fail due to the networking API failures \n\nGoogle Cloud Memorystore\n * Create/Delete events failing\n\nApp Engine Flexible \n * Deployments seeing elevated failure rates\n\n\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T15:28:23Z"}, {"created": "2019-11-01T14:15:05Z", "modified": "2019-11-01T14:15:05Z", "text": "Description: Mitigation work is still underway by our engineering team. \n\nWe will provide more information by Friday, 2019-11-01 08:30 US/Pacific.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups, routes and firewall rules.\nCloud Armor rules might not be updated.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T14:15:05Z"}, {"created": "2019-11-01T12:51:50Z", "modified": "2019-11-01T12:51:50Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 07:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups, routes and firewall rules.\nCloud Armor rules might not be updated.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T12:51:50Z"}, {"created": "2019-11-01T11:56:11Z", "modified": "2019-11-01T11:56:11Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 06:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\nNew GKE nodes creation might fail with NetworkUnavailable status set to True.\nCloud Armor rules might not be updated.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T11:56:11Z"}, {"created": "2019-11-01T10:54:21Z", "modified": "2019-11-01T10:54:21Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 05:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T10:54:21Z"}, {"created": "2019-11-01T09:55:06Z", "modified": "2019-11-01T09:55:06Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 04:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T09:55:06Z"}, {"created": "2019-11-01T08:53:18Z", "modified": "2019-11-01T08:53:18Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 03:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T08:53:18Z"}, {"created": "2019-11-01T06:51:11Z", "modified": "2019-11-01T06:51:11Z", "text": "Description: Our engineering team still investigating the issue.\n\nWe will provide an update by Friday, 2019-11-01 02:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T06:51:11Z"}, {"created": "2019-11-01T05:54:26Z", "modified": "2019-11-01T05:54:26Z", "text": "Description: Our engineering team has determined that further investigation is required to mitigate the issue.\n\nWe will provide an update by Thursday, 2019-10-31 23:50 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T05:54:26Z"}, {"created": "2019-11-01T05:06:46Z", "modified": "2019-11-01T05:06:46Z", "text": "Description: We observing recurrence of the issue. The engineering team continues the investigation.\n\nWe will provide an update by Thursday, 2019-10-31 23:00 US/Pacific with current details.\n\nDiagnosis: Customer may experience errors while creating or deleting backend services, subnets, instance groups and firewall rules.\n\nWorkaround: No workaround is available at the moment", "when": "2019-11-01T05:06:46Z"}], "uri": "/incident/compute/19008"}, {"begin": "2019-10-23T00:38:00Z", "created": "2019-10-23T01:17:51Z", "end": "2019-10-23T04:21:00Z", "external_desc": "We are investigating an issue with Google Compute Engine.", "modified": "2019-10-23T04:37:27Z", "most-recent-update": {"created": "2019-10-23T01:17:51Z", "modified": "2019-10-23T01:17:51Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:17:51Z"}, "number": 19007, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "high", "updates": [{"created": "2019-10-23T01:17:51Z", "modified": "2019-10-23T01:17:51Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:17:51Z"}], "uri": "/incident/compute/19007"}, {"begin": "2019-10-23T00:14:00Z", "created": "2019-10-23T01:02:30Z", "end": "2019-10-23T02:05:00Z", "external_desc": "We are investigating an issue with Cloud Memorystore.", "modified": "2019-10-23T02:58:21Z", "most-recent-update": {"created": "2019-10-23T01:02:30Z", "modified": "2019-10-23T01:02:30Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:02:30Z"}, "number": 19001, "public": true, "service_key": "cloud-memorystore", "service_name": "Cloud Memorystore", "severity": "medium", "updates": [{"created": "2019-10-23T01:02:30Z", "modified": "2019-10-23T01:02:30Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:02:30Z"}], "uri": "/incident/cloud-memorystore/19001"}, {"begin": "2019-10-23T00:11:00Z", "created": "2019-10-23T01:06:32Z", "end": "2019-10-23T02:42:00Z", "external_desc": "We are investigating an issue with Google Kubernetes Engine.", "modified": "2019-10-23T03:03:56Z", "most-recent-update": {"created": "2019-10-23T01:06:32Z", "modified": "2019-10-23T01:06:32Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:06:32Z"}, "number": 19010, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-10-23T01:06:32Z", "modified": "2019-10-23T01:06:32Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:06:32Z"}], "uri": "/incident/container-engine/19010"}, {"begin": "2019-10-23T00:11:00Z", "created": "2019-10-23T01:10:49Z", "end": "2019-10-23T03:04:00Z", "external_desc": "We are investigating an issue with Google Cloud Storage.", "modified": "2019-10-23T03:07:51Z", "most-recent-update": {"created": "2019-10-23T01:10:49Z", "modified": "2019-10-23T01:10:49Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:10:49Z"}, "number": 19008, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-10-23T01:10:49Z", "modified": "2019-10-23T01:10:49Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:10:49Z"}], "uri": "/incident/storage/19008"}, {"begin": "2019-10-23T00:04:00Z", "created": "2019-10-23T01:21:18Z", "end": "2019-10-23T03:04:00Z", "external_desc": "We are investigating an issue with Cloud Bigtable.", "modified": "2019-10-23T03:05:47Z", "most-recent-update": {"created": "2019-10-23T01:21:18Z", "modified": "2019-10-23T01:21:18Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:21:18Z"}, "number": 19001, "public": true, "service_key": "cloud-bigtable", "service_name": "Google Cloud Bigtable", "severity": "medium", "updates": [{"created": "2019-10-23T01:21:18Z", "modified": "2019-10-23T01:21:18Z", "text": "Our engineers have determined this issue to be linked to the Google Cloud Networking incident in us-west1-b.  For regular status updates, please visit (https://status.cloud.google.com/incident/cloud-networking/19020).  No further updates will be made through this incident.", "when": "2019-10-23T01:21:18Z"}], "uri": "/incident/cloud-bigtable/19001"}, {"begin": "2019-10-22T23:47:00Z", "created": "2019-10-23T00:36:15Z", "end": "2019-10-23T04:35:53Z", "external_desc": "We've received a report of an issue with Cloud Networking.", "modified": "2019-10-31T19:08:34Z", "most-recent-update": {"created": "2019-10-31T19:07:58Z", "modified": "2019-10-31T19:08:34Z", "text": "# ISSUE SUMMARY\r\nOn Tuesday 22 October, 2019, Google Compute Engine experienced 100% packet loss to and from ~20% of instances in us-west1-b for a duration of 2 hours, 31 minutes. Additionally, 20% of Cloud Routers, and 6% of Cloud VPN gateways experienced equivalent packet loss in us-west1. Specific service impact is outlined in detail below. We apologize to our customers whose services or businesses were impacted during this incident, and we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Tuesday 22 October, 2019 from 16:20 to 18:51 US/Pacific, the Google Cloud Networking control plane in us-west1-b experienced failures in programming Google Cloud's virtualized networking stack. This means that new or migrated instances would have been unable to obtain network addresses and routes, making them unavailable  Existing instances should have seen no impact; however, an additional software bug, triggered by the programming failure, caused 100% packet loss to 20% of existing instances in this zone. Impact in the us-west1 region for specific services is outlined below:\r\n\r\n## Compute Engine\r\nGoogle Compute Engine experienced 100% packet loss to 20% of instances in us-west1-b. Additionally, creation of new instances in this zone failed, while existing instances that were live migrated during the incident would have experienced 100% packet loss.\r\n\r\n## Cloud VPN\r\nGoogle Cloud VPN experienced failures creating new or modifying existing gateways in us-west1. Additionally, 6% of existing gateways experienced 100% packet loss.\r\n\r\n## Cloud Router\r\nGoogle Cloud Router experienced failures creating new or modifying existing routes in us-west1. Additionally, 20% of existing cloud routers experienced 100% packet loss.\r\n\r\n## Cloud Memorystore\r\n<1% of Google Cloud Memorystore instances in us-west1 were unreachable, and operations to create new instances failed. This affected basic tier instances and standard tier instances with the primary node in the affected zone. None of the affected instances experienced a cache flush, and impacted instances resumed normal operations as soon as the network was restored. \r\n\r\n## Kubernetes Engine\r\nGoogle Kubernetes Engine clusters in us-west1 may have reported as unhealthy due to packet loss to and from the nodes and master, which may have triggered unnecessary node repair operations. ~1% of clusters were affected of which most were Zonal clusters in us-west1-b. Some regional clusters in us-west1 may have been briefly impacted if the elected etcd leader for the master was in us-west1-b, until it was re-elected. \r\n\r\n## Cloud Bigtable \r\nGoogle Cloud Bigtable customers in us-west1-b without high availability replication and routing configured, would have experienced a high error rate. High Availability configurations had their traffic routed around the impact zone, and may have experienced a short period of increased latency. \r\n\r\n## Cloud SQL\r\nGoogle Cloud SQL instances in us-west1 may have been temporarily unavailable. ~1% of instances were affected during the incident.  \r\n\r\n\r\n# ROOT CAUSE\r\nGoogle Cloud Networking consists of a software stack which is made up of two components, a control plane and data plane. The data plane is where packets are processed and routed based on the configuration set up by the control plane. Each zone has its own control plane service, and each control plane service is sharded such that network programming is spread across multiple shards. Additionally, each shard is made up of several leader elected [1] processes.\r\n\r\nDuring this incident, a failure in the underlying leader election system (Chubby [2]) resulted in components in the control plane losing and gaining leadership in short succession. These frequent leadership changes halted network programming, preventing VM instances from being created or modified.\r\n\r\nGoogle\u2019s standard defense-in-depth philosophy means that existing network routes should continue to work normally when programming fails. The impact to existing instances was a result of this defense-in-depth failing: a race condition in the code which handles leadership changes caused programming updates to contain invalid configurations, resulting in packet loss for impacted instances. This particular bug has already been fixed, and a rollout of this fix was coincidentally in progress at the time of the outage.\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\nGoogle engineers were alerted to the problem at 16:30 US/Pacific and immediately began investigating. Mitigation efforts began at 17:20 which involved a combination of actions including rate limits, forcing leader election, and redirection of traffic. These efforts gradually reduced the rate of packet loss, which eventually led to a full recovery of the networking control plane by 18:51. \r\n\r\nIn order to increase the reliability of Cloud Networking, we will be taking these immediate steps to prevent a recurrence:\r\n\r\nWe will complete the rollout of the fix for the race condition during leadership election which resulted in incorrect configuration being distributed.\r\nWe will harden the components which process that configuration such that they reject obviously invalid configuration.\r\nWe will improve incident response tooling used in this particular failure case to reduce time to recover.\r\n\r\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\r\n\r\n\r\n[1] https://landing.google.com/sre/sre-book/chapters/managing-critical-state/#highly-available-processing-using-leader-election\r\n[2] https://ai.google/research/pubs/pub27897", "when": "2019-10-31T19:07:58Z"}, "number": 19020, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "high", "updates": [{"created": "2019-10-31T19:07:58Z", "modified": "2019-10-31T19:08:34Z", "text": "# ISSUE SUMMARY\r\nOn Tuesday 22 October, 2019, Google Compute Engine experienced 100% packet loss to and from ~20% of instances in us-west1-b for a duration of 2 hours, 31 minutes. Additionally, 20% of Cloud Routers, and 6% of Cloud VPN gateways experienced equivalent packet loss in us-west1. Specific service impact is outlined in detail below. We apologize to our customers whose services or businesses were impacted during this incident, and we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Tuesday 22 October, 2019 from 16:20 to 18:51 US/Pacific, the Google Cloud Networking control plane in us-west1-b experienced failures in programming Google Cloud's virtualized networking stack. This means that new or migrated instances would have been unable to obtain network addresses and routes, making them unavailable  Existing instances should have seen no impact; however, an additional software bug, triggered by the programming failure, caused 100% packet loss to 20% of existing instances in this zone. Impact in the us-west1 region for specific services is outlined below:\r\n\r\n## Compute Engine\r\nGoogle Compute Engine experienced 100% packet loss to 20% of instances in us-west1-b. Additionally, creation of new instances in this zone failed, while existing instances that were live migrated during the incident would have experienced 100% packet loss.\r\n\r\n## Cloud VPN\r\nGoogle Cloud VPN experienced failures creating new or modifying existing gateways in us-west1. Additionally, 6% of existing gateways experienced 100% packet loss.\r\n\r\n## Cloud Router\r\nGoogle Cloud Router experienced failures creating new or modifying existing routes in us-west1. Additionally, 20% of existing cloud routers experienced 100% packet loss.\r\n\r\n## Cloud Memorystore\r\n<1% of Google Cloud Memorystore instances in us-west1 were unreachable, and operations to create new instances failed. This affected basic tier instances and standard tier instances with the primary node in the affected zone. None of the affected instances experienced a cache flush, and impacted instances resumed normal operations as soon as the network was restored. \r\n\r\n## Kubernetes Engine\r\nGoogle Kubernetes Engine clusters in us-west1 may have reported as unhealthy due to packet loss to and from the nodes and master, which may have triggered unnecessary node repair operations. ~1% of clusters were affected of which most were Zonal clusters in us-west1-b. Some regional clusters in us-west1 may have been briefly impacted if the elected etcd leader for the master was in us-west1-b, until it was re-elected. \r\n\r\n## Cloud Bigtable \r\nGoogle Cloud Bigtable customers in us-west1-b without high availability replication and routing configured, would have experienced a high error rate. High Availability configurations had their traffic routed around the impact zone, and may have experienced a short period of increased latency. \r\n\r\n## Cloud SQL\r\nGoogle Cloud SQL instances in us-west1 may have been temporarily unavailable. ~1% of instances were affected during the incident.  \r\n\r\n\r\n# ROOT CAUSE\r\nGoogle Cloud Networking consists of a software stack which is made up of two components, a control plane and data plane. The data plane is where packets are processed and routed based on the configuration set up by the control plane. Each zone has its own control plane service, and each control plane service is sharded such that network programming is spread across multiple shards. Additionally, each shard is made up of several leader elected [1] processes.\r\n\r\nDuring this incident, a failure in the underlying leader election system (Chubby [2]) resulted in components in the control plane losing and gaining leadership in short succession. These frequent leadership changes halted network programming, preventing VM instances from being created or modified.\r\n\r\nGoogle\u2019s standard defense-in-depth philosophy means that existing network routes should continue to work normally when programming fails. The impact to existing instances was a result of this defense-in-depth failing: a race condition in the code which handles leadership changes caused programming updates to contain invalid configurations, resulting in packet loss for impacted instances. This particular bug has already been fixed, and a rollout of this fix was coincidentally in progress at the time of the outage.\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\nGoogle engineers were alerted to the problem at 16:30 US/Pacific and immediately began investigating. Mitigation efforts began at 17:20 which involved a combination of actions including rate limits, forcing leader election, and redirection of traffic. These efforts gradually reduced the rate of packet loss, which eventually led to a full recovery of the networking control plane by 18:51. \r\n\r\nIn order to increase the reliability of Cloud Networking, we will be taking these immediate steps to prevent a recurrence:\r\n\r\nWe will complete the rollout of the fix for the race condition during leadership election which resulted in incorrect configuration being distributed.\r\nWe will harden the components which process that configuration such that they reject obviously invalid configuration.\r\nWe will improve incident response tooling used in this particular failure case to reduce time to recover.\r\n\r\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\r\n\r\n\r\n[1] https://landing.google.com/sre/sre-book/chapters/managing-critical-state/#highly-available-processing-using-leader-election\r\n[2] https://ai.google/research/pubs/pub27897", "when": "2019-10-31T19:07:58Z"}, {"created": "2019-10-23T04:35:53Z", "modified": "2019-10-23T04:35:53Z", "text": "The issue with Cloud Networking has been resolved for all affected projects as of Tuesday, 2019-10-22 19:19 US/Pacific.\r\n\r\nWe will publish analysis of this incident once we have completed our internal investigation.\r\n\r\nWe thank you for your patience while we've worked on resolving the issue.", "when": "2019-10-23T04:35:53Z"}, {"created": "2019-10-23T03:23:10Z", "modified": "2019-10-23T03:23:10Z", "text": "Summary: We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage\r\n\r\nDescription: We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage starting around Tuesday, 2019-10-22 16:47 US/Pacific. Mitigation work has been completed by our Engineering Team and services such as Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage have recovered. We will provide another status update by Tuesday. 2019-10-22 23:00 US/Pacific with current details.\r\n\r\nDetails:\r\n\r\nCloud Networking - Network programming and packet loss levels have recovered, but not all jobs have recovered.\r\n\r\nGoogle Compute Engine - Packet loss behavior is starting to recover. There may be some remaining stuck VMs that are not reachable by SSH but the engineering team is working on fixing this.", "when": "2019-10-23T03:23:10Z"}, {"created": "2019-10-23T02:50:09Z", "modified": "2019-10-23T02:50:09Z", "text": "Summary: We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage\r\n\r\nDescription: We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage starting around Tuesday, 2019-10-22 16:47 US/Pacific. Mitigation work has been completed by our Engineering Team and some services are starting to recover. We will provide another status update by Tuesday. 2019-10-22 22:00 US/Pacific with current details.\r\n\r\nDetails:\r\n\r\nCloud Networking - Network programming and packet loss levels have recovered, but not all jobs have recovered.\r\n\r\nGoogle Compute Engine - Packet loss behavior is starting to recover. There may be some remaining stuck VMs that are not reachable by SSH but the engineering team is working on fixing this.\r\n\r\nCloud Memorystore - Redis instances are recovering in us-west1-b.\r\n\r\nGoogle Kubernetes Engine - Situation has recovered.\r\n\r\nCloud Bigtable - We are still seeing elevated levels of latency and errors in us-west1-b. Our engineering team is still working on recovery.\r\n\r\nGoogle Cloud Storage - Services in us-west1-b should be recovering.", "when": "2019-10-23T02:50:09Z"}, {"created": "2019-10-23T01:33:44Z", "modified": "2019-10-23T01:36:10Z", "text": "Summary:  We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage\r\n\r\nDescription: We've received a report of issues with multiple Cloud products including Google Compute Engine, Cloud Memorystore, Google Kubernetes Engine, Cloud Bigtable and Google Cloud Storage starting around Tuesday, 2019-10-22 16:47 US/Pacific. Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday. 2019-10-22 19:00 US/Pacific with current details.\r\n\r\nDiagnosis:\r\n\r\nCloud Networking - Affected customers will experience packet loss  or network programming changes fail to complete in us-west1-b.\r\n\r\nGoogle Compute Engine - Unpredictable behavior due to packet loss such as failure to SSH to VMs in us-west1-b.\r\n\r\nCloud Memorystore - New instances cannot be created and existing instances might not be reachable in us-west1-b.\r\n\r\nGoogle Kubernetes Engine - Regional clusters in us-west1 are impacted, zonal clusters in us-west1-b are impacted.\r\n\r\nCloud Bigtable - Elevated latency and errors in us-west1-b.\r\n\r\nGoogle Cloud Storage - Service disruption in us-west1-b.", "when": "2019-10-23T01:33:43Z"}, {"created": "2019-10-23T00:36:15Z", "modified": "2019-10-23T00:36:15Z", "text": "Description: We are experiencing an issue with Cloud Networking in us-west1. Our engineering team continues to investigate the issue. We will provide an update by Tuesday, 2019-10-22 17:41 US/Pacific with current details. We apologize to all who are affected by the disruption.\r\n\r\nDiagnosis: Affected customers will experience network programming changes fail to complete in us-west1-b.\r\n\r\nWorkaround: None at this time.", "when": "2019-10-23T00:36:15Z"}], "uri": "/incident/cloud-networking/19020"}, {"begin": "2019-10-11T15:55:51Z", "created": "2019-10-11T18:59:50Z", "end": "2019-10-11T20:21:00Z", "external_desc": "BigQuery ingestion errors in eu-multiregion", "modified": "2019-10-12T03:20:04Z", "most-recent-update": {"created": "2019-10-12T03:20:03Z", "modified": "2019-10-12T03:20:03Z", "text": "The issue with ingestion and incorrect queries is believed to be affecting a very small number of projects, and has been isolated. Our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-10-11T20:21:00Z"}, "number": 19008, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-10-12T03:20:03Z", "modified": "2019-10-12T03:20:03Z", "text": "The issue with ingestion and incorrect queries is believed to be affecting a very small number of projects, and has been isolated. Our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-10-11T20:21:00Z"}, {"created": "2019-10-11T18:59:51Z", "modified": "2019-10-11T18:59:51Z", "text": "Description: Our team has found Ingestion issues isolated to a single bad system in the eu-multiregion starting around Friday, 2019-10-11 08:55 US/Pacific. This system has since been isolated to avoid further issues with ingestion. \n\nA second mitigation stream is currently underway by our engineering team to resolve reports of data unavailable errors for users affected by the first issue.\n\nWe do not have an ETA for mitigation at this point.\n\nWe will provide more information by Friday, 2019-10-11 13:30 US/Pacific.\n\nDiagnosis: Affected customers may see queries return STREAMING_DATA_UNAVAILABLE.\n\nWorkaround: New ingestions should not be delayed at this time. If a new job is submitted it should complete as expected.\n\nNo work around for the STREAMING_DATA_UNAVAILABLE at this time.", "when": "2019-10-11T18:59:51Z"}], "uri": "/incident/bigquery/19008"}, {"begin": "2019-10-10T20:14:11Z", "created": "2019-10-10T20:27:16Z", "end": "2019-10-10T22:25:01Z", "external_desc": "Some Google Kubernetes Clusters are experiencing errors in us-central1,  us-west1, and us-west2", "modified": "2019-10-10T22:25:02Z", "most-recent-update": {"created": "2019-10-10T22:25:02Z", "modified": "2019-10-10T22:25:02Z", "text": "Closing Message: Our engineering team is still investigating root cause and impact. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-10-10T22:25:01Z"}, "number": 19009, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-10-10T22:25:02Z", "modified": "2019-10-10T22:25:02Z", "text": "Closing Message: Our engineering team is still investigating root cause and impact. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-10-10T22:25:01Z"}, {"created": "2019-10-10T21:26:03Z", "modified": "2019-10-10T21:26:03Z", "text": "Description: We are seeing an increase in errors to requests made to the GKE master API in us-central1,  us-west1, and us-west2.\n\nExisting workloads will continue to run.\n\nOur engineering team continues to investigate the issue.\n\nWe will provide an update by Thursday, 2019-10-10 15:15 US/Pacific with current details.\n\n\nDiagnosis: None at the moment\n\nWorkaround: Resend requests to master API with a back off if you are receiving errors.", "when": "2019-10-10T21:26:03Z"}, {"created": "2019-10-10T20:27:17Z", "modified": "2019-10-10T20:27:17Z", "text": "Description: We are investigating an issue with Google Kubernetes Engine. \n\nGoogle Kubernetes Clusters are unavailable in us-central1-*, us-west1*, and us-west2-*.\n\nWe will provide more information by Thursday, 2019-10-10 14:30 US/Pacific.\n\nDiagnosis: None at the moment\n\nWorkaround: None at the moment", "when": "2019-10-10T20:27:17Z"}], "uri": "/incident/container-engine/19009"}, {"begin": "2019-09-30T17:16:37Z", "created": "2019-09-30T17:35:41Z", "end": "2019-09-30T17:59:33Z", "external_desc": "Google BigQuery is experiencing query failures with US multi-regional datasets", "modified": "2019-09-30T17:59:33Z", "most-recent-update": {"created": "2019-09-30T17:59:33Z", "modified": "2019-09-30T17:59:33Z", "text": "The issue with Google BigQuery 15% failure of queries, imports, and exports on datasets located in the US multi-region has been resolved as of Monday, 2019-09-30 10:40 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-30T17:59:33Z"}, "number": 19007, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-09-30T17:59:33Z", "modified": "2019-09-30T17:59:33Z", "text": "The issue with Google BigQuery 15% failure of queries, imports, and exports on datasets located in the US multi-region has been resolved as of Monday, 2019-09-30 10:40 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-30T17:59:33Z"}, {"created": "2019-09-30T17:35:41Z", "modified": "2019-09-30T17:35:41Z", "text": "Description: The Google BigQuery service is experiencing a 15% error rate on queries for multi-regional datasets located in the US multi-region. We will provide another status update by Monday, 2019-09-30 11:15 US/Pacific with current details.\n\nDiagnosis: Affected users will see some queries on their US multi-regional datasets will fail.\n\nWorkaround: None at this time", "when": "2019-09-30T17:35:41Z"}], "uri": "/incident/bigquery/19007"}, {"begin": "2019-09-24T20:07:00Z", "created": "2019-09-24T23:20:17Z", "end": "2019-09-25T01:15:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T13:59:18Z", "most-recent-update": {"created": "2019-09-24T23:20:17Z", "modified": "2019-09-27T13:59:18Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:20:17Z"}, "number": 19011, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-09-24T23:20:17Z", "modified": "2019-09-27T13:59:18Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:20:17Z"}], "uri": "/incident/appengine/19011"}, {"begin": "2019-09-24T20:07:00Z", "created": "2019-09-24T23:27:56Z", "end": "2019-09-25T01:18:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T14:07:52Z", "most-recent-update": {"created": "2019-09-24T23:27:56Z", "modified": "2019-09-27T14:07:52Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:27:56Z"}, "number": 19001, "public": true, "service_key": "cloud-dns", "service_name": "Google Cloud DNS", "severity": "medium", "updates": [{"created": "2019-09-24T23:27:56Z", "modified": "2019-09-27T14:07:52Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:27:56Z"}], "uri": "/incident/cloud-dns/19001"}, {"begin": "2019-09-24T20:07:00Z", "created": "2019-09-26T08:46:05Z", "end": "2019-09-25T01:15:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T14:06:18Z", "most-recent-update": {"created": "2019-09-26T08:46:05Z", "modified": "2019-09-27T14:06:18Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-26T08:46:05Z"}, "number": 19003, "public": true, "service_key": "cloud-firestore", "service_name": "Cloud Firestore", "severity": "medium", "updates": [{"created": "2019-09-26T08:46:05Z", "modified": "2019-09-27T14:06:18Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-26T08:46:05Z"}], "uri": "/incident/cloud-firestore/19003"}, {"begin": "2019-09-24T20:07:00Z", "created": "2019-09-24T23:21:43Z", "end": "2019-09-25T01:15:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T13:58:55Z", "most-recent-update": {"created": "2019-09-24T23:21:43Z", "modified": "2019-09-27T13:58:55Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:21:43Z"}, "number": 19006, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-09-24T23:21:43Z", "modified": "2019-09-27T13:58:55Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:21:43Z"}], "uri": "/incident/compute/19006"}, {"begin": "2019-09-24T20:00:00Z", "created": "2019-09-25T00:45:31Z", "end": "2019-09-25T01:56:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T14:02:31Z", "most-recent-update": {"created": "2019-09-25T00:45:31Z", "modified": "2019-09-27T14:02:31Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T00:45:31Z"}, "number": 19003, "public": true, "service_key": "cloud-dataflow", "service_name": "Google Cloud Dataflow", "severity": "medium", "updates": [{"created": "2019-09-25T00:45:31Z", "modified": "2019-09-27T14:02:31Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T00:45:31Z"}], "uri": "/incident/cloud-dataflow/19003"}, {"begin": "2019-09-24T20:00:00Z", "created": "2019-09-25T01:04:47Z", "end": "2019-09-25T04:25:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T14:02:52Z", "most-recent-update": {"created": "2019-09-25T01:04:47Z", "modified": "2019-09-27T14:02:52Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T01:04:47Z"}, "number": 19002, "public": true, "service_key": "cloud-dataproc", "service_name": "Google Cloud Dataproc", "severity": "medium", "updates": [{"created": "2019-09-25T01:04:47Z", "modified": "2019-09-27T14:02:52Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T01:04:47Z"}], "uri": "/incident/cloud-dataproc/19002"}, {"begin": "2019-09-24T20:00:00Z", "created": "2019-09-24T23:19:20Z", "end": "2019-09-24T23:00:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-11-05T19:05:36Z", "most-recent-update": {"created": "2019-09-24T23:19:20Z", "modified": "2019-09-27T14:00:38Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:19:20Z"}, "number": 19002, "public": true, "service_key": "cloud-dev-tools", "service_name": "Cloud Developer Tools", "severity": "medium", "updates": [{"created": "2019-09-24T23:19:20Z", "modified": "2019-09-27T14:00:38Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:19:20Z"}], "uri": "/incident/cloud-dev-tools/19002"}, {"begin": "2019-09-24T20:00:00Z", "created": "2019-09-25T00:47:39Z", "end": "2019-09-25T01:15:00Z", "external_desc": "We're experiencing issues with Google Cloud Functions", "modified": "2019-09-27T14:02:10Z", "most-recent-update": {"created": "2019-09-25T00:47:39Z", "modified": "2019-09-27T14:02:10Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T00:47:39Z"}, "number": 19008, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-09-25T00:47:39Z", "modified": "2019-09-27T14:02:10Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-25T00:47:39Z"}], "uri": "/incident/cloud-functions/19008"}, {"begin": "2019-09-24T20:00:00Z", "created": "2019-09-24T23:21:51Z", "end": "2019-09-25T01:49:00Z", "external_desc": "We've received a report of issues with multiple Cloud products.", "modified": "2019-09-27T14:01:11Z", "most-recent-update": {"created": "2019-09-24T23:21:51Z", "modified": "2019-09-27T14:01:10Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:21:51Z"}, "number": 19002, "public": true, "service_key": "composer", "service_name": "Google Cloud Composer", "severity": "medium", "updates": [{"created": "2019-09-24T23:21:51Z", "modified": "2019-09-27T14:01:10Z", "text": "Our engineers have determined this issue to be linked to a single Google incident.  For regular status updates, please visit [https://status.cloud.google.com/incident/google-stackdriver/19007](https://status.cloud.google.com/incident/google-stackdriver/19007).  No further updates will be made through this incident.", "when": "2019-09-24T23:21:51Z"}], "uri": "/incident/composer/19002"}, {"begin": "2019-09-24T19:46:35Z", "created": "2019-09-24T20:30:01Z", "end": "2019-09-25T03:00:33Z", "external_desc": "We've received a report of issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, and Cloud Firestore", "modified": "2019-10-08T17:44:03Z", "most-recent-update": {"created": "2019-09-27T22:38:22Z", "modified": "2019-10-08T17:44:02Z", "text": "## ISSUE SUMMARY\r\n\r\nOn Tuesday 24 September, 2019, the following Google Cloud Platform services were partially impacted by an overload condition in an internal publish/subscribe messaging system which is a dependency of these products: App Engine, Compute Engine, Kubernetes Engine, Cloud Build, Cloud Composer, Cloud Dataflow, Cloud Dataproc, Cloud Firestore, Cloud Functions, Cloud DNS, Cloud Run, and Stackdriver Logging & Monitoring. Impact was limited to administrative operations for a number of these products, with existing workloads and instances not affected in most cases. \r\n\r\nWe apologize to those customers whose services were impacted during this incident; we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\n## DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 24 September, 2019 from 12:46 to 20:00 US/Pacific, Google Cloud Platform experienced a partial disruption to multiple services with their respective impacts detailed below:\r\n\r\n### App Engine\r\nGoogle App Engine (GAE) create, update, and delete admin operations failed globally from 12:57 to 18:21 for a duration of 5 hours and 24 minutes. Affected customers may have seen error messages like \u201cAPP_ERROR\u201d. Existing GAE workloads were unaffected.\r\n\r\n### Compute Engine\r\nGoogle Compute Engine (GCE) instances failed to start in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes, and GCE Internal DNS in us-central1, us-east1, and us-east4 experienced delays for newly created hostnames to become resolvable. Existing GCE instances and hostnames were unaffected.\r\n\r\n### Kubernetes Engine\r\nGoogle Kubernetes Engine (GKE) experienced delayed resource metadata and inaccurate Stackdriver Monitoring for cluster metrics globally. Additionally, cluster creation operations failed in us-central1-a from 3:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on GCE instance creation. Most existing GKE clusters were unaffected by the GCE instance creation failures, except for clusters in us-central1-a that were may have been unable to repair nodes or scale a node pool.\r\n\r\n### Stackdriver Logging & Monitoring\r\nStackdriver Logging experienced delays of up to two hours for logging events generated globally. Exports were delayed by up to 3 hours and 30 minutes. Some user requests to write logs in us-central1 failed. Some logs-based metric monitoring charts displayed lower counts, and queries to Stackdriver Logging briefly experienced a period of 50% error rates. The impact to Stackdriver Logging & Monitoring took place from 12:54 to 18:45 for a total duration of 5 hours and 51 minutes.\r\n\r\n### Cloud Functions\r\nCloud Functions deployments failed globally from 12:57 to 18:21 and experienced peak error rates of 13% in us-east1 and 80% in us-central1 from 19:12 to 19:57 for a combined duration of 6 hours and 15 minutes. Existing Cloud Function deployments were unaffected.\r\n\r\n### Cloud Build\r\nCloud Build failed to update build status for GitHub App triggers from 12:54 to 16:00 for a duration of 3 hours and 6 minutes.\r\n\r\n### Cloud Composer\r\nCloud Composer environment creations failed globally from 13:25 to 18:05 for a duration of 4 hours and 40 minutes. Existing Cloud Composer clusters were unaffected.\r\n\r\n### Cloud Dataflow\r\nCloud Dataflow workers failed to start in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on Google Compute Engine instance creation. Affected jobs saw error messages like \u201cStartup of the worker pool in zone us-central1-a failed to bring up any of the desired X workers. INTERNAL_ERROR: Internal error. Please try again or contact Google Support. (Code: '-473021768383484163')\u201d. All other Cloud Dataflow regions and zones were unaffected.\r\n\r\n### Cloud Dataproc\r\nCloud Dataproc cluster creations failed in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on Google Compute Engine instance creation. All other Cloud Dataproc regions and zones were unaffected.\r\n\r\n### Cloud DNS\r\nCloud DNS in us-central1, us-east1, and us-east4 experienced delays for newly created or updated Private DNS records to become resolvable from 12:46 to 19:51 for a duration of 7 hours and 5 minutes.\r\n\r\n### Cloud Firestore\r\nCloud Firestore API was unable to be enabled (if not previously enabled) globally from 13:36 to 17:50 for a duration of 4 hours and 14 minutes.\r\n\r\n### Cloud Run\r\nCloud Run new deployments failed in the us-central1 region from 12:48 to 16:35 for a duration of 3 hours and 53 minutes. Existing Cloud Run workloads, and deployments in other regions were unaffected.\r\n\r\n\r\n\r\n## ROOT CAUSE\r\n\r\nGoogle runs an internal publish/subscribe messaging system, which many services use to propagate state for control plane operations. That system is built using a replicated, high-availability key-value store, holding information about current lists of publishers, subscribers and topics, which all clients of the system need access to.\r\n\r\nThe outage was triggered when a routine software rollout of the key-value store in a single region restarted one of its tasks. Soon after, a network partition isolated other tasks, transferring load to a small number of replicas of the key-value store. As a defense-in-depth, clients of the key-value store are designed to continue working from existing, cached data when it is unavailable; unfortunately, an issue in a large number of clients caused them to fail and attempt to resynchronize state. The smaller number of key-value store replicas were unable to sustain the load of clients synchronizing state, causing those replicas to fail. The continued failures moved load around the available replicas of the key-value store, resulting in a degraded state of the interconnected components.\r\n\r\nThe failure of the key-value store, combined with the issue in the key-value store client, meant that publishers and subscribers in the impacted region were unable to correctly send and receive messages, causing the documented impact on dependent services.\r\n\r\n\r\n## REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted to the incident at 12:56 US/Pacific and immediately began their investigation. As the situation began to show signs of cascading failures, the scope of the incident quickly became apparent and our specialized incident response team joined the investigation at 13:58 to address the problem. The early hours of the investigation were spent organizing, developing, and trialing various mitigation strategies. At 15:59 a potential root cause was identified and a configuration change submitted which increased the client synchronization delay allowed by the system, allowing clients to successfully complete their requests without timing out and reducing the overall load on the system. By 17:24, the change had fully propagated and the degraded components had returned to nominal performance. \r\n\r\nIn order to reduce the risk of recurrence, Google engineers configured the system to limit the number of tasks coordinating publishers and subscribers, which is a driver of load on the key-value store. The initial rollout of the constraint was faulty, and caused a more limited recurrence of problems at 19:00. This was quickly spotted and completely mitigated by 20:00, resolving the incident.\r\n\r\nWe would like to apologize for the length and severity of this incident. We have taken immediate steps to prevent recurrence of this incident and improve reliability in the future. In order to reduce the chance of a similar class of errors from occurring we are taking the following actions. We will revise provisioning of the key-value store to ensure that it is sufficiently resourced to handle sudden failover, and fix the issue in the key-value store client so that it continues to work from cached data, as designed, when the key-value store fails. We will also shard the data to reduce the scope of potential impact when the key-value store fails. Furthermore, we will be implementing automatic horizontal scaling of key-value store tasks to enable faster time to mitigation in the future. Finally, we will be improving our communication tooling to more effectively communicate multi-product outages and disruptions.\r\n\r\n\r\n## NOTE REGARDING CLOUD STATUS DASHBOARD COMMUNICATION\r\n\r\nIncident communication was centralized on a single product - in this case Stackdriver - in order to provide a central location for customers to follow for updates. We realize this may have created the incorrect impression that Stackdriver was the root cause. We apologize for the miscommunication and will make changes to ensure that we communicate more clearly in the future.\r\n\r\n\r\n## SLA CREDITS\r\n\r\nIf you believe your paid application experienced an SLA violation as a result of this incident, please submit the SLA credit request: https://support.google.com/cloud/contact/cloud_platform_sla\r\n\r\nA full list of all Google Cloud Platform Service Level Agreements can be found at: https://cloud.google.com/terms/sla/.", "when": "2019-09-27T22:38:22Z"}, "number": 19007, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-09-27T22:38:22Z", "modified": "2019-10-08T17:44:02Z", "text": "## ISSUE SUMMARY\r\n\r\nOn Tuesday 24 September, 2019, the following Google Cloud Platform services were partially impacted by an overload condition in an internal publish/subscribe messaging system which is a dependency of these products: App Engine, Compute Engine, Kubernetes Engine, Cloud Build, Cloud Composer, Cloud Dataflow, Cloud Dataproc, Cloud Firestore, Cloud Functions, Cloud DNS, Cloud Run, and Stackdriver Logging & Monitoring. Impact was limited to administrative operations for a number of these products, with existing workloads and instances not affected in most cases. \r\n\r\nWe apologize to those customers whose services were impacted during this incident; we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\n## DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 24 September, 2019 from 12:46 to 20:00 US/Pacific, Google Cloud Platform experienced a partial disruption to multiple services with their respective impacts detailed below:\r\n\r\n### App Engine\r\nGoogle App Engine (GAE) create, update, and delete admin operations failed globally from 12:57 to 18:21 for a duration of 5 hours and 24 minutes. Affected customers may have seen error messages like \u201cAPP_ERROR\u201d. Existing GAE workloads were unaffected.\r\n\r\n### Compute Engine\r\nGoogle Compute Engine (GCE) instances failed to start in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes, and GCE Internal DNS in us-central1, us-east1, and us-east4 experienced delays for newly created hostnames to become resolvable. Existing GCE instances and hostnames were unaffected.\r\n\r\n### Kubernetes Engine\r\nGoogle Kubernetes Engine (GKE) experienced delayed resource metadata and inaccurate Stackdriver Monitoring for cluster metrics globally. Additionally, cluster creation operations failed in us-central1-a from 3:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on GCE instance creation. Most existing GKE clusters were unaffected by the GCE instance creation failures, except for clusters in us-central1-a that were may have been unable to repair nodes or scale a node pool.\r\n\r\n### Stackdriver Logging & Monitoring\r\nStackdriver Logging experienced delays of up to two hours for logging events generated globally. Exports were delayed by up to 3 hours and 30 minutes. Some user requests to write logs in us-central1 failed. Some logs-based metric monitoring charts displayed lower counts, and queries to Stackdriver Logging briefly experienced a period of 50% error rates. The impact to Stackdriver Logging & Monitoring took place from 12:54 to 18:45 for a total duration of 5 hours and 51 minutes.\r\n\r\n### Cloud Functions\r\nCloud Functions deployments failed globally from 12:57 to 18:21 and experienced peak error rates of 13% in us-east1 and 80% in us-central1 from 19:12 to 19:57 for a combined duration of 6 hours and 15 minutes. Existing Cloud Function deployments were unaffected.\r\n\r\n### Cloud Build\r\nCloud Build failed to update build status for GitHub App triggers from 12:54 to 16:00 for a duration of 3 hours and 6 minutes.\r\n\r\n### Cloud Composer\r\nCloud Composer environment creations failed globally from 13:25 to 18:05 for a duration of 4 hours and 40 minutes. Existing Cloud Composer clusters were unaffected.\r\n\r\n### Cloud Dataflow\r\nCloud Dataflow workers failed to start in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on Google Compute Engine instance creation. Affected jobs saw error messages like \u201cStartup of the worker pool in zone us-central1-a failed to bring up any of the desired X workers. INTERNAL_ERROR: Internal error. Please try again or contact Google Support. (Code: '-473021768383484163')\u201d. All other Cloud Dataflow regions and zones were unaffected.\r\n\r\n### Cloud Dataproc\r\nCloud Dataproc cluster creations failed in us-central1-a from 13:11 to 14:32 for a duration of 1 hour and 21 minutes due to its dependency on Google Compute Engine instance creation. All other Cloud Dataproc regions and zones were unaffected.\r\n\r\n### Cloud DNS\r\nCloud DNS in us-central1, us-east1, and us-east4 experienced delays for newly created or updated Private DNS records to become resolvable from 12:46 to 19:51 for a duration of 7 hours and 5 minutes.\r\n\r\n### Cloud Firestore\r\nCloud Firestore API was unable to be enabled (if not previously enabled) globally from 13:36 to 17:50 for a duration of 4 hours and 14 minutes.\r\n\r\n### Cloud Run\r\nCloud Run new deployments failed in the us-central1 region from 12:48 to 16:35 for a duration of 3 hours and 53 minutes. Existing Cloud Run workloads, and deployments in other regions were unaffected.\r\n\r\n\r\n\r\n## ROOT CAUSE\r\n\r\nGoogle runs an internal publish/subscribe messaging system, which many services use to propagate state for control plane operations. That system is built using a replicated, high-availability key-value store, holding information about current lists of publishers, subscribers and topics, which all clients of the system need access to.\r\n\r\nThe outage was triggered when a routine software rollout of the key-value store in a single region restarted one of its tasks. Soon after, a network partition isolated other tasks, transferring load to a small number of replicas of the key-value store. As a defense-in-depth, clients of the key-value store are designed to continue working from existing, cached data when it is unavailable; unfortunately, an issue in a large number of clients caused them to fail and attempt to resynchronize state. The smaller number of key-value store replicas were unable to sustain the load of clients synchronizing state, causing those replicas to fail. The continued failures moved load around the available replicas of the key-value store, resulting in a degraded state of the interconnected components.\r\n\r\nThe failure of the key-value store, combined with the issue in the key-value store client, meant that publishers and subscribers in the impacted region were unable to correctly send and receive messages, causing the documented impact on dependent services.\r\n\r\n\r\n## REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted to the incident at 12:56 US/Pacific and immediately began their investigation. As the situation began to show signs of cascading failures, the scope of the incident quickly became apparent and our specialized incident response team joined the investigation at 13:58 to address the problem. The early hours of the investigation were spent organizing, developing, and trialing various mitigation strategies. At 15:59 a potential root cause was identified and a configuration change submitted which increased the client synchronization delay allowed by the system, allowing clients to successfully complete their requests without timing out and reducing the overall load on the system. By 17:24, the change had fully propagated and the degraded components had returned to nominal performance. \r\n\r\nIn order to reduce the risk of recurrence, Google engineers configured the system to limit the number of tasks coordinating publishers and subscribers, which is a driver of load on the key-value store. The initial rollout of the constraint was faulty, and caused a more limited recurrence of problems at 19:00. This was quickly spotted and completely mitigated by 20:00, resolving the incident.\r\n\r\nWe would like to apologize for the length and severity of this incident. We have taken immediate steps to prevent recurrence of this incident and improve reliability in the future. In order to reduce the chance of a similar class of errors from occurring we are taking the following actions. We will revise provisioning of the key-value store to ensure that it is sufficiently resourced to handle sudden failover, and fix the issue in the key-value store client so that it continues to work from cached data, as designed, when the key-value store fails. We will also shard the data to reduce the scope of potential impact when the key-value store fails. Furthermore, we will be implementing automatic horizontal scaling of key-value store tasks to enable faster time to mitigation in the future. Finally, we will be improving our communication tooling to more effectively communicate multi-product outages and disruptions.\r\n\r\n\r\n## NOTE REGARDING CLOUD STATUS DASHBOARD COMMUNICATION\r\n\r\nIncident communication was centralized on a single product - in this case Stackdriver - in order to provide a central location for customers to follow for updates. We realize this may have created the incorrect impression that Stackdriver was the root cause. We apologize for the miscommunication and will make changes to ensure that we communicate more clearly in the future.\r\n\r\n\r\n## SLA CREDITS\r\n\r\nIf you believe your paid application experienced an SLA violation as a result of this incident, please submit the SLA credit request: https://support.google.com/cloud/contact/cloud_platform_sla\r\n\r\nA full list of all Google Cloud Platform Service Level Agreements can be found at: https://cloud.google.com/terms/sla/.", "when": "2019-09-27T22:38:22Z"}, {"created": "2019-09-26T00:26:23Z", "modified": "2019-09-26T00:26:23Z", "text": "Affected Services:\r\nGoogle Compute Engine, Google Kubernetes Engine, Google App Engine, Cloud DNS, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Functions, Cloud Dataflow, Cloud Dataproc, Stackdriver Logging.\r\n    \r\nAffected Features:\r\nVarious resource operations, Environment creation, Log event ingestion.\r\n\r\nIssue Summary: \r\nGoogle Cloud Platform experienced a disruption to multiple services in us-central1, us-east1, and us-east4, while a few services were affected globally. Impact lasted for 7 hours, 6 minutes. We will publish a complete analysis of this incident once we have completed our internal investigation.\r\n\r\n(preliminary) Root cause: \r\nWe rolled out a release of a key/value store component to an internal Google system which triggered a shift in load causing an out of memory (OOM) crash loop on these instances. The control plane was unable to automatically coordinate the increased load by publisher and subscriber clients, which resulted in degraded state of the interconnected components.\r\n\r\nMitigation: \r\nRisk of recurrence has been mitigated by a configuration change that enabled control plane clients to successfully complete their requests and reduce overall load on the system.\r\n\r\nA Note on Cloud Status Dashboard Communication:\r\nIncident communication was centralized on a single product - in this case Stackdriver - in order to provide a central location for customers to follow for updates. Neither Stackdriver Logging nor Stackdriver Monitoring were the root cause for this incident. We realize this may have caused some confusion about the root cause of the issue. We apologize for the miscommunication and will make changes to ensure that we communicate more clearly in the future.", "when": "2019-09-26T00:26:23Z"}, {"created": "2019-09-25T04:32:33Z", "modified": "2019-09-25T04:32:33Z", "text": "The issue with multiple Cloud products has been resolved for all affected projects as of Tuesday, 2019-09-24 21:31 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-09-25T04:32:33Z"}, {"created": "2019-09-25T03:43:59Z", "modified": "2019-09-25T03:43:59Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Dataflow and Cloud Dataproc starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nWe have done some mitigations that have had positive impact, and some services have reported full recovery. Full recovery of all services is ongoing as our systems are processing significant backlogs. \nServices that have recovered include Compute Engine Internal DNS, App Engine, Cloud Run, Cloud Functions, Cloud Firestore, Cloud Composer, Cloud Build, Cloud Dataflow, Google Kubernetes Engine resource metadata and Stackdriver Monitoring metrics for clusters . \nCloud DNS has also recovered but we are still monitoring to ensure the issue does not reoccur. Stackdriver Logging has also recovered for the majority of projects.\nWe will provide another status update by Tuesday, 2019-09-24 22:00 US/Pacific with current details.\n\n\nDiagnosis: \n\n\n\nWorkaround: None at this time.", "when": "2019-09-25T03:43:59Z"}, {"created": "2019-09-25T03:11:50Z", "modified": "2019-09-25T03:11:50Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Dataflow and Cloud Dataproc starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nWe have done some mitigations that have had positive impact, and some services have reported full recovery. Full recovery of all services is ongoing as our systems are processing significant backlogs. \nServices that have recovered include Compute Engine Internal DNS, App Engine, Cloud Run, Cloud Functions, Cloud Firestore, Cloud Composer, Cloud Build, Cloud Dataflow, Google Kubernetes Engine resource metadata and Stackdriver Monitoring metrics for clusters . \nCloud DNS has also recovered but we are still monitoring to ensure the issue does not reoccur. Stackdriver Logging has also recovered for the majority of projects.\nWe will provide another status update by Tuesday, 2019-09-24 20:45 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\n\n\n\nWorkaround: None at this time.", "when": "2019-09-25T03:11:50Z"}, {"created": "2019-09-25T02:30:11Z", "modified": "2019-09-25T02:31:45Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Dataflow and Cloud Dataproc starting around Tuesday, 2019-09-24 13:00 US/Pacific. \r\nWe have done some mitigations that have had positive impact, and some services have reported full recovery. Full recovery of all services is ongoing as our systems are processing significant backlogs. \r\nServices that have recovered include Compute Engine Internal DNS, App Engine, Cloud Run, Cloud Functions, Cloud Firestore, Cloud Composer, Cloud Build and Cloud Dataflow. \r\nCloud DNS has also recovered but we are still monitoring to ensure the issue does not reoccur. Stackdriver Logging has also recovered for the majority of projects.\r\nWe will provide another status update by Tuesday, 2019-09-24 20:00 US/Pacific with current details.\r\n\r\n\r\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\r\nCloud DNS users in us-central1 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\r\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\r\n\r\n\r\nWorkaround: None at this time.", "when": "2019-09-25T02:30:11Z"}, {"created": "2019-09-25T02:02:31Z", "modified": "2019-09-25T02:02:31Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Dataflow and Cloud Dataproc starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nWe have done some mitigations that have had positive impact, and some services have reported full recovery. Full recovery of all services is ongoing as our systems are processing significant backlogs. \nServices that have recovered include Compute Engine Internal DNS, App Engine, Cloud Run, Cloud Functions, Cloud Firestore and Cloud Composer. \nCloud DNS has also recovered but we are still monitoring to ensure the issue does not reoccur. Stackdriver Logging has also recovered for the majority of projects.\nWe will provide another status update by Tuesday, 2019-09-24 19:30 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCloud DNS users in us-central1 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \n\n\n\nWorkaround: None at this time.", "when": "2019-09-25T02:02:31Z"}, {"created": "2019-09-25T01:20:25Z", "modified": "2019-09-25T01:41:59Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, Cloud Firestore, Cloud Dataflow and Cloud Dataproc starting around Tuesday, 2019-09-24 13:00 US/Pacific. \r\nWe have done some mitigations that have had positive impact, and some services have reported full recovery. Full recovery of all services is ongoing as our systems are processing significant backlogs. \r\nServices that have recovered include Compute Engine Internal DNS, App Engine, Cloud Run, Cloud Functions and Cloud Firestore. \r\nWe will provide another status update by Tuesday, 2019-09-24 18:45 US/Pacific with current details.\r\n\r\n\r\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\r\nCloud DNS users in us-central1 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\r\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\r\nCloud Build users may see Github status updates for GitHub App triggers fail.\r\nCloud Composer environment creations are failing. \r\n\r\n\r\n\r\nWorkaround: None at this time.", "when": "2019-09-25T01:20:25Z"}, {"created": "2019-09-25T00:24:14Z", "modified": "2019-09-25T00:24:14Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, and Cloud Firestore starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nMitigation work is still underway by our Engineering Team, and some services are starting to see recovery. We will provide another status update by Tuesday, 2019-09-24 18:00 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCompute Engine Internal DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created VMs to become resolvable.\nCloud DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \nNew and existing projects are failing to enable Cloud Firestore if it is not already enabled.\n\n\nWorkaround: None at this time.", "when": "2019-09-25T00:24:14Z"}, {"created": "2019-09-24T23:19:32Z", "modified": "2019-09-24T23:19:32Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, and Cloud Firestore starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nMitigation work is still underway by our Engineering Team. We will provide another status update by Tuesday, 2019-09-24 17:30 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCompute Engine Internal DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created VMs to become resolvable.\nCloud DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \nNew and existing projects are failing to enable Cloud Firestore if it is not already enabled.\n\n\nWorkaround: None at this time.", "when": "2019-09-24T23:19:32Z"}, {"created": "2019-09-24T23:02:02Z", "modified": "2019-09-24T23:02:02Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, Cloud Composer, and Cloud Firestore starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nMitigation work is still underway by our Engineering Team. We will provide another status update by Tuesday, 2019-09-24 16:45 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCompute Engine Internal DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created VMs to become resolvable.\nCloud DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \nNew and existing projects are failing to enable Cloud Firestore if it is not already enabled.\n\n\nWorkaround: None at this time.", "when": "2019-09-24T23:02:02Z"}, {"created": "2019-09-24T22:28:30Z", "modified": "2019-09-24T22:28:30Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, and Cloud Composer starting around Tuesday, 2019-09-24 13:00 US/Pacific. \nMitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-09-24 16:30 US/Pacific with current details.\n\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCompute Engine Internal DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created VMs to become resolvable.\nCloud DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \n\n\nWorkaround: None at this time.", "when": "2019-09-24T22:28:30Z"}, {"created": "2019-09-24T21:51:54Z", "modified": "2019-09-24T21:51:54Z", "text": "Description: We are investigating issues with multiple Cloud products including Compute Engine Internal DNS, Cloud DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, Cloud Run, Cloud Build, and Cloud Composer starting around Tuesday, 2019-09-24 13:00 US/Pacific. We will provide another status update by Tuesday, 2019-09-24 15:30 US/Pacific with current details.\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nCompute Engine Internal DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created VMs to become resolvable.\nCloud DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than usual delay for newly created or updated Private DNS records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\nCloud Build users may see Github status updates for GitHub App triggers fail.\nCloud Composer environment creations are failing. \n\n\nWorkaround: None at this time.", "when": "2019-09-24T21:51:54Z"}, {"created": "2019-09-24T21:25:30Z", "modified": "2019-09-24T21:25:30Z", "text": "Description: We are investigating issues with multiple Cloud products including GCE DNS, Stackdriver Logging and Monitoring, Compute Engine, App Engine, and Cloud Run starting around Tuesday, 2019-09-24 13:00 US/Pacific. We will provide another status update by Tuesday, 2019-09-24 15:00 US/Pacific with current details.\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nGCE DNS users in us-central1, us-east1, and us-east4 are experiencing a longer than normal delay for newly created records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nNew Google App Engine deployments and updates to existing deployments may fail.\nCloud Run deployments are failing for new deployments in us-central1.\n\n\nWorkaround: None at this time.", "when": "2019-09-24T21:25:30Z"}, {"created": "2019-09-24T21:04:35Z", "modified": "2019-09-24T21:04:35Z", "text": "Description: We are investigating issues with multiple Cloud products including GCE DNS, Stackdriver Logging and Monitoring, Compute Engine, and Cloud Run and App Engine starting around Tuesday, 2019-09-24 13:00 US/Pacific. We will provide another status update by Tuesday, 2019-09-24 15:00 US/Pacific with current details.\n\nDiagnosis: The Stackdriver Logging service is experiencing a delay on logging events generated globally after Tuesday, 2019-09-24 12:53 US/Pacific.\nGCE DNS users in us-central1 are experiencing a longer than normal delays for newly created records to become resolvable.\nGoogle Kubernetes Engine resource metadata may be delayed and Stackdriver Monitoring metrics for clusters may be inaccurate during this time.\nGoogle Compute Engine instances are failing to start in us-central1-a.\nCloud Run deployments are failing for new deployments in us-central1.\n\n\nWorkaround: None at this time.", "when": "2019-09-24T21:04:35Z"}, {"created": "2019-09-24T20:30:09Z", "modified": "2019-09-24T20:30:09Z", "text": "Description: The Stackdriver Logging service is experiencing a delay on logging events generated after Tuesday, 2019-09-24 12:53 US/Pacific. We will provide another status update by Tuesday, 2019-09-24 15:00 US/Pacific with current details.\n\nDiagnosis: Log entries after Tuesday, 2019-09-24 12:53 US/Pacific will see a delay in propagating to the Stackdriver console\n\nWorkaround: None at this time. We will be processing the backlog as soon as we can.", "when": "2019-09-24T20:30:09Z"}], "uri": "/incident/google-stackdriver/19007"}, {"begin": "2019-09-17T21:48:21Z", "created": "2019-09-17T21:54:09Z", "end": "2019-09-18T01:26:08Z", "external_desc": "Google Cloud Storage is experiencing an issue with elevated error rates on object creation and some mutation events in the US multi-region.", "modified": "2019-09-18T01:26:08Z", "most-recent-update": {"created": "2019-09-18T01:26:08Z", "modified": "2019-09-18T01:26:08Z", "text": "The issue with Google Cloud Storage elevated error rates on some object creation and mutation operations in the US multi-region has been resolved for all affected projects as of Tuesday, 2019-09-17 18:24 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-18T01:26:08Z"}, "number": 19006, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-09-18T01:26:08Z", "modified": "2019-09-18T01:26:08Z", "text": "The issue with Google Cloud Storage elevated error rates on some object creation and mutation operations in the US multi-region has been resolved for all affected projects as of Tuesday, 2019-09-17 18:24 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-18T01:26:08Z"}, {"created": "2019-09-17T23:49:50Z", "modified": "2019-09-17T23:49:50Z", "text": "Description: Google Cloud Storage is still seeing an elevated error rate on some object creation and mutation operations in the US multi-region.  Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-09-17 18:30 US/Pacific with current details.\n\nDiagnosis: Users of Google Cloud Storage may see errors during creation or mutation of GCS objects in the US multi-region.\n\nWorkaround: Retrying the failed operation may succeed.", "when": "2019-09-17T23:49:50Z"}, {"created": "2019-09-17T23:01:46Z", "modified": "2019-09-17T23:01:46Z", "text": "Description: Google Cloud Storage is experiencing an issue with elevated error rates on object creation and some mutation events in the US multi-region. Our Engineering Team is still investigating possible causes. We will provide another status update by Tuesday, 2019-09-17 16:45 US/Pacific with current details.\n\nDiagnosis: Users of Google Cloud Storage may see errors during creation or mutation of GCS objects in the US multi-region.\n\nWorkaround: Retrying the failed operation may succeed.", "when": "2019-09-17T23:01:46Z"}, {"created": "2019-09-17T21:54:31Z", "modified": "2019-09-17T21:54:31Z", "text": "Description: We are investigating errors with Google Cloud Storage in the US multi-region. Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-09-17 16:00 US/Pacific with current details.\n\nDiagnosis: Users of Google Cloud Storage may see errors during object creation in the US multi-region\n\nWorkaround: Retry failed object creation", "when": "2019-09-17T21:54:31Z"}], "uri": "/incident/storage/19006"}, {"begin": "2019-09-17T03:52:07Z", "created": "2019-09-17T03:56:52Z", "end": "2019-09-17T06:02:44Z", "external_desc": "We have received reports of an issue with Cloud Dataflow", "modified": "2019-09-17T06:02:44Z", "most-recent-update": {"created": "2019-09-17T06:02:44Z", "modified": "2019-09-17T06:02:44Z", "text": "The issue with Cloud Dataflow jobs using private IP are failing has been resolved for all affected users as of Monday, 2019-09-16 23:01 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-17T06:02:44Z"}, "number": 19002, "public": true, "service_key": "cloud-dataflow", "service_name": "Google Cloud Dataflow", "severity": "medium", "updates": [{"created": "2019-09-17T06:02:44Z", "modified": "2019-09-17T06:02:44Z", "text": "The issue with Cloud Dataflow jobs using private IP are failing has been resolved for all affected users as of Monday, 2019-09-16 23:01 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-17T06:02:44Z"}, {"created": "2019-09-17T04:48:47Z", "modified": "2019-09-17T04:48:47Z", "text": "Description: The issue with Cloud Dataflow, where templated jobs using private IP are failing, is mitigated in us-central1. It should be resolved in the rest of the regions shortly and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-09-16 23:00 US/Pacific with current details.\n\nDiagnosis: Templated Dataflow jobs using private IP will fail. Non-templated jobs are not affected.\n\nWorkaround: Configure your Dataflow jobs to use public IP instead", "when": "2019-09-17T04:48:47Z"}, {"created": "2019-09-17T04:05:11Z", "modified": "2019-09-17T04:05:11Z", "text": "Description: We are experiencing an issue with Cloud Dataflow where jobs using private IP are failing. Current data indicates that the failure only affecting templated jobs while non-template jobs are able to use private IPs successfully. Mitigation is underway by the Engineering team. For everyone who is affected, we apologize for the disruption. We will provide an update by Monday, 2019-09-16 22:30 US/Pacific with current details.\n\nDiagnosis: Templated Dataflow jobs using private IP will fail. Non-templated jobs are not affected.\n\nWorkaround: Configure your Dataflow jobs to use public IP instead", "when": "2019-09-17T04:05:11Z"}, {"created": "2019-09-17T03:57:18Z", "modified": "2019-09-17T03:57:18Z", "text": "Description: The Cloud Dataflow service is reporting errors on Dataflow jobs using private IP. We will provide another status update by Monday, 2019-09-16 21:30 US/Pacific with current details.\n\nDiagnosis: Dataflow jobs using private IP may fail\n\nWorkaround: Configure the Dataflow jobs to use public IP instead", "when": "2019-09-17T03:57:18Z"}], "uri": "/incident/cloud-dataflow/19002"}, {"begin": "2019-09-13T19:49:51Z", "created": "2019-09-13T19:49:57Z", "end": "2019-09-13T20:15:34Z", "external_desc": "We are investigating elevated errors with Cloud Interconnect in Google Cloud Networking in us-west2.", "modified": "2019-09-13T20:15:34Z", "most-recent-update": {"created": "2019-09-13T20:15:34Z", "modified": "2019-09-13T20:15:34Z", "text": "The Cloud Interconnect issue is believed to be affecting less than 1% of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-09-13T20:15:34Z"}, "number": 19019, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-09-13T20:15:34Z", "modified": "2019-09-13T20:15:34Z", "text": "The Cloud Interconnect issue is believed to be affecting less than 1% of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-09-13T20:15:34Z"}, {"created": "2019-09-13T19:50:16Z", "modified": "2019-09-13T19:50:16Z", "text": "Description: We are investigating elevated errors with Cloud Interconnect in Google Cloud Networking in us-west2. Our Engineering Team is investigating possible causes. We will provide another status update by Friday, 2019-09-13 13:30 US/Pacific with current details.\n\nDiagnosis: Customers peering with us-west2 may experience packet loss.\n\nWorkaround: None at this time.", "when": "2019-09-13T19:50:16Z"}], "uri": "/incident/cloud-networking/19019"}, {"begin": "2019-09-11T14:18:47Z", "created": "2019-09-11T14:28:30Z", "end": "2019-09-11T15:56:39Z", "external_desc": "We've received a report of an issue with Google Cloud Functions.", "modified": "2019-09-11T15:56:39Z", "most-recent-update": {"created": "2019-09-11T15:56:39Z", "modified": "2019-09-11T15:56:39Z", "text": "The issue with Google Cloud Functions deployments to Asia has been resolved for all affected projects as of Wednesday, 2019-09-11 08:34 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-11T15:56:39Z"}, "number": 19006, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-09-11T15:56:39Z", "modified": "2019-09-11T15:56:39Z", "text": "The issue with Google Cloud Functions deployments to Asia has been resolved for all affected projects as of Wednesday, 2019-09-11 08:34 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-11T15:56:39Z"}, {"created": "2019-09-11T15:15:31Z", "modified": "2019-09-11T15:15:31Z", "text": "Description: Our Engineering Team believes they have identified the root cause of the errors and is working to mitigate. We will provide another status update by Wednesday, 2019-09-11 10:00 US/Pacific with current details.\n\nDiagnosis: Cloud Function deployment to any region in Asia fails", "when": "2019-09-11T15:15:31Z"}, {"created": "2019-09-11T14:28:45Z", "modified": "2019-09-11T14:28:45Z", "text": "Description: We are experiencing an issue with Google Cloud Functions beginning at Wednesday, 2019-09-11 07:18 US/Pacific. Current data indicate(s) that approximately 100% of projects deploying Cloud Functions to Asia are affected by this issue. For everyone who is affected, we apologize for the disruption. We will provide an update by Wednesday, 2019-09-11 08:30 US/Pacific with current details.\n\nDiagnosis: Cloud Function deployment to any region in Asia fails", "when": "2019-09-11T14:28:45Z"}], "uri": "/incident/cloud-functions/19006"}, {"begin": "2019-09-10T00:53:01Z", "created": "2019-09-10T01:16:48Z", "end": "2019-09-10T07:30:30Z", "external_desc": "We investigating an Global issue with log ingestion delays on Stackdriver Logging.", "modified": "2019-09-10T07:30:30Z", "most-recent-update": {"created": "2019-09-10T07:30:30Z", "modified": "2019-09-10T07:30:30Z", "text": "The issue with Stackdriver Logging ingestion has been resolved for all affected users as of Tuesday, 2019-09-10 00:07 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-10T07:30:30Z"}, "number": 19006, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-09-10T07:30:30Z", "modified": "2019-09-10T07:30:30Z", "text": "The issue with Stackdriver Logging ingestion has been resolved for all affected users as of Tuesday, 2019-09-10 00:07 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-10T07:30:30Z"}, {"created": "2019-09-10T04:54:09Z", "modified": "2019-09-10T04:54:09Z", "text": "Description: The issue is mitigated. At the moment we processing the backlog. The expected time to completion is around 7 hours. We will provide more information by Tuesday, 2019-09-10 06:00 US/Pacific.", "when": "2019-09-10T04:54:09Z"}, {"created": "2019-09-10T04:41:13Z", "modified": "2019-09-10T04:41:13Z", "text": "Description: The issue is mitigated. At the moment we begin processing the backlog. The age of the oldest messages in the backlog in about 5 hours. We will provide more information by Monday, 2019-09-09 23:00 US/Pacific.", "when": "2019-09-10T04:41:13Z"}, {"created": "2019-09-10T04:15:21Z", "modified": "2019-09-10T04:15:21Z", "text": "Description: The original mitigation does not take expected effect. The Engineering Team working on another mitigation. We will provide more information by Monday, 2019-09-09 22:30 US/Pacific.", "when": "2019-09-10T04:15:21Z"}, {"created": "2019-09-10T03:17:38Z", "modified": "2019-09-10T03:17:38Z", "text": "Description: Engineering Team make a progress on investigation. At the moment we trying to apply suggested mitigation and waiting on confirmation of whether it is working as expected or not. We will provide more information by Monday, 2019-09-09 21:30 US/Pacific.", "when": "2019-09-10T03:17:38Z"}, {"created": "2019-09-10T02:17:53Z", "modified": "2019-09-10T02:17:53Z", "text": "Description: We continue investigating the causes of log ingestion delays in Stackdriver Logging. As of now, some logs can be found delayed for up to 3.5 hours. We will provide more information by Monday, 2019-09-09 20:30 US/Pacific.", "when": "2019-09-10T02:17:53Z"}, {"created": "2019-09-10T01:17:13Z", "modified": "2019-09-10T01:17:13Z", "text": "Description: We investigating log ingestion delays in Stackdriver Logging. As of now logs can be delayed for up to 3 hours. We will provide more information by Monday, 2019-09-09 19:30 US/Pacific.", "when": "2019-09-10T01:17:13Z"}], "uri": "/incident/google-stackdriver/19006"}, {"begin": "2019-09-05T10:23:47Z", "created": "2019-09-05T13:04:03Z", "end": "2019-09-05T23:41:07Z", "external_desc": "Issues creating instances and deploying apps in asia-east2. ", "modified": "2019-09-05T23:41:07Z", "most-recent-update": {"created": "2019-09-05T23:41:07Z", "modified": "2019-09-05T23:41:07Z", "text": "The issue with Google Compute Engine instance creation failures has been resolved for all affected projects as of Thursday, 2019-09-05 16:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-05T23:41:07Z"}, "number": 19005, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-09-05T23:41:07Z", "modified": "2019-09-05T23:41:07Z", "text": "The issue with Google Compute Engine instance creation failures has been resolved for all affected projects as of Thursday, 2019-09-05 16:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-05T23:41:07Z"}, {"created": "2019-09-05T20:40:56Z", "modified": "2019-09-05T20:40:56Z", "text": "Description: We are rolling back a potential fix to mitigate this issue. We will provide another status update by Thursday, 2019-09-05 17:00 US/Pacific with current details.\n\nDiagnosis: Failed instance creation and app deployment within asia-east2\n\nWorkaround: Projects in asia-east2 can create instances larger than or equal to n1-standard-2.  Other regions are unaffected.", "when": "2019-09-05T20:40:56Z"}, {"created": "2019-09-05T17:58:05Z", "modified": "2019-09-05T17:58:05Z", "text": "Description: We are investigating issues creating instances in asia-east2 region. This affects Google Compute Engine, Google App Engine and Cloud SQL in these regions. Our Engineering Team is investigating possible causes. We will provide another status update by Thursday, 2019-09-05 17:00 US/Pacific with current details.  Affected projects can utilize machines larger than (or equal to) n1-standard-2 instances in order to avoid failed instance creation.\n\nDiagnosis: Failed instance creation within asia-east2\n\nWorkaround: Projects in asia-east2 can create instances larger than or equal to n1-standard-2.  Other regions are unaffected.", "when": "2019-09-05T17:58:05Z"}, {"created": "2019-09-05T14:39:11Z", "modified": "2019-09-05T14:39:11Z", "text": "Description: We are investigating issues creating instances in asia-east2 region. This affects Google Compute Engine, Google App Engine and Cloud SQL in these regions. Our Engineering Team is investigating possible causes. We will provide another status update by Thursday, 2019-09-05 11:00 US/Pacific with current details.\n\nDiagnosis: Failed instance creation within asia-east2\n\nWorkaround: Projects in asia-east2 can create instances larger than or equal to n1-standard-2.  Other regions are unaffected.", "when": "2019-09-05T14:39:11Z"}, {"created": "2019-09-05T13:04:07Z", "modified": "2019-09-05T13:04:07Z", "text": "Description: We are investigating issues creating instances in asia-east2 region. This affects Google Compute Engine, Google App Engine and Cloud SQL in these regions. Our Engineering Team is investigating possible causes. We will provide another status update by Thursday, 2019-09-05 09:00 US/Pacific with current details.\n\nWorkaround: Other regions are unaffected.", "when": "2019-09-05T13:04:07Z"}], "uri": "/incident/compute/19005"}, {"begin": "2019-09-04T14:04:11Z", "created": "2019-09-04T14:24:49Z", "end": "2019-09-04T19:42:40Z", "external_desc": "We've received a report of an issue with Cloud SQL.", "modified": "2019-09-04T19:42:41Z", "most-recent-update": {"created": "2019-09-04T19:42:40Z", "modified": "2019-09-04T19:42:40Z", "text": "The issue with some Cloud SQL databases becoming stuck has been resolved for all affected projects as of Wednesday, 2019-09-04 12:39 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-04T19:42:40Z"}, "number": 19004, "public": true, "service_key": "cloud-sql", "service_name": "Google Cloud SQL", "severity": "medium", "updates": [{"created": "2019-09-04T19:42:40Z", "modified": "2019-09-04T19:42:40Z", "text": "The issue with some Cloud SQL databases becoming stuck has been resolved for all affected projects as of Wednesday, 2019-09-04 12:39 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-09-04T19:42:40Z"}, {"created": "2019-09-04T18:54:18Z", "modified": "2019-09-04T18:54:18Z", "text": "Description: The issue with some Cloud SQL databases becoming stuck after recent changes were applied should be resolved for the majority of users and we expect a full resolution within the next hour. We will provide another status update by Wednesday, 2019-09-04 13:00 US/Pacific with current details.\n\nDiagnosis: Cloud SQL databases becoming stuck and showing an \"unknown error\".\n\nWorkaround: None at this time", "when": "2019-09-04T18:54:18Z"}, {"created": "2019-09-04T16:22:49Z", "modified": "2019-09-04T16:22:49Z", "text": "Description: Our engineering team is still in progress with applying a mitigation. We will provide another status update by Wednesday, 2019-09-04 12:00 US/Pacific with current details.\n\nDiagnosis: None at this time\n\nWorkaround: None at this time", "when": "2019-09-04T16:22:49Z"}, {"created": "2019-09-04T15:04:38Z", "modified": "2019-09-04T15:04:38Z", "text": "Description: Following investigation, we believe we have found a mitigation for this issue.  Our engineering team will apply this mitigation shortly. We will provide another status update by Wednesday, 2019-09-04 09:15 US/Pacific with current details.\n\nDiagnosis: None at this time\n\nWorkaround: None at this time", "when": "2019-09-04T15:04:38Z"}, {"created": "2019-09-04T14:24:59Z", "modified": "2019-09-04T14:24:59Z", "text": "Description: We've received a report of an issue with Cloud SQL following the latest roll out as of Wednesday, 2019-09-04 07:04 US/Pacific.  Some databases are not coming back to serving after changes have been applied. We will provide more information by Wednesday, 2019-09-04 08:00 US/Pacific.\n\nDiagnosis: None at this time\n\nWorkaround: None at this time", "when": "2019-09-04T14:24:59Z"}], "uri": "/incident/cloud-sql/19004"}, {"begin": "2019-09-03T16:08:17Z", "created": "2019-09-03T17:09:00Z", "end": "2019-09-03T20:39:38Z", "external_desc": "We've received a report of an issue with Google App Engine.", "modified": "2019-09-03T20:39:39Z", "most-recent-update": {"created": "2019-09-03T20:39:39Z", "modified": "2019-09-03T20:39:39Z", "text": "The Google App Engine issue is believed to have been resolved as of 12:21 US/Pacific for all affected projects. \n\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-09-03T20:39:38Z"}, "number": 19010, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-09-03T20:39:39Z", "modified": "2019-09-03T20:39:39Z", "text": "The Google App Engine issue is believed to have been resolved as of 12:21 US/Pacific for all affected projects. \n\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\n\nNo further updates will be provided here.", "when": "2019-09-03T20:39:38Z"}, {"created": "2019-09-03T20:12:17Z", "modified": "2019-09-03T20:12:17Z", "text": "Description: The issue with Google App Engine experiencing DNS lookup failures should be resolved for the majority of projects as of 12:21 US/Pacific and we are continuing to monitor the situation in case of recurrence.\n\nWe will provide another status update by Tuesday, 2019-09-03 14:15 US/Pacific with current details.\n\nDiagnosis: DNS lookup failures across applications\n\nWorkaround: None needed - mitigation is in progress", "when": "2019-09-03T20:12:17Z"}, {"created": "2019-09-03T19:29:01Z", "modified": "2019-09-03T19:29:01Z", "text": "Description: The rollback has completed successfully.\n\nThe issue with Google App Engine experiencing DNS lookup failures should be resolved for the majority of projects and we expect a full resolution in the near future. We are continuing to monitor the situation before declaring the all-clear.\n\nWe will provide another status update by Tuesday, 2019-09-03 13:15 US/Pacific with current details.\n\nDiagnosis: DNS lookup failures across applications\n\nWorkaround: None needed - mitigation is in progress", "when": "2019-09-03T19:29:00Z"}, {"created": "2019-09-03T18:14:40Z", "modified": "2019-09-03T18:14:40Z", "text": "Description: The rollback is still in progress (65% complete). We expect it to complete within the next hour. We will provide another status update by Tuesday, 2019-09-03 12:30 US/Pacific with current details.\n\nDiagnosis: DNS lookup failures across applications\n\nWorkaround: None needed - mitigation is in progress", "when": "2019-09-03T18:14:40Z"}, {"created": "2019-09-03T17:09:10Z", "modified": "2019-09-03T17:09:10Z", "text": "Description: Rollback of the potential fix to mitigate the issue is in progress and has passed 50% completion - we expect all affected customers to be mitigated within 1.5 hours. We will provide another status update by Tuesday, 2019-09-03 11:30 US/Pacific with current details.\n\nDiagnosis: DNS lookup failures across applications\n\nWorkaround: None needed - mitigation is in progress", "when": "2019-09-03T17:09:10Z"}], "uri": "/incident/appengine/19010"}, {"begin": "2019-08-27T21:34:00Z", "created": "2019-08-27T21:37:02Z", "end": "2019-08-27T23:00:54Z", "external_desc": "Internal load balancers in us-east1 not updating", "modified": "2019-08-27T23:00:54Z", "most-recent-update": {"created": "2019-08-27T23:00:54Z", "modified": "2019-08-27T23:00:54Z", "text": "The issue with Cloud Networking Internal load balancers in us-east1 not updating has been resolved for all affected projects as of Tuesday, 2019-08-27 15:56 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-27T23:00:54Z"}, "number": 19018, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-08-27T23:00:54Z", "modified": "2019-08-27T23:00:54Z", "text": "The issue with Cloud Networking Internal load balancers in us-east1 not updating has been resolved for all affected projects as of Tuesday, 2019-08-27 15:56 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-27T23:00:54Z"}, {"created": "2019-08-27T22:54:43Z", "modified": "2019-08-27T22:54:43Z", "text": "Description: We are rolling back a potential fixto mitigate this issue. We will provide another status update by Tuesday, 2019-08-27 17:00 US/Pacific with current details.\n\nDiagnosis: Affected customers may experience configuration changes not propagating, including health checks for backends. This only affects Internal HTTP(S) Load Balancers. Internal TCP/UDP Load Balancing are not affected.\n\nWorkaround: None at this time.", "when": "2019-08-27T22:54:43Z"}, {"created": "2019-08-27T22:18:07Z", "modified": "2019-08-27T22:18:07Z", "text": "Description: Our Engineering Team believes they have identified the root cause and is working to mitigate. We will provide another status update by Tuesday, 2019-08-27 15:54 US/Pacific with current details.\n\nDiagnosis: Affected customers may experience configuration changes not propagating, including health checks for backends. This only affects Internal HTTP(S) Load Balancers. Internal TCP/UDP Load Balancing are not affected.\n\nWorkaround: None at this time.", "when": "2019-08-27T22:18:07Z"}, {"created": "2019-08-27T22:13:43Z", "modified": "2019-08-27T22:13:43Z", "text": "Description: Our Engineering Team believes they have identified the root cause and is working to mitigate. We will provide another status update by Tuesday, 2019-08-27 15:54 US/Pacific with current details.\n\nDiagnosis: Affected customers may experience configuration changes not propagating, including health checks for backends.\n\nWorkaround: None at this time.", "when": "2019-08-27T22:13:43Z"}, {"created": "2019-08-27T21:37:12Z", "modified": "2019-08-27T21:37:12Z", "text": "Description: We are investigating errors with Google Cloud Networking, regional internal load balancers in us-east1. Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-08-27 15:30 US/Pacific with current details.\n\nDiagnosis: Affected customers may experience configuration changes not propagating, including health checks for backends.\n\nWorkaround: None at this time.", "when": "2019-08-27T21:37:12Z"}], "uri": "/incident/cloud-networking/19018"}, {"begin": "2019-08-19T18:30:16Z", "created": "2019-08-19T18:52:49Z", "end": "2019-08-19T20:27:42Z", "external_desc": "We are currently experiencing an issue with authentication to Google App Engine sites, the Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints.", "modified": "2019-08-19T20:27:42Z", "most-recent-update": {"created": "2019-08-19T20:27:42Z", "modified": "2019-08-19T20:27:42Z", "text": "The issue with authentication to Google App Engine sites, the Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints has been resolved for all affected customers as of Monday, 2019-08-19 12:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-19T20:27:42Z"}, "number": 19008, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-08-19T20:27:42Z", "modified": "2019-08-19T20:27:42Z", "text": "The issue with authentication to Google App Engine sites, the Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints has been resolved for all affected customers as of Monday, 2019-08-19 12:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-19T20:27:42Z"}, {"created": "2019-08-19T20:03:37Z", "modified": "2019-08-19T20:03:37Z", "text": "Description: The issue with authentication to Google App Engine sites, Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints should be resolved for the majority of customers and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-08-19 13:30 US/Pacific with current details.\n\nWorkaround: Some customers have reported success attempting to utilize an incognito window under the Chrome browser to login.", "when": "2019-08-19T20:03:37Z"}, {"created": "2019-08-19T20:01:34Z", "modified": "2019-08-19T20:01:34Z", "text": "Description: The issue with authentication to Google App Engine sites, Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints should be resolved for the majority of customers and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-08-19 13:30 US/Pacific with current details.\n\n\nWorkaround: Some customers have reported success attempting to utilize an incognito window under the Chrome browser to login.", "when": "2019-08-19T20:01:34Z"}, {"created": "2019-08-19T19:43:19Z", "modified": "2019-08-19T19:43:19Z", "text": "Description: Mitigation work is currently underway by our Engineering Team to address the issue with authentication to Google App Engine sites, Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints.  Error rates are dropping and we're seeing service improvement. We will provide another status update by Monday, 2019-08-19 13:45 US/Pacific with current details.\n\nWorkaround: Some customers have reported success attempting to utilize an incognito window under the Chrome browser to login.", "when": "2019-08-19T19:43:19Z"}, {"created": "2019-08-19T19:18:32Z", "modified": "2019-08-19T19:18:32Z", "text": "Description: Investigation of the authentication issues with Google App Engine, Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints is currently underway by our Engineering Team. We will provide another status update by Monday, 2019-08-19 12:45 US/Pacific with current details.\n\nWorkaround: Some customers have reported success attempting to utilize an incognito window under the Chrome browser to login.", "when": "2019-08-19T19:18:32Z"}, {"created": "2019-08-19T18:53:15Z", "modified": "2019-08-19T18:53:15Z", "text": "Description: We are investigating an issue with authentication to Google App Engine sites, the Google Cloud Console, Identity Aware Proxy, and Google OAuth 2.0 endpoints. Our Engineering Team is investigating possible causes. We will provide another status update by Monday, 2019-08-19 12:15 US/Pacific with current details.\n\nWorkaround: None at this time.", "when": "2019-08-19T18:53:15Z"}], "uri": "/incident/developers-console/19008"}, {"begin": "2019-08-16T08:02:56Z", "created": "2019-08-16T08:04:29Z", "end": "2019-08-16T08:46:44Z", "external_desc": "We are investigating an intermittent networking issue", "modified": "2019-08-16T08:46:44Z", "most-recent-update": {"created": "2019-08-16T08:46:44Z", "modified": "2019-08-16T08:46:44Z", "text": "The issue with Cloud Networking packet loss has been resolved for all affected projects. Issue lasted from Friday, 2019-08-16 00:10 to Friday, 2019-08-16 00:26 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-16T08:46:44Z"}, "number": 19017, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-08-16T08:46:44Z", "modified": "2019-08-16T08:46:44Z", "text": "The issue with Cloud Networking packet loss has been resolved for all affected projects. Issue lasted from Friday, 2019-08-16 00:10 to Friday, 2019-08-16 00:26 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-08-16T08:46:44Z"}, {"created": "2019-08-16T08:29:07Z", "modified": "2019-08-16T08:29:07Z", "text": "Description: Mitigation is currently underway by our Engineering Team. We will provide another status update by Friday, 2019-08-16 02:00 US/Pacific with current details.\n\nDiagnosis: Affected projects may experience intermittent packet loss", "when": "2019-08-16T08:29:07Z"}, {"created": "2019-08-16T08:04:30Z", "modified": "2019-08-16T08:04:30Z", "text": "Description: We are investigating a networking issue. We will provide more information by Friday, 2019-08-16 01:30 US/Pacific.", "when": "2019-08-16T08:04:30Z"}], "uri": "/incident/cloud-networking/19017"}, {"begin": "2019-07-25T12:13:58Z", "created": "2019-07-25T14:51:01Z", "end": "2019-07-26T02:59:34Z", "external_desc": "We've received a report of an issue with Google Cloud Functions.", "modified": "2019-08-01T18:50:39Z", "most-recent-update": {"created": "2019-08-01T18:50:39Z", "modified": "2019-08-01T18:50:39Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:50:38Z"}, "number": 19005, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-08-01T18:50:39Z", "modified": "2019-08-01T18:50:39Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:50:38Z"}, {"created": "2019-07-26T01:23:34Z", "modified": "2019-07-26T01:23:34Z", "text": "The issue with Google Cloud Functions deployments has been resolved for all affected projects as of Thursday, 2019-07-25 18:22 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:23:34Z"}, {"created": "2019-07-25T23:47:21Z", "modified": "2019-07-25T23:47:21Z", "text": "The issue with Google Cloud Functions deployments should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:47:21Z"}, {"created": "2019-07-25T23:45:30Z", "modified": "2019-07-25T23:45:30Z", "text": "The issue with Google Cloud Functions deployments should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:45:30Z"}, {"created": "2019-07-25T22:38:46Z", "modified": "2019-07-25T22:38:46Z", "text": "The rate of errors is decreasing. We will provide another status update by Thursday, 2019-07-25 16:45 US/Pacific with current details.", "when": "2019-07-25T22:38:46Z"}, {"created": "2019-07-25T21:44:32Z", "modified": "2019-07-25T21:44:32Z", "text": "We are still seeing errors. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T21:44:32Z"}, {"created": "2019-07-25T20:46:21Z", "modified": "2019-07-25T20:46:21Z", "text": "We are still seeing errors. Our Engineers are attempting several mitigation strategies in parallel and are beginning to see positive results. We will provide another status update by Thursday, 2019-07-25 14:45 US/Pacific with current details.", "when": "2019-07-25T20:46:21Z"}, {"created": "2019-07-25T19:31:14Z", "modified": "2019-07-25T19:31:14Z", "text": "Mitigation work is currently still underway by our Engineering Team. We will provide another status update by Thursday, 2019-07-25 13:45 US/Pacific with current details.", "when": "2019-07-25T19:31:14Z"}, {"created": "2019-07-25T17:35:12Z", "modified": "2019-07-25T17:35:12Z", "text": "Our engineering team has identified a root cause, and mitigation work is currently underway. Efforts to accelerate recovery using the previous mitigation were not effective and the issue has been escalated internally, we are continuing to work on a new mitigation. We will provide another status update by Thursday, 2019-07-25 12:30 US/Pacific with current details.", "when": "2019-07-25T17:35:12Z"}, {"created": "2019-07-25T16:06:33Z", "modified": "2019-07-25T16:06:33Z", "text": "Our engineering team has identified a root cause, and mitigation work is currently underway. Mitigation appears effective, and we are working to accelerate recovery. We will provide another status update by Thursday, 2019-07-25 10:30 US/Pacific with current details.", "when": "2019-07-25T16:06:33Z"}, {"created": "2019-07-25T15:02:38Z", "modified": "2019-07-25T15:02:38Z", "text": "Our engineering team has identified a root cause, and mitigation work is currently underway.  We will provide another status update by Thursday, 2019-07-25 09:00 US/Pacific with current details.", "when": "2019-07-25T15:02:37Z"}, {"created": "2019-07-25T14:52:00Z", "modified": "2019-07-25T14:52:00Z", "text": "We are experiencing elevated deployment errors for projects which are deploying to GCF for the first time. Projects with existing deployments are unaffected unless they are being deployed to a new region. We will provide another status update by Thursday, 2019-07-25 08:30 US/Pacific with current details.", "when": "2019-07-25T14:52:00Z"}], "uri": "/incident/cloud-functions/19005"}, {"begin": "2019-07-25T12:13:38Z", "created": "2019-07-26T00:53:08Z", "end": "2019-07-26T02:59:55Z", "external_desc": "We've received a report of an issue with Google Kubernetes Engine.", "modified": "2019-08-01T18:39:42Z", "most-recent-update": {"created": "2019-08-01T18:39:42Z", "modified": "2019-08-01T18:39:42Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:39:42Z"}, "number": 19008, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-08-01T18:39:42Z", "modified": "2019-08-01T18:39:42Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:39:42Z"}, {"created": "2019-07-26T03:08:55Z", "modified": "2019-07-26T03:08:55Z", "text": "The issue with Google Kubernetes Engine errors when viewing clusters on the dashboard has been resolved for all affected projects as of Thursday, 2019-07-25 20:07 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T03:08:55Z"}, {"created": "2019-07-26T02:48:53Z", "modified": "2019-07-26T02:48:53Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Thursday, 2019-07-25 22:00 US/Pacific with current details.", "when": "2019-07-26T02:48:53Z"}, {"created": "2019-07-26T00:58:24Z", "modified": "2019-07-26T00:58:24Z", "text": "This is an update regarding a workaround for this issue. The next update will be provided by Thursday, 2019-07-25 20:00 US/Pacific.", "when": "2019-07-26T00:58:24Z"}, {"created": "2019-07-26T00:53:24Z", "modified": "2019-07-26T00:53:24Z", "text": "We are investigating errors with GKE dashboard. Our Engineering Team is investigating possible causes. We will provide another status update by Thursday, 2019-07-25 20:00 US/Pacific with current details.", "when": "2019-07-26T00:53:24Z"}], "uri": "/incident/container-engine/19008"}, {"begin": "2019-07-25T12:13:37Z", "created": "2019-07-25T22:03:37Z", "end": "2019-07-26T02:59:04Z", "external_desc": "Customers are not able to create new private IP instances in Cloud SQL.", "modified": "2019-08-01T18:46:15Z", "most-recent-update": {"created": "2019-08-01T18:46:15Z", "modified": "2019-08-01T18:46:15Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:46:15Z"}, "number": 19003, "public": true, "service_key": "cloud-sql", "service_name": "Google Cloud SQL", "severity": "medium", "updates": [{"created": "2019-08-01T18:46:15Z", "modified": "2019-08-01T18:46:15Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:46:15Z"}, {"created": "2019-07-26T01:17:05Z", "modified": "2019-07-26T01:17:05Z", "text": "The issue with Cloud SQL private IP instance creation has been resolved for all affected projects as of Thursday, 2019-07-25 18:16 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:17:04Z"}, {"created": "2019-07-25T23:51:56Z", "modified": "2019-07-25T23:51:56Z", "text": "The issue with Cloud SQL private IP instance creation should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:51:56Z"}, {"created": "2019-07-25T22:41:57Z", "modified": "2019-07-25T22:41:57Z", "text": "The rate of errors is decreasing. We will provide another status update by Thursday, 2019-07-25 16:45 US/Pacific with current details.", "when": "2019-07-25T22:41:57Z"}, {"created": "2019-07-25T22:04:09Z", "modified": "2019-07-25T22:04:09Z", "text": "Customers are not able to create new private IP instances in Cloud SQL. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T22:04:09Z"}], "uri": "/incident/cloud-sql/19003"}, {"begin": "2019-07-25T12:13:29Z", "created": "2019-07-26T02:00:48Z", "end": "2019-07-26T02:59:51Z", "external_desc": "Cloud Console Dashboard Errors", "modified": "2019-08-01T18:38:21Z", "most-recent-update": {"created": "2019-08-01T18:38:21Z", "modified": "2019-08-01T18:38:21Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:38:21Z"}, "number": 19007, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-08-01T18:38:21Z", "modified": "2019-08-01T18:38:21Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:38:21Z"}, {"created": "2019-07-26T03:13:51Z", "modified": "2019-07-26T03:13:51Z", "text": "The issue with Cloud Console Dashboard errors when viewing or performing operations has been resolved for all affected projects as of Thursday, 2019-07-25 20:11 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T03:13:51Z"}, {"created": "2019-07-26T02:53:35Z", "modified": "2019-07-26T02:53:35Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Thursday, 2019-07-25 22:00 US/Pacific with current details.", "when": "2019-07-26T02:53:35Z"}, {"created": "2019-07-26T02:05:53Z", "modified": "2019-07-26T02:05:53Z", "text": "We are investigating errors with the Cloud Console Dashboard. Our Engineering Team is currently working on mitigating the issue. We will provide another status update by Thursday, 2019-07-25 20:30 US/Pacific with current details.", "when": "2019-07-26T02:05:53Z"}, {"created": "2019-07-26T02:01:37Z", "modified": "2019-07-26T02:01:37Z", "text": "We are investigating errors with the Cloud Console Dashboard. Our Engineering Team is currently working on mitigating the issue. We will provide another status update by Thursday, 2019-07-25 20:30 US/Pacific with current details.", "when": "2019-07-26T02:01:37Z"}], "uri": "/incident/developers-console/19007"}, {"begin": "2019-07-25T12:13:28Z", "created": "2019-07-25T21:00:37Z", "end": "2019-07-26T02:59:22Z", "external_desc": "We've received a report of an issue with Cloud Firestore.", "modified": "2019-08-01T18:52:57Z", "most-recent-update": {"created": "2019-08-01T18:51:05Z", "modified": "2019-08-01T18:51:05Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:51:05Z"}, "number": 19002, "public": true, "service_key": "cloud-firestore", "service_name": "Cloud Firestore", "severity": "medium", "updates": [{"created": "2019-08-01T18:51:05Z", "modified": "2019-08-01T18:51:05Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:51:05Z"}, {"created": "2019-07-26T01:21:22Z", "modified": "2019-07-26T01:21:22Z", "text": "The issue with Cloud Firestore database creation has been resolved for all affected projects as of Thursday, 2019-07-25 18:20 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:21:22Z"}, {"created": "2019-07-25T23:46:49Z", "modified": "2019-07-25T23:46:49Z", "text": "The issue with Cloud Firestore database creation should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:46:49Z"}, {"created": "2019-07-25T22:41:14Z", "modified": "2019-07-25T22:41:14Z", "text": "The rate of errors is decreasing. We will provide another status update by Thursday, 2019-07-25 16:45 US/Pacific with current details.", "when": "2019-07-25T22:41:14Z"}, {"created": "2019-07-25T21:47:48Z", "modified": "2019-07-25T21:47:48Z", "text": "We are still seeing errors. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T21:47:48Z"}, {"created": "2019-07-25T21:00:37Z", "modified": "2019-07-25T21:50:28Z", "text": "Customers are not able to create new databases in Cloud Firestore. Our Engineers are attempting several mitigation strategies in parallel and are beginning to see positive results. We will provide another status update by Thursday, 2019-07-25 14:45 US/Pacific with current details.", "when": "2019-07-25T21:00:37Z"}], "uri": "/incident/cloud-firestore/19002"}, {"begin": "2019-07-25T12:13:22Z", "created": "2019-07-25T20:17:22Z", "end": "2019-07-26T02:59:02Z", "external_desc": "The issue with Google App Engine application creation should be resolved for the majority of users.", "modified": "2019-08-01T18:38:15Z", "most-recent-update": {"created": "2019-08-01T18:38:14Z", "modified": "2019-08-01T18:38:14Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:38:14Z"}, "number": 19009, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-08-01T18:38:14Z", "modified": "2019-08-01T18:38:14Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:38:14Z"}, {"created": "2019-07-26T01:13:02Z", "modified": "2019-07-26T01:13:02Z", "text": "The issue with Google App Engine application creation has been resolved for all affected projects as of Thursday, 2019-07-25 18:10 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:13:02Z"}, {"created": "2019-07-25T23:17:54Z", "modified": "2019-07-25T23:17:54Z", "text": "The issue with Google App Engine application creation should be resolved for the majority of projects. Currently, approximately 5% of requests are still failing. We expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:17:54Z"}, {"created": "2019-07-25T22:12:46Z", "modified": "2019-07-25T22:12:46Z", "text": "The issue with Google App Engine application creation should be resolved for the majority of users and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 16:15 US/Pacific with current details.", "when": "2019-07-25T22:12:46Z"}, {"created": "2019-07-25T21:45:22Z", "modified": "2019-07-25T21:45:22Z", "text": "We are still seeing errors. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T21:45:22Z"}, {"created": "2019-07-25T20:47:06Z", "modified": "2019-07-25T20:47:06Z", "text": "We are still seeing errors. Our Engineers are attempting several mitigation strategies in parallel and are beginning to see positive results. We will provide another status update by Thursday, 2019-07-25 14:45 US/Pacific with current details.", "when": "2019-07-25T20:47:06Z"}, {"created": "2019-07-25T20:17:34Z", "modified": "2019-07-25T20:17:34Z", "text": "We've received a report of an issue with application creation on Google App Engine. Mitigation work is currently underway by our Engineering Team. We will provide another status update by Thursday, 2019-07-25 13:45 US/Pacific with current details.", "when": "2019-07-25T20:17:34Z"}], "uri": "/incident/appengine/19009"}, {"begin": "2019-07-25T12:13:22Z", "created": "2019-07-25T15:35:40Z", "end": "2019-07-26T02:59:42Z", "external_desc": "We are mitigating an issue with creating and deleting Cloud Composer environments.", "modified": "2019-08-01T18:44:00Z", "most-recent-update": {"created": "2019-08-01T18:44:00Z", "modified": "2019-08-01T18:44:00Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:44:00Z"}, "number": 19001, "public": true, "service_key": "composer", "service_name": "Google Cloud Composer", "severity": "medium", "updates": [{"created": "2019-08-01T18:44:00Z", "modified": "2019-08-01T18:44:00Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:44:00Z"}, {"created": "2019-07-26T01:31:42Z", "modified": "2019-07-26T01:31:42Z", "text": "The issue with Cloud Composer environment creation/deletion errors has been resolved for all affected projects as of Thursday, 2019-07-25 18:31 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:31:42Z"}, {"created": "2019-07-25T23:51:06Z", "modified": "2019-07-25T23:51:06Z", "text": "The issue with Cloud Composer environment creation/deletion errors should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:51:06Z"}, {"created": "2019-07-25T23:42:26Z", "modified": "2019-07-25T23:42:26Z", "text": "The issue with Cloud Composer environment creation/deletion errors should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:42:26Z"}, {"created": "2019-07-25T22:39:26Z", "modified": "2019-07-25T22:39:26Z", "text": "The rate of errors is decreasing. We will provide another status update by Thursday, 2019-07-25 16:45 US/Pacific with current details.", "when": "2019-07-25T22:39:26Z"}, {"created": "2019-07-25T21:42:38Z", "modified": "2019-07-25T21:42:38Z", "text": "We are still seeing errors. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T21:42:38Z"}, {"created": "2019-07-25T20:43:52Z", "modified": "2019-07-25T20:43:52Z", "text": "We are still seeing errors. Our Engineers are attempting several mitigation strategies in parallel and are beginning to see positive results. We will provide another status update by Thursday, 2019-07-25 14:45 US/Pacific with current details.", "when": "2019-07-25T20:43:52Z"}, {"created": "2019-07-25T19:29:03Z", "modified": "2019-07-25T19:29:03Z", "text": "Mitigation work is currently still underway by our Engineering Team. We will provide another status update by Thursday, 2019-07-25 13:45 US/Pacific with current details.", "when": "2019-07-25T19:29:03Z"}, {"created": "2019-07-25T17:34:01Z", "modified": "2019-07-25T22:01:13Z", "text": "We are investigating errors with creating and deleting Cloud Composer environments. Our engineering team has identified a root cause, and mitigation work is currently underway. Efforts to accelerate recovery using the previous mitigation were not effective and the issue has been escalated internally, we are continuing to work on a new mitigation. We will provide another status update by Thursday, 2019-07-25 12:30 US/Pacific with current details.", "when": "2019-07-25T17:34:01Z"}, {"created": "2019-07-25T16:14:12Z", "modified": "2019-07-25T22:01:31Z", "text": "We are investigating errors with creating and deleting Cloud Composer environments. Our engineering team has identified a root cause, and mitigation work is currently underway. Mitigation appears effective, and we are working to accelerate recovery. We will provide another status update by Thursday, 2019-07-25 10:30 US/Pacific with current details.", "when": "2019-07-25T16:14:12Z"}, {"created": "2019-07-25T15:36:02Z", "modified": "2019-07-25T22:01:47Z", "text": "We are investigating errors with creating and deleting Cloud Composer environments. Our Engineering Team believes they have identified the root cause and mitigation work is underway. We will provide another status update by Thursday, 2019-07-25 09:30 US/Pacific with current details.", "when": "2019-07-25T15:36:02Z"}], "uri": "/incident/composer/19001"}, {"begin": "2019-07-25T12:13:03Z", "created": "2019-07-25T20:53:41Z", "end": "2019-07-26T02:59:09Z", "external_desc": "We've received a report of an issue with Cloud Datastore.", "modified": "2019-08-01T18:40:29Z", "most-recent-update": {"created": "2019-08-01T18:40:29Z", "modified": "2019-08-01T18:40:29Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:40:28Z"}, "number": 19004, "public": true, "service_key": "cloud-datastore", "service_name": "Google Cloud Datastore", "severity": "medium", "updates": [{"created": "2019-08-01T18:40:29Z", "modified": "2019-08-01T18:40:29Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:40:28Z"}, {"created": "2019-07-26T01:19:09Z", "modified": "2019-07-26T01:19:09Z", "text": "The issue with Cloud Datastore database creation has been resolved for all affected projects as of Thursday, 2019-07-25 18:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-26T01:19:09Z"}, {"created": "2019-07-25T23:43:32Z", "modified": "2019-07-25T23:43:32Z", "text": "The issue with Cloud Datastore database creation should be resolved for the majority of projects. Currently, approximately 5% of the requests are still failing. Our engineers are still working to mitigate the issue and we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-07-25 18:00 US/Pacific with current details.", "when": "2019-07-25T23:43:32Z"}, {"created": "2019-07-25T22:40:16Z", "modified": "2019-07-25T22:40:16Z", "text": "The rate of errors is decreasing. We will provide another status update by Thursday, 2019-07-25 16:45 US/Pacific with current details.", "when": "2019-07-25T22:40:16Z"}, {"created": "2019-07-25T21:47:00Z", "modified": "2019-07-25T21:47:00Z", "text": "We are still seeing errors. Our Engineers have implemented a mitigation and are now monitoring the situation before gradually rolling it out to a wider audience. We will provide another status update by Thursday, 2019-07-25 15:45 US/Pacific with current details.", "when": "2019-07-25T21:47:00Z"}, {"created": "2019-07-25T20:54:23Z", "modified": "2019-07-25T21:49:20Z", "text": "Customers are not able to create new databases in Cloud Datastore. Our Engineers are attempting several mitigation strategies in parallel and are beginning to see positive results. We will provide another status update by Thursday, 2019-07-25 14:45 US/Pacific with current details.", "when": "2019-07-25T20:54:23Z"}], "uri": "/incident/cloud-datastore/19004"}, {"begin": "2019-07-24T13:45:00Z", "created": "2019-07-24T15:23:38Z", "end": "2019-07-24T14:25:00Z", "external_desc": "We've received a report of an issue with Cloud Firestore", "modified": "2019-08-01T18:49:25Z", "most-recent-update": {"created": "2019-08-01T18:49:25Z", "modified": "2019-08-01T18:49:25Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:49:25Z"}, "number": 19001, "public": true, "service_key": "cloud-firestore", "service_name": "Cloud Firestore", "severity": "medium", "updates": [{"created": "2019-08-01T18:49:25Z", "modified": "2019-08-01T18:49:25Z", "text": "After further investigation, we have determined that the impact was minimal and affected a subset of users. We have conducted an internal investigation of this issue and made appropriate improvements to our systems to help prevent or minimize a future recurrence.", "when": "2019-08-01T18:49:25Z"}, {"created": "2019-07-24T15:23:38Z", "modified": "2019-07-24T15:23:38Z", "text": "The issue with Cloud Firestore elevated write errors has been resolved for all affected projects as of Wednesday, 2019-07-24 07:25 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-07-24T15:23:38Z"}], "uri": "/incident/cloud-firestore/19001"}, {"begin": "2019-07-15T17:24:04Z", "created": "2019-07-15T17:56:29Z", "end": "2019-07-15T19:15:54Z", "external_desc": "Issues with Google Cloud Storage Object Lifecycle Management on US multi-regional buckets.", "modified": "2019-07-15T19:15:54Z", "most-recent-update": {"created": "2019-07-15T19:15:54Z", "modified": "2019-07-15T19:15:54Z", "text": "The Google Cloud Storage issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-07-15T19:15:54Z"}, "number": 19005, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-07-15T19:15:54Z", "modified": "2019-07-15T19:15:54Z", "text": "The Google Cloud Storage issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-07-15T19:15:54Z"}, {"created": "2019-07-15T18:23:10Z", "modified": "2019-07-15T18:23:10Z", "text": "Mitigation work is currently underway by our Engineering Team and we are rolling back back a configuration change. We will provide another status update by Monday, 2019-07-15 12:30 US/Pacific with current details.", "when": "2019-07-15T18:23:10Z"}, {"created": "2019-07-15T17:56:46Z", "modified": "2019-07-15T17:56:46Z", "text": "The Google Cloud Storage service is experiencing issues with Object Lifecycle Management on multi-regional buckets in \"US\". We will provide another status update by Monday, 2019-07-15 12:00 US/Pacific with current details.", "when": "2019-07-15T17:56:46Z"}], "uri": "/incident/storage/19005"}, {"begin": "2019-07-08T20:23:00Z", "created": "2019-07-08T21:55:56Z", "end": "2019-07-09T03:43:32Z", "external_desc": "The Cloud Functions service is experiencing an elevated error rate with new and reconfigured functions.", "modified": "2019-07-09T03:43:32Z", "most-recent-update": {"created": "2019-07-09T03:43:32Z", "modified": "2019-07-09T03:43:32Z", "text": "The issue with Firebase Console elevated error rate with newly deployed or recently reconfigured deployments has been resolved for all affected users as of Monday, 2019-07-08 17:00 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-07-09T03:43:32Z"}, "number": 19004, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-07-09T03:43:32Z", "modified": "2019-07-09T03:43:32Z", "text": "The issue with Firebase Console elevated error rate with newly deployed or recently reconfigured deployments has been resolved for all affected users as of Monday, 2019-07-08 17:00 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-07-09T03:43:32Z"}, {"created": "2019-07-09T00:16:58Z", "modified": "2019-07-09T00:16:58Z", "text": "We believe the issue with Firebase Console deployments has been resolved for most affected projects as of Monday, 2019-07-08 16:25 US/Pacific. There is an ongoing backfill of tasks that we expect to complete over the next few hours. We will provide an update on the current status at Monday, 2019-07-08 22:00 US/Pacific", "when": "2019-07-09T00:16:58Z"}, {"created": "2019-07-08T22:51:03Z", "modified": "2019-07-08T22:51:03Z", "text": "Our Engineering Team believes they have identified the potential root cause of the errors and is working to mitigate. We will provide another status update by Monday, 2019-07-08 16:30 US/Pacific with current details.", "when": "2019-07-08T22:51:03Z"}, {"created": "2019-07-08T21:55:56Z", "modified": "2019-07-08T21:55:56Z", "text": "The Cloud Functions service is experiencing an elevated error rate with new and reconfigured functions. This includes cloud function deploys from the Firebase CLI. We will provide another status update by Monday, 2019-07-08 15:45 US/Pacific with current details.", "when": "2019-07-08T21:55:56Z"}], "uri": "/incident/cloud-functions/19004"}, {"begin": "2019-07-02T17:25:43Z", "created": "2019-07-02T17:25:52Z", "end": "2019-07-03T19:00:21Z", "external_desc": "Cloud Networking issues in us-east1", "modified": "2019-07-03T19:00:22Z", "most-recent-update": {"created": "2019-07-03T19:00:21Z", "modified": "2019-07-03T19:00:21Z", "text": "The disruption with Google Cloud Networking and Load Balancing has been resolved as of Wednesday, 2019-07-03 07:35 US/Pacific.\n\nThe physical damage to the fiber bundles serving some network paths in us-east1 have been repaired, and we have removed the elective routing mitigations previously in place and returned to normal routing in the region.\n\nWe will perform an internal review of the incident and make appropriate improvements so that our customers continue to be able to operate reliably during incidents such as this.", "when": "2019-07-03T19:00:21Z"}, "number": 19016, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-07-03T19:00:21Z", "modified": "2019-07-03T19:00:21Z", "text": "The disruption with Google Cloud Networking and Load Balancing has been resolved as of Wednesday, 2019-07-03 07:35 US/Pacific.\n\nThe physical damage to the fiber bundles serving some network paths in us-east1 have been repaired, and we have removed the elective routing mitigations previously in place and returned to normal routing in the region.\n\nWe will perform an internal review of the incident and make appropriate improvements so that our customers continue to be able to operate reliably during incidents such as this.", "when": "2019-07-03T19:00:21Z"}, {"created": "2019-07-02T23:05:16Z", "modified": "2019-07-02T23:05:16Z", "text": "The disruptions with Google Cloud Networking and Load Balancing have been root caused to physical damage to multiple concurrent fiber bundles serving network paths in us-east1, and we expect a full resolution within the next 24 hours. \n\nIn the meantime, we are electively rerouting traffic to ensure that customers' services will continue to operate reliably until the affected fiber paths are repaired. Some customers may observe elevated latency during this period.\n\nWe will provide another status update either as the situation warrants or by Wednesday, 2019-07-03 12:00 US/Pacific tomorrow.", "when": "2019-07-02T23:05:16Z"}, {"created": "2019-07-02T21:31:43Z", "modified": "2019-07-02T21:31:43Z", "text": "The disruptions with Google Cloud Networking and Load Balancing have been root caused to physical damage to multiple concurrent fiber bundles serving network paths in us-east1. We are electively rerouting some traffic to ensure that customers\u2019 services will continue to operate reliably until the affected fiber paths are repaired. Some customers will observe elevated latency during this period.\n\nWe will provide another status update by Tuesday, 2019-07-02 16:30 US/Pacific with current details.", "when": "2019-07-02T21:31:43Z"}, {"created": "2019-07-02T19:39:18Z", "modified": "2019-07-02T19:39:18Z", "text": "Mitigation work is currently underway to address the issue with Google Cloud Networking and Load Balancing in us-east1. The rate of errors has decreased, however some users may still notice elevated latency, and our Engineering Team is continuing to work on it. We will provide another status update by Tuesday, 2019-07-02 14:30 US/Pacific with current details.", "when": "2019-07-02T19:39:18Z"}, {"created": "2019-07-02T18:35:48Z", "modified": "2019-07-02T18:35:48Z", "text": "Mitigation work is currently underway by our Engineering Team to address the issue with Google Cloud Networking and Load Balancing in us-east1. The rate of errors is decreasing, however some users may still notice elevated latency. We will provide another status update by Tuesday, 2019-07-02 12:30 US/Pacific with current details.", "when": "2019-07-02T18:35:48Z"}, {"created": "2019-07-02T17:56:26Z", "modified": "2019-07-02T17:56:26Z", "text": "We continue to experience an issue with Cloud Networking and Load balancing within us-east1. Issue is currently mitigated and we are working towards a resolution. Customers may still observe traffic through Global Load balancers being directed away from backends in us-east1 at this time. We will provide an update by Tuesday, 2019-07-02 11:29 US/Pacific with current details.", "when": "2019-07-02T17:56:26Z"}, {"created": "2019-07-02T17:25:57Z", "modified": "2019-07-02T17:25:57Z", "text": "We continue to experience an issue with Cloud Networking within us-east1. Issue is currently mitigated and we are working towards a resolution. Customers may still observe traffic through Global Load-balancers being directed away from back-ends in us-east1 at this time. We will provide an update by Tuesday, 2019-07-02 11:29 US/Pacific with current details.", "when": "2019-07-02T17:25:57Z"}], "uri": "/incident/cloud-networking/19016"}, {"begin": "2019-07-02T14:36:19Z", "created": "2019-07-02T14:55:23Z", "end": "2019-07-02T16:12:15Z", "external_desc": "Capacity loss in us-east1 region", "modified": "2019-07-02T16:12:15Z", "most-recent-update": {"created": "2019-07-02T16:12:15Z", "modified": "2019-07-02T16:12:15Z", "text": "The issue with Cloud Networking has been resolved for all affected users as of Tuesday, 2019-07-02 09:11 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-07-02T16:12:15Z"}, "number": 19015, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-07-02T16:12:15Z", "modified": "2019-07-02T16:12:15Z", "text": "The issue with Cloud Networking has been resolved for all affected users as of Tuesday, 2019-07-02 09:11 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-07-02T16:12:15Z"}, {"created": "2019-07-02T15:50:42Z", "modified": "2019-07-02T15:50:42Z", "text": "The Cloud Networking service (Standard Tier) has lost multiple independent fiber links within us-east1 zone. Vendor has been notified and are currently investigating the issue. In order to restore service, we have reduced our network usage and prioritised customer workloads. We will provide another status update by Tuesday, 2019-07-02 09:38 US/Pacific with current details.", "when": "2019-07-02T15:50:42Z"}, {"created": "2019-07-02T14:55:51Z", "modified": "2019-07-02T14:55:51Z", "text": "The Cloud Networking service (Standard Tier) is experiencing external connectivity loss for all us-east1 zones and traffic between us-east1 and other regions has approximately 10% loss. We will provide another status update by Tuesday, 2019-07-02 08:48 US/Pacific with current details.", "when": "2019-07-02T14:55:51Z"}], "uri": "/incident/cloud-networking/19015"}, {"begin": "2019-06-26T22:08:22Z", "created": "2019-06-26T22:12:28Z", "end": "2019-06-26T22:37:10Z", "external_desc": "Cloud VPN endpoints in us-east1 and us-east4 experiencing elevated rates of packet loss.", "modified": "2019-06-26T22:37:10Z", "most-recent-update": {"created": "2019-06-26T22:37:10Z", "modified": "2019-06-26T22:37:10Z", "text": "The Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-26T22:37:10Z"}, "number": 19014, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-06-26T22:37:10Z", "modified": "2019-06-26T22:37:10Z", "text": "The Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-26T22:37:10Z"}, {"created": "2019-06-26T22:12:48Z", "modified": "2019-06-26T22:12:48Z", "text": "We are investigating an issue with elevated packet loss while using Cloud VPN endpoints in us-east1 and us-east4. We will provide more information by Wednesday, 2019-06-26 16:45 US/Pacific.", "when": "2019-06-26T22:12:48Z"}], "uri": "/incident/cloud-networking/19014"}, {"begin": "2019-06-25T13:49:19Z", "created": "2019-06-25T14:32:19Z", "end": "2019-06-25T14:42:40Z", "external_desc": "We've received a report of an issue with Google BigQuery.", "modified": "2019-06-25T14:56:35Z", "most-recent-update": {"created": "2019-06-25T14:55:40Z", "modified": "2019-06-25T14:55:40Z", "text": "The issue with Google BigQuery errors has been resolved for all affected projects as of Tuesday, 2019-06-25 07:42 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-25T14:55:40Z"}, "number": 19006, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-06-25T14:55:40Z", "modified": "2019-06-25T14:55:40Z", "text": "The issue with Google BigQuery errors has been resolved for all affected projects as of Tuesday, 2019-06-25 07:42 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-25T14:55:40Z"}, {"created": "2019-06-25T14:32:20Z", "modified": "2019-06-25T14:32:20Z", "text": "We've received a report of an issue with Google BigQuery query error rates in the US region as of Tuesday, 2019-06-25 07:19 US/Pacific.\nWe will provide more information by Tuesday, 2019-06-25 08:15 US/Pacific.", "when": "2019-06-25T14:32:20Z"}], "uri": "/incident/bigquery/19006"}, {"begin": "2019-06-24T16:16:03Z", "created": "2019-06-24T16:22:40Z", "end": "2019-06-24T16:38:24Z", "external_desc": "We've received a report of an issue with Google BigQuery.", "modified": "2019-06-24T16:38:24Z", "most-recent-update": {"created": "2019-06-24T16:38:24Z", "modified": "2019-06-24T16:38:24Z", "text": "The Google BigQuery issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-24T16:38:24Z"}, "number": 19005, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-06-24T16:38:24Z", "modified": "2019-06-24T16:38:24Z", "text": "The Google BigQuery issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-24T16:38:24Z"}, {"created": "2019-06-24T16:22:41Z", "modified": "2019-06-24T16:22:41Z", "text": "The Google BigQuery service is experiencing a 5% error rate on insert job requests. We will provide another status update by Monday, 2019-06-24 09:50 US/Pacific with current details.", "when": "2019-06-24T16:22:41Z"}], "uri": "/incident/bigquery/19005"}, {"begin": "2019-06-22T01:03:06Z", "created": "2019-06-22T01:10:52Z", "end": "2019-06-22T02:03:06Z", "external_desc": "We've received a report of an issue with Google Kubernetes Engine.", "modified": "2019-06-22T02:03:06Z", "most-recent-update": {"created": "2019-06-22T02:03:06Z", "modified": "2019-06-22T02:03:06Z", "text": "The Google Kubernetes Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-22T02:03:06Z"}, "number": 19007, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-06-22T02:03:06Z", "modified": "2019-06-22T02:03:06Z", "text": "The Google Kubernetes Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-22T02:03:06Z"}, {"created": "2019-06-22T01:25:28Z", "modified": "2019-06-22T01:25:28Z", "text": "The Google Kubernetes Engine service is reporting an issue that Node pools which have been created at or upgraded to version 1.12.7-gke.22 are not able to find the kube-proxy image, thus are not functional. We will provide another status update by Friday, 2019-06-21 19:20 US/Pacific with current details.", "when": "2019-06-22T01:25:28Z"}, {"created": "2019-06-22T01:14:49Z", "modified": "2019-06-22T01:14:49Z", "text": "The Google Kubernetes Engine service is reporting an issue that Node pools which have been created at or upgraded to version 1.12.7-gke.22 are not able to find the kube-proxy image, thus are not functional. We will provide another status update by Friday, 2019-06-21 19:20 US/Pacific with current details.", "when": "2019-06-22T01:14:49Z"}, {"created": "2019-06-22T01:11:01Z", "modified": "2019-06-22T01:11:01Z", "text": "We've received a report of an issue with Google Kubernetes Engine as of Friday, 2019-06-21 18:03 US/Pacific.\nWe will provide more information by Friday, 2019-06-21 18:45 US/Pacific.", "when": "2019-06-22T01:11:01Z"}], "uri": "/incident/container-engine/19007"}, {"begin": "2019-06-13T17:04:07Z", "created": "2019-06-13T17:06:37Z", "end": "2019-06-13T18:03:45Z", "external_desc": "We are investigating an issue with network connectivity in southamerica-east1", "modified": "2019-06-13T18:03:45Z", "most-recent-update": {"created": "2019-06-13T18:03:45Z", "modified": "2019-06-13T18:03:45Z", "text": "The issue with Cloud Networking increased packet loss into and out of southamerica-east1 has been resolved for all affected users as of Thursday, 2019-06-13 10:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-13T18:03:45Z"}, "number": 19013, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-06-13T18:03:45Z", "modified": "2019-06-13T18:03:45Z", "text": "The issue with Cloud Networking increased packet loss into and out of southamerica-east1 has been resolved for all affected users as of Thursday, 2019-06-13 10:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-13T18:03:45Z"}, {"created": "2019-06-13T17:26:53Z", "modified": "2019-06-13T17:26:53Z", "text": "We are investigating increased packet loss into and out of southamerica-east1. Impact to the region began 09:38 US/Pacific. Our Engineering Team believes they have identified the root cause for the packet loss and are working on mitigation. We will provide another status update by Thursday, 2019-06-13 11:30 US/Pacific with current details.", "when": "2019-06-13T17:26:53Z"}, {"created": "2019-06-13T17:06:41Z", "modified": "2019-06-13T17:06:41Z", "text": "We are investigating increased packet loss into and out of southamerica-east1. We will provide another status update by Thursday, 2019-06-13 10:45 US/Pacific with current details.", "when": "2019-06-13T17:06:41Z"}], "uri": "/incident/cloud-networking/19013"}, {"begin": "2019-06-12T15:09:16Z", "created": "2019-06-12T15:13:39Z", "end": "2019-06-12T15:37:37Z", "external_desc": "We are investigating a Cloud Networking issue in europe-west6", "modified": "2019-06-12T15:37:37Z", "most-recent-update": {"created": "2019-06-12T15:37:37Z", "modified": "2019-06-12T15:37:37Z", "text": "The issue with Cloud Networking in europe-west6 has been resolved for all affected users as of Wednesday, 2019-06-12 08:11 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T15:37:37Z"}, "number": 19012, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-06-12T15:37:37Z", "modified": "2019-06-12T15:37:37Z", "text": "The issue with Cloud Networking in europe-west6 has been resolved for all affected users as of Wednesday, 2019-06-12 08:11 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T15:37:37Z"}, {"created": "2019-06-12T15:13:45Z", "modified": "2019-06-12T15:13:45Z", "text": "We are experiencing an issue with Cloud Networking in europe-west6 beginning at Wednesday, 2019-06-12 07:45 US/Pacific. Current data indicates traffic to other regions may see approximately 12% packet loss. For everyone who is affected, we apologize for the disruption. We will provide an update by Wednesday, 2019-06-12 09:00 US/Pacific with current details.", "when": "2019-06-12T15:13:45Z"}], "uri": "/incident/cloud-networking/19012"}, {"begin": "2019-06-12T10:13:45Z", "created": "2019-06-12T11:24:14Z", "end": "2019-06-12T12:07:01Z", "external_desc": "We've Received A Report Of An Issue With Cloud Networking", "modified": "2019-06-12T12:07:02Z", "most-recent-update": {"created": "2019-06-12T12:07:01Z", "modified": "2019-06-12T12:07:01Z", "text": "The Cloud Networking issue is believed to be affecting a very small number of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-12T12:07:01Z"}, "number": 19011, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-06-12T12:07:01Z", "modified": "2019-06-12T12:07:01Z", "text": "The Cloud Networking issue is believed to be affecting a very small number of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-06-12T12:07:01Z"}, {"created": "2019-06-12T12:03:14Z", "modified": "2019-06-12T12:03:14Z", "text": "The Cloud Networking issue is currently affecting instances using the standard network tier in the following zones  europe-west1-b, us-central1-c, europe-west1-d, europe-west1-c, asia-northeast1-b, us-east4-a, us-east4-c asia-northeast1-a, us-west1-a, us-east4-b  and asia-northeast1-c . We will provide another status update by Wednesday, 2019-06-12 06:01 US/Pacific with current details..", "when": "2019-06-12T12:03:14Z"}, {"created": "2019-06-12T11:24:22Z", "modified": "2019-06-12T11:24:22Z", "text": "The Cloud Networking issue is currently affecting instances in the following zones  europe-west1-b us-central1-c, europe-west1-d, europe-west1-c, asia-northeast1-b, us-east4-a, us-east4-c asia-northeast1-a, us-west1-a, us-east4-b  and asia-northeast1-c . We will provide another status update by Wednesday, 2019-06-12 04:56 US/Pacific with current details.", "when": "2019-06-12T11:24:22Z"}], "uri": "/incident/cloud-networking/19011"}, {"begin": "2019-06-12T09:17:30Z", "created": "2019-06-12T10:10:09Z", "end": "2019-06-12T10:13:21Z", "external_desc": "We've received a report of an issue with Google Cloud Build.", "modified": "2019-06-12T10:13:21Z", "most-recent-update": {"created": "2019-06-12T10:13:21Z", "modified": "2019-06-12T10:13:21Z", "text": "The issue with Cloud Build errors has been resolved for all affected users as of Wednesday, 2019-06-12 03:12 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T10:13:21Z"}, "number": 19001, "public": true, "service_key": "cloud-dev-tools", "service_name": "Cloud Developer Tools", "severity": "medium", "updates": [{"created": "2019-06-12T10:13:21Z", "modified": "2019-06-12T10:13:21Z", "text": "The issue with Cloud Build errors has been resolved for all affected users as of Wednesday, 2019-06-12 03:12 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T10:13:21Z"}, {"created": "2019-06-12T10:10:14Z", "modified": "2019-06-12T10:10:14Z", "text": "Google Cloud functions deployments are experiencing a 20% error rate. Our engineering team is still investigating the issue.We will provide another status update by Wednesday, 2019-06-12 03:58 US/Pacific with current details.", "when": "2019-06-12T10:10:14Z"}], "uri": "/incident/cloud-dev-tools/19001"}, {"begin": "2019-06-12T09:10:17Z", "created": "2019-06-12T09:26:07Z", "end": "2019-06-12T10:15:22Z", "external_desc": "We've received a report of an issue with Google Cloud Functions.", "modified": "2019-06-12T10:15:23Z", "most-recent-update": {"created": "2019-06-12T10:15:22Z", "modified": "2019-06-12T10:15:22Z", "text": "The issue with Google Cloud Functions deployments has been resolved for all affected projects as of Wednesday, 2019-06-12 03:12 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T10:15:22Z"}, "number": 19003, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-06-12T10:15:22Z", "modified": "2019-06-12T10:15:22Z", "text": "The issue with Google Cloud Functions deployments has been resolved for all affected projects as of Wednesday, 2019-06-12 03:12 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-12T10:15:22Z"}, {"created": "2019-06-12T09:26:17Z", "modified": "2019-06-12T09:26:17Z", "text": "We are investigating timeouts with Google Cloud Functions deployments. Our Engineering Team is investigating possible causes. We will provide another status update by Wednesday, 2019-06-12 03:25 US/Pacific with current details.", "when": "2019-06-12T09:26:17Z"}], "uri": "/incident/cloud-functions/19003"}, {"begin": "2019-06-06T18:56:17Z", "created": "2019-06-07T02:30:05Z", "end": "2019-06-07T11:20:32Z", "external_desc": "Users trying to add API restrictions to an API key in Cloud Console may see an incomplete list of possible APIs to restrict their keys to.", "modified": "2019-06-07T11:20:32Z", "most-recent-update": {"created": "2019-06-07T11:20:32Z", "modified": "2019-06-07T11:20:32Z", "text": "The issue with Cloud Console API restrictions has been resolved for all affected users as of Friday, 2019-06-07 03:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-07T11:20:32Z"}, "number": 19006, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-06-07T11:20:32Z", "modified": "2019-06-07T11:20:32Z", "text": "The issue with Cloud Console API restrictions has been resolved for all affected users as of Friday, 2019-06-07 03:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-07T11:20:32Z"}, {"created": "2019-06-07T06:34:45Z", "modified": "2019-06-07T06:34:45Z", "text": "Summary: Users trying to add API restrictions to an API key in Cloud Console may see an incomplete list of possible APIs to restrict their keys to.\r\n\r\nDescription: The engineering team is still in the process of rolling back a configuration change to mitigate this issue. Currently there is no estimate as to when this will be completed. We will provide another status update by Friday, 2019-06-07 06:00 US/Pacific with current details.\r\n\r\nDiagnosis: If you are unable to see a complete list of possible APIs to restrict your keys to while trying to add API restrictions to an API key in Cloud Console, then you are affected by this issue.\r\n\r\nWorkaround: None at this time.", "when": "2019-06-07T02:23:00Z"}], "uri": "/incident/developers-console/19006"}, {"begin": "2019-06-04T21:23:32Z", "created": "2019-06-04T21:23:34Z", "end": "2019-06-04T22:31:43Z", "external_desc": "Cloud network programming delays in us-east1 and us-east4 and Load balancer health check failures in us-east4-a.", "modified": "2019-06-04T22:31:44Z", "most-recent-update": {"created": "2019-06-04T22:31:44Z", "modified": "2019-06-04T22:31:44Z", "text": "The issue with Cloud network programming delays and connectivity issues in us-east1 and us-east4, Load balancer health check failures in us-east4-a has been resolved for all affected users as of Tuesday, 2019-06-04 15:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-04T22:31:44Z"}, "number": 19010, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-06-04T22:31:44Z", "modified": "2019-06-04T22:31:44Z", "text": "The issue with Cloud network programming delays and connectivity issues in us-east1 and us-east4, Load balancer health check failures in us-east4-a has been resolved for all affected users as of Tuesday, 2019-06-04 15:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-06-04T22:31:44Z"}, {"created": "2019-06-04T21:45:05Z", "modified": "2019-06-04T21:45:05Z", "text": "Our Engineering Team believes they have identified the root cause of the issue and is working to mitigate. Current data indicate(s) that GCE VM creation, live instance migration and any changes to network programming might be delayed. Google Cloud Load Balancer and Internal Load Balancer backends may fail health checking in us-east4-a. Newly created VMs, Cloud VPN and Router in the affected regions may experience connectivity issues. We will provide another status update by Tuesday, 2019-06-04 16:00 US/Pacific with current details.", "when": "2019-06-04T21:45:05Z"}, {"created": "2019-06-04T21:23:37Z", "modified": "2019-06-04T21:23:37Z", "text": "We are experiencing an issue with Cloud network programming delays in us-east1 and us-east4 and Load balancer health check failures in us-east4-a beginning at Tuesday, 2019-06-04 13.26 US/Pacific. Current data indicate(s) that GCE VM creation, live instance migration and any changes to network programming might be delayed. Google Cloud Load Balancer and Internal Load Balancer backends may fail health checking in us-east4-a. Newly created VMs in the affected regions may experience connectivity issues. For everyone who is affected, we apologize for the disruption. We will provide an update by Tuesday, 2019-06-04 15:30 US/Pacific with current details.", "when": "2019-06-04T21:23:37Z"}, {"created": "2019-06-04T21:23:35Z", "modified": "2019-06-04T21:23:35Z", "text": "Cloud network programming delays in us-east1 and us-east4 and Load balancer health check failures in us-east4-a.", "when": "2019-06-04T21:23:35Z"}], "uri": "/incident/cloud-networking/19010"}, {"begin": "2019-06-02T18:45:24Z", "created": "2019-06-02T19:53:30Z", "end": "2019-06-02T22:40:28Z", "external_desc": "The network congestion issue in eastern USA, affecting Google Cloud, G Suite, and YouTube has been resolved for all affected users as of 4:00pm US/Pacific.", "modified": "2019-06-06T16:42:37Z", "most-recent-update": {"created": "2019-06-06T16:42:37Z", "modified": "2019-06-06T16:42:37Z", "text": "# ISSUE SUMMARY\r\nOn Sunday 2 June, 2019, Google Cloud projects running services in multiple US regions experienced elevated packet loss as a result of network congestion for a duration of between 3 hours 19 minutes, and 4 hours 25 minutes. The duration and degree of packet loss varied considerably from region to region and is explained in detail below. Other Google Cloud services which depend on Google's US network were also impacted, as were several non-Cloud Google services which could not fully redirect users to unaffected regions. Customers may have experienced increased latency, intermittent errors, and connectivity loss to instances in us-central1, us-east1, us-east4, us-west2, northamerica-northeast1, and southamerica-east1. Google Cloud instances in us-west1, and all European regions and Asian regions, did not experience regional network congestion.\r\n\r\nGoogle Cloud Platform services were affected until mitigation completed for each region, including: Google Compute Engine, App Engine, Cloud Endpoints, Cloud Interconnect, Cloud VPN, Cloud Console, Stackdriver Metrics, Cloud Pub/Sub, Bigquery, regional Cloud Spanner instances, and Cloud Storage regional buckets. G Suite services in these regions were also affected.\r\n\r\nWe apologize to our customers whose services or businesses were impacted during this incident, and we are taking immediate steps to improve the platform\u2019s performance and availability. A detailed assessment of impact is at the end of this report.\r\n\r\n\r\n# ROOT CAUSE AND REMEDIATION\r\nThis was a major outage, both in its scope and duration. As is always the case in such instances, multiple failures combined to amplify the impact.\r\n\r\nWithin any single physical datacenter location, Google's machines are segregated into multiple logical clusters which have their own dedicated cluster management software, providing resilience to failure of any individual cluster manager. Google's network control plane runs under the control of different instances of the same cluster management software; in any single location, again, multiple instances of that cluster management software are used, so that failure of any individual instance has no impact on network capacity.\r\n\r\nGoogle's cluster management software plays a significant role in automating datacenter maintenance events, like power infrastructure changes or network augmentation. Google's scale means that maintenance events are globally common, although rare in any single location. Jobs run by the cluster management software are labelled with an indication of how they should behave in the face of such an event: typically jobs are either moved to a machine which is not under maintenance, or stopped and rescheduled after the event.\r\n\r\nTwo normally-benign misconfigurations, and a specific software bug, combined to initiate the outage: firstly, network control plane jobs and their supporting infrastructure in the impacted regions were configured to be stopped in the face of a maintenance event. Secondly, the multiple instances of cluster management software running the network control plane were marked as eligible for inclusion in a particular, relatively rare maintenance event type. Thirdly, the software initiating maintenance events had a specific bug, allowing it to deschedule multiple independent software clusters at once, crucially even if those clusters were in different physical locations.\r\n\r\nThe outage progressed as follows: at 11:45 US/Pacific, the previously-mentioned maintenance event started in a single physical location; the automation software created a list of jobs to deschedule in that physical location, which included the logical clusters running network control jobs. Those logical clusters also included network control jobs in other physical locations. The automation then descheduled each in-scope logical cluster, including the network control jobs and their supporting infrastructure in multiple physical locations.\r\n\r\nGoogle's resilience strategy relies on the principle of defense in depth. Specifically, despite the network control infrastructure being designed to be highly resilient, the network is designed to 'fail static' and run for a period of time without the control plane being present as an additional line of defense against failure. The network ran normally for a short period - several minutes - after the control plane had been descheduled. After this period, BGP routing between specific impacted physical locations was withdrawn, resulting in the significant reduction in network capacity observed by our services and users, and the inaccessibility of some Google Cloud regions. End-user impact began to be seen in the period 11:47-11:49 US/Pacific.\r\n\r\nGoogle engineers were alerted to the failure two minutes after it began, and rapidly engaged the incident management protocols used for the most significant of production incidents. Debugging the problem was significantly hampered by failure of tools competing over use of the now-congested network. The defense in depth philosophy means we have robust backup plans for handling failure of such tools, but use of these backup plans (including engineers travelling to secure facilities designed to withstand the most catastrophic failures, and a reduction in priority of less critical network traffic classes to reduce congestion) added to the time spent debugging. Furthermore, the scope and scale of the outage, and collateral damage to tooling as a result of network congestion, made it initially difficult to precisely identify impact and communicate accurately with customers.\r\n\r\nAs of 13:01 US/Pacific, the incident had been root-caused, and engineers halted the automation software responsible for the maintenance event. We then set about re-enabling the network control plane and its supporting infrastructure. Additional problems once again extended the recovery time: with all instances of the network control plane descheduled in several locations, configuration data had been lost and needed to be rebuilt and redistributed. Doing this during such a significant network configuration event, for multiple locations, proved to be time-consuming. The new configuration began to roll out at 14:03.\r\n\r\nIn parallel with these efforts, multiple teams within Google applied mitigations specific to their services, directing traffic away from the affected regions to allow continued serving from elsewhere.\r\n\r\nAs the network control plane was rescheduled in each location, and the relevant configuration was recreated and distributed, network capacity began to come back online. Recovery of network capacity started at 15:19, and full service was resumed at 16:10 US/Pacific time.\r\n\r\nThe multiple concurrent failures which contributed to the initiation of the outage, and the prolonged duration, are the focus of a significant post-mortem process at Google which is designed to eliminate not just these specific issues, but the entire class of similar problems. Full details follow in the Prevention and Follow-Up section.\r\n\r\n\r\n# PREVENTION AND FOLLOW-UP\r\nWe have immediately halted the datacenter automation software which deschedules jobs in the face of maintenance events. We will re-enable this software only when we have ensured the appropriate safeguards are in place to avoid descheduling of jobs in multiple physical locations concurrently. Further, we will harden Google's cluster management software such that it rejects such requests regardless of origin, providing an additional layer of defense in depth and eliminating other similar classes of failure.\r\n\r\nGoogle's network control plane software and supporting infrastructure will be reconfigured such that it handles datacenter maintenance events correctly, by rejecting maintenance requests of the type implicated in this incident. Furthermore, the network control plane in any single location will be modified to persist its configuration so that the configuration does not need to be rebuilt and redistributed in the event of all jobs being descheduled. This will reduce recovery time by an order of magnitude. Finally, Google's network will be updated to continue in 'fail static' mode for a longer period in the event of loss of the control plane, to allow an adequate window for recovery with no user impact.\r\n\r\nGoogle's emergency response tooling and procedures will be reviewed, updated and tested to ensure that they are robust to network failures of this kind, including our tooling for communicating with the customer base. Furthermore, we will extend our continuous disaster recovery testing regime to include this and other similarly catastrophic failures.\r\n\r\nOur post-mortem process will be thorough and broad, and remains at a relatively early stage. Further action items may be identified as this process progresses.\r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n## Compute Engine\r\nCompute Engine instances in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were inaccessible for the duration of the incident, with recovery times as described above. \r\n\r\nInstance to instance packet loss for traffic on private IPs and internet traffic:\r\n\r\n  * us-east1 up to 33% packet loss from 11:38 to 12:17, up to 8% packet loss from 12:17 to 14:50.\r\n  * us-central1 spike of 9% packet loss immediately after 11:38 and subsiding by 12:05.\r\n  * us-west1 initial spikes up to 20% and 8.6% packet loss to us-east1 and us-central1 respectively, falling below 0.1% by 12:55. us-west1 to European regions saw an initial packet loss of up to 1.9%, with packet loss subsiding by 12:05. us-west1 to Asian regions did not see elevated packet loss.\r\n\r\nInstances accessing Google services via Google Private Access were largely unaffected.\r\n\r\nCompute Engine admin operations returned an average of 1.2% errors.\r\n\r\n## App Engine\r\nApp Engine applications hosted in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were unavailable for the duration of the disruption. The us-central region saw a 23.2% drop in requests per second (RPS). Requests that reached App Engine executed normally, while requests that did not returned client timeout errors.\r\n\r\n## Cloud Endpoints\r\nRequests to Endpoints services during the network incident experienced a spike in error rates up to 4.4% at the start of the incident, decreasing to 0.6% average error rate between 12:50 and 15:40, at 15:40 error rates decreased to less than 0.1%. A separate Endpoints incident was caused by this disruption and its impact extended beyond the resolution time above.\r\n\r\nFrom Sunday 2 June, 2019 12:00 until Tuesday 4 June, 2019 11:30, 50% of service configuration push workflows failed. For the duration of the Cloud Endpoints disruption, requests to existing Endpoints services continued to serve based on an existing configuration. Requests to new Endpoints services, created after the disruption start time, failed with 500 errors unless the ESP flag service_control_network_fail_open was enabled, which is disabled by default.\r\n\r\nSince Tuesday 4 June, 2019 11:30, service configuration pushes have been successful, but may take up to one hour to take effect. As a result, requests to new Endpoints services may return 500 errors for up to 1 hour after the configuration push. We expect to return to the expected sub-minute configuration propagation by Friday 7 June 2019. Customers who are running on platforms other than Google App Engine Flex can work around this by setting the ESP flag service_control_network_fail_open to true. For customers whose backend is running on Google App Engine Flex, there is no mitigation for the delayed config pushes available at this time.  \r\n\r\n## Cloud Interconnect\r\nCloud Interconnect reported packet loss ranging from 10% to 100% in affected regions during this incident. Interconnect Attachments in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 reported packet loss ranging from 50% to 100% from 11:45 to 16:10. As part of this packet loss, some BGP sessions also reported going down. During this time, monitoring statistics were inconsistent where the disruption impacted our monitoring as well as Stackdriver monitoring, noted below. As a result we currently estimate that us-east4, us-west2, northamerica-northeast1 and southamerica-east1 sustained heavy packet loss until recovery at approximately 16:10. Further, Interconnect Attachments located in us-west1, us-east1, and us-central1 but connecting from Interconnects located on the east coast (e.g. New York, Washington DC) saw 10-50% packet loss caused by congestion on Google\u2019s backbone in those geographies during this same time frame.\r\n\r\n## Cloud VPN\r\nCloud VPN gateways in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were unreachable for the duration of the incident. us-central1 VPN endpoints reported 25% packet loss and us-east1 endpoints reported 10% packet loss. VPN gateways in us-east4 recovered at 15:40. VPN gateways in us-west2, northamerica-northeast1 and southamerica-east1 recovered at 16:30. Additional intervention was required in us-west2, northamerica-northeast1 and southamerica-east1 to move the VPN control plane in these regions out of a fail-safe state, designed to protect existing gateways from potentially incorrect changes, caused by the disruption. \r\n\r\n## Cloud Console\r\nCloud Console customers may have seen pages load more slowly, partially or not at all. Impact was more severe for customers who were in the eastern US as the congested links were concentrated between central US and eastern US regions for the duration of the disruption.\r\n\r\n## Stackdriver Monitoring\r\nStackdriver Monitoring experienced a 5-10% drop in requests per second (RPS) for the duration of the event. Login failures to the Stackdriver Monitoring Frontend averaged 8.4% over the duration of the incident. The frontend was also loading with increased latency and encountering a 3.5% error rate when loading data in UI components.\r\n\r\n## Cloud Pub/Sub\r\nCloud Pub/Sub experienced Publish and Subscribe unavailability in the affected regions averaged over the duration of the incident:\r\n\r\n  * us-east4 publish requests reported 0.3% error rate and subscribe requests reported a 25% error rate.\r\n  * southamerica-east1 publish requests reported 11% error rate and subscribe requests reported a 36% error rate.\r\n  * northamerica-northeast1 publish requests reported a 6% error rate and subscribe requests reported a 31% error rate.\r\n  * us-west2 did not have a statistically significant change in usage.\r\n\r\nAdditional Subscribe unavailability was experienced in other regions on requests for messages stored in the affected Cloud regions. Analysis shows a 27% global drop in successful publish and subscribe requests during the disruption. There were two periods of global unavailability for Cloud Pub/Sub Admin operations (create/delete topic/subscriptions) . First from 11:50 to 12:05 and finally from 16:05 to 16:25.\r\n\r\n## BigQuery\r\nBigQuery saw an average error rate of 0.7% over the duration of the incident. Impact was greatest at the beginning of the incident, between 11:47 and 12:02 where jobs.insert API calls had an error rate of 27%. Streaming Inserts (tabledata.insertAll API calls) had an average error rate of less than 0.01% over the duration of the incident, peaking to 24% briefly between 11:47 and 12:02.\r\n\r\n## Cloud Spanner\r\nCloud Spanner in regions us-east4, us-west2, and northamerica-northeast1 were unavailable during the duration 11:48 to 15:44. We are continuing to investigate reports that multi-region nam3 was affected, as it involves impacted regions. Other regions' availability was not affected. Modest latency increases at the 50th percentile were observed in us-central1 and us-east1 regions for brief periods during the incident window; exact values were dependent on customer workload. Significant latency increases at the 99th percentile were observed:\r\n\r\n  * nam-eur-asia1 had 120 ms of additional latency from 13:50 to 15:20.\r\n  * nam3 had greater than 1 second of additional latency from 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 100 ms.\r\n  * nam6 had an additional 320 ms of latency between 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 130 ms.\r\n  * us-central1 had an additional 80 ms of latency between 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 10 ms.\r\n  * us-east1 had an additional 2 seconds of latency between 11:50 to 13:10, from 13:10 to 15:50 latency was increased by 250 ms.\r\n  * us-west1 had an additional 20 ms of latency between 11:50 to 14:10.\r\n\r\n## Cloud Storage\r\nCloud Storage average error rates for bucket locations during the incident are as follows. This data is the best available approximation of the error rate available at the time of publishing:\r\n\r\n  * us-west2 96.2%\r\n  * southamerica-east1 79.3%\r\n  * us-east4 62.4%\r\n  * northamerica-northeast1 43.4%\r\n  * us 3.5%\r\n  * us-east1 1.7%\r\n  * us-west1 1.2%\r\n  * us-central1 0.7%\r\n\r\n## G Suite\r\nThe impact on G Suite users was different from and generally lower than the impact on Google Cloud users due to differences in architecture and provisioning of these services. Please see the G Suite Status Dashboard (https://www.google.com/appsstatus) for details on affected G Suite services.\r\n\r\n\r\n# SLA CREDITS\r\nIf you believe your paid application experienced an SLA violation as a result of this incident, please populate the SLA credit request: https://support.google.com/cloud/contact/cloud_platform_sla\r\n\r\nA full list of all Google Cloud Platform Service Level Agreements can be found at https://cloud.google.com/terms/sla/.\r\n\r\nFor G Suite, please request an SLA credit through one of the Support channels: https://support.google.com/a/answer/104721\r\n\r\nG Suite Service Level Agreement can be found at https://gsuite.google.com/intl/en/terms/sla.html", "when": "2019-06-06T16:42:37Z"}, "number": 19009, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "high", "updates": [{"created": "2019-06-06T16:42:37Z", "modified": "2019-06-06T16:42:37Z", "text": "# ISSUE SUMMARY\r\nOn Sunday 2 June, 2019, Google Cloud projects running services in multiple US regions experienced elevated packet loss as a result of network congestion for a duration of between 3 hours 19 minutes, and 4 hours 25 minutes. The duration and degree of packet loss varied considerably from region to region and is explained in detail below. Other Google Cloud services which depend on Google's US network were also impacted, as were several non-Cloud Google services which could not fully redirect users to unaffected regions. Customers may have experienced increased latency, intermittent errors, and connectivity loss to instances in us-central1, us-east1, us-east4, us-west2, northamerica-northeast1, and southamerica-east1. Google Cloud instances in us-west1, and all European regions and Asian regions, did not experience regional network congestion.\r\n\r\nGoogle Cloud Platform services were affected until mitigation completed for each region, including: Google Compute Engine, App Engine, Cloud Endpoints, Cloud Interconnect, Cloud VPN, Cloud Console, Stackdriver Metrics, Cloud Pub/Sub, Bigquery, regional Cloud Spanner instances, and Cloud Storage regional buckets. G Suite services in these regions were also affected.\r\n\r\nWe apologize to our customers whose services or businesses were impacted during this incident, and we are taking immediate steps to improve the platform\u2019s performance and availability. A detailed assessment of impact is at the end of this report.\r\n\r\n\r\n# ROOT CAUSE AND REMEDIATION\r\nThis was a major outage, both in its scope and duration. As is always the case in such instances, multiple failures combined to amplify the impact.\r\n\r\nWithin any single physical datacenter location, Google's machines are segregated into multiple logical clusters which have their own dedicated cluster management software, providing resilience to failure of any individual cluster manager. Google's network control plane runs under the control of different instances of the same cluster management software; in any single location, again, multiple instances of that cluster management software are used, so that failure of any individual instance has no impact on network capacity.\r\n\r\nGoogle's cluster management software plays a significant role in automating datacenter maintenance events, like power infrastructure changes or network augmentation. Google's scale means that maintenance events are globally common, although rare in any single location. Jobs run by the cluster management software are labelled with an indication of how they should behave in the face of such an event: typically jobs are either moved to a machine which is not under maintenance, or stopped and rescheduled after the event.\r\n\r\nTwo normally-benign misconfigurations, and a specific software bug, combined to initiate the outage: firstly, network control plane jobs and their supporting infrastructure in the impacted regions were configured to be stopped in the face of a maintenance event. Secondly, the multiple instances of cluster management software running the network control plane were marked as eligible for inclusion in a particular, relatively rare maintenance event type. Thirdly, the software initiating maintenance events had a specific bug, allowing it to deschedule multiple independent software clusters at once, crucially even if those clusters were in different physical locations.\r\n\r\nThe outage progressed as follows: at 11:45 US/Pacific, the previously-mentioned maintenance event started in a single physical location; the automation software created a list of jobs to deschedule in that physical location, which included the logical clusters running network control jobs. Those logical clusters also included network control jobs in other physical locations. The automation then descheduled each in-scope logical cluster, including the network control jobs and their supporting infrastructure in multiple physical locations.\r\n\r\nGoogle's resilience strategy relies on the principle of defense in depth. Specifically, despite the network control infrastructure being designed to be highly resilient, the network is designed to 'fail static' and run for a period of time without the control plane being present as an additional line of defense against failure. The network ran normally for a short period - several minutes - after the control plane had been descheduled. After this period, BGP routing between specific impacted physical locations was withdrawn, resulting in the significant reduction in network capacity observed by our services and users, and the inaccessibility of some Google Cloud regions. End-user impact began to be seen in the period 11:47-11:49 US/Pacific.\r\n\r\nGoogle engineers were alerted to the failure two minutes after it began, and rapidly engaged the incident management protocols used for the most significant of production incidents. Debugging the problem was significantly hampered by failure of tools competing over use of the now-congested network. The defense in depth philosophy means we have robust backup plans for handling failure of such tools, but use of these backup plans (including engineers travelling to secure facilities designed to withstand the most catastrophic failures, and a reduction in priority of less critical network traffic classes to reduce congestion) added to the time spent debugging. Furthermore, the scope and scale of the outage, and collateral damage to tooling as a result of network congestion, made it initially difficult to precisely identify impact and communicate accurately with customers.\r\n\r\nAs of 13:01 US/Pacific, the incident had been root-caused, and engineers halted the automation software responsible for the maintenance event. We then set about re-enabling the network control plane and its supporting infrastructure. Additional problems once again extended the recovery time: with all instances of the network control plane descheduled in several locations, configuration data had been lost and needed to be rebuilt and redistributed. Doing this during such a significant network configuration event, for multiple locations, proved to be time-consuming. The new configuration began to roll out at 14:03.\r\n\r\nIn parallel with these efforts, multiple teams within Google applied mitigations specific to their services, directing traffic away from the affected regions to allow continued serving from elsewhere.\r\n\r\nAs the network control plane was rescheduled in each location, and the relevant configuration was recreated and distributed, network capacity began to come back online. Recovery of network capacity started at 15:19, and full service was resumed at 16:10 US/Pacific time.\r\n\r\nThe multiple concurrent failures which contributed to the initiation of the outage, and the prolonged duration, are the focus of a significant post-mortem process at Google which is designed to eliminate not just these specific issues, but the entire class of similar problems. Full details follow in the Prevention and Follow-Up section.\r\n\r\n\r\n# PREVENTION AND FOLLOW-UP\r\nWe have immediately halted the datacenter automation software which deschedules jobs in the face of maintenance events. We will re-enable this software only when we have ensured the appropriate safeguards are in place to avoid descheduling of jobs in multiple physical locations concurrently. Further, we will harden Google's cluster management software such that it rejects such requests regardless of origin, providing an additional layer of defense in depth and eliminating other similar classes of failure.\r\n\r\nGoogle's network control plane software and supporting infrastructure will be reconfigured such that it handles datacenter maintenance events correctly, by rejecting maintenance requests of the type implicated in this incident. Furthermore, the network control plane in any single location will be modified to persist its configuration so that the configuration does not need to be rebuilt and redistributed in the event of all jobs being descheduled. This will reduce recovery time by an order of magnitude. Finally, Google's network will be updated to continue in 'fail static' mode for a longer period in the event of loss of the control plane, to allow an adequate window for recovery with no user impact.\r\n\r\nGoogle's emergency response tooling and procedures will be reviewed, updated and tested to ensure that they are robust to network failures of this kind, including our tooling for communicating with the customer base. Furthermore, we will extend our continuous disaster recovery testing regime to include this and other similarly catastrophic failures.\r\n\r\nOur post-mortem process will be thorough and broad, and remains at a relatively early stage. Further action items may be identified as this process progresses.\r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n## Compute Engine\r\nCompute Engine instances in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were inaccessible for the duration of the incident, with recovery times as described above. \r\n\r\nInstance to instance packet loss for traffic on private IPs and internet traffic:\r\n\r\n  * us-east1 up to 33% packet loss from 11:38 to 12:17, up to 8% packet loss from 12:17 to 14:50.\r\n  * us-central1 spike of 9% packet loss immediately after 11:38 and subsiding by 12:05.\r\n  * us-west1 initial spikes up to 20% and 8.6% packet loss to us-east1 and us-central1 respectively, falling below 0.1% by 12:55. us-west1 to European regions saw an initial packet loss of up to 1.9%, with packet loss subsiding by 12:05. us-west1 to Asian regions did not see elevated packet loss.\r\n\r\nInstances accessing Google services via Google Private Access were largely unaffected.\r\n\r\nCompute Engine admin operations returned an average of 1.2% errors.\r\n\r\n## App Engine\r\nApp Engine applications hosted in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were unavailable for the duration of the disruption. The us-central region saw a 23.2% drop in requests per second (RPS). Requests that reached App Engine executed normally, while requests that did not returned client timeout errors.\r\n\r\n## Cloud Endpoints\r\nRequests to Endpoints services during the network incident experienced a spike in error rates up to 4.4% at the start of the incident, decreasing to 0.6% average error rate between 12:50 and 15:40, at 15:40 error rates decreased to less than 0.1%. A separate Endpoints incident was caused by this disruption and its impact extended beyond the resolution time above.\r\n\r\nFrom Sunday 2 June, 2019 12:00 until Tuesday 4 June, 2019 11:30, 50% of service configuration push workflows failed. For the duration of the Cloud Endpoints disruption, requests to existing Endpoints services continued to serve based on an existing configuration. Requests to new Endpoints services, created after the disruption start time, failed with 500 errors unless the ESP flag service_control_network_fail_open was enabled, which is disabled by default.\r\n\r\nSince Tuesday 4 June, 2019 11:30, service configuration pushes have been successful, but may take up to one hour to take effect. As a result, requests to new Endpoints services may return 500 errors for up to 1 hour after the configuration push. We expect to return to the expected sub-minute configuration propagation by Friday 7 June 2019. Customers who are running on platforms other than Google App Engine Flex can work around this by setting the ESP flag service_control_network_fail_open to true. For customers whose backend is running on Google App Engine Flex, there is no mitigation for the delayed config pushes available at this time.  \r\n\r\n## Cloud Interconnect\r\nCloud Interconnect reported packet loss ranging from 10% to 100% in affected regions during this incident. Interconnect Attachments in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 reported packet loss ranging from 50% to 100% from 11:45 to 16:10. As part of this packet loss, some BGP sessions also reported going down. During this time, monitoring statistics were inconsistent where the disruption impacted our monitoring as well as Stackdriver monitoring, noted below. As a result we currently estimate that us-east4, us-west2, northamerica-northeast1 and southamerica-east1 sustained heavy packet loss until recovery at approximately 16:10. Further, Interconnect Attachments located in us-west1, us-east1, and us-central1 but connecting from Interconnects located on the east coast (e.g. New York, Washington DC) saw 10-50% packet loss caused by congestion on Google\u2019s backbone in those geographies during this same time frame.\r\n\r\n## Cloud VPN\r\nCloud VPN gateways in us-east4, us-west2, northamerica-northeast1 and southamerica-east1 were unreachable for the duration of the incident. us-central1 VPN endpoints reported 25% packet loss and us-east1 endpoints reported 10% packet loss. VPN gateways in us-east4 recovered at 15:40. VPN gateways in us-west2, northamerica-northeast1 and southamerica-east1 recovered at 16:30. Additional intervention was required in us-west2, northamerica-northeast1 and southamerica-east1 to move the VPN control plane in these regions out of a fail-safe state, designed to protect existing gateways from potentially incorrect changes, caused by the disruption. \r\n\r\n## Cloud Console\r\nCloud Console customers may have seen pages load more slowly, partially or not at all. Impact was more severe for customers who were in the eastern US as the congested links were concentrated between central US and eastern US regions for the duration of the disruption.\r\n\r\n## Stackdriver Monitoring\r\nStackdriver Monitoring experienced a 5-10% drop in requests per second (RPS) for the duration of the event. Login failures to the Stackdriver Monitoring Frontend averaged 8.4% over the duration of the incident. The frontend was also loading with increased latency and encountering a 3.5% error rate when loading data in UI components.\r\n\r\n## Cloud Pub/Sub\r\nCloud Pub/Sub experienced Publish and Subscribe unavailability in the affected regions averaged over the duration of the incident:\r\n\r\n  * us-east4 publish requests reported 0.3% error rate and subscribe requests reported a 25% error rate.\r\n  * southamerica-east1 publish requests reported 11% error rate and subscribe requests reported a 36% error rate.\r\n  * northamerica-northeast1 publish requests reported a 6% error rate and subscribe requests reported a 31% error rate.\r\n  * us-west2 did not have a statistically significant change in usage.\r\n\r\nAdditional Subscribe unavailability was experienced in other regions on requests for messages stored in the affected Cloud regions. Analysis shows a 27% global drop in successful publish and subscribe requests during the disruption. There were two periods of global unavailability for Cloud Pub/Sub Admin operations (create/delete topic/subscriptions) . First from 11:50 to 12:05 and finally from 16:05 to 16:25.\r\n\r\n## BigQuery\r\nBigQuery saw an average error rate of 0.7% over the duration of the incident. Impact was greatest at the beginning of the incident, between 11:47 and 12:02 where jobs.insert API calls had an error rate of 27%. Streaming Inserts (tabledata.insertAll API calls) had an average error rate of less than 0.01% over the duration of the incident, peaking to 24% briefly between 11:47 and 12:02.\r\n\r\n## Cloud Spanner\r\nCloud Spanner in regions us-east4, us-west2, and northamerica-northeast1 were unavailable during the duration 11:48 to 15:44. We are continuing to investigate reports that multi-region nam3 was affected, as it involves impacted regions. Other regions' availability was not affected. Modest latency increases at the 50th percentile were observed in us-central1 and us-east1 regions for brief periods during the incident window; exact values were dependent on customer workload. Significant latency increases at the 99th percentile were observed:\r\n\r\n  * nam-eur-asia1 had 120 ms of additional latency from 13:50 to 15:20.\r\n  * nam3 had greater than 1 second of additional latency from 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 100 ms.\r\n  * nam6 had an additional 320 ms of latency between 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 130 ms.\r\n  * us-central1 had an additional 80 ms of latency between 11:50 to 13:10, from 13:10 to 16:50 latency was increased by 10 ms.\r\n  * us-east1 had an additional 2 seconds of latency between 11:50 to 13:10, from 13:10 to 15:50 latency was increased by 250 ms.\r\n  * us-west1 had an additional 20 ms of latency between 11:50 to 14:10.\r\n\r\n## Cloud Storage\r\nCloud Storage average error rates for bucket locations during the incident are as follows. This data is the best available approximation of the error rate available at the time of publishing:\r\n\r\n  * us-west2 96.2%\r\n  * southamerica-east1 79.3%\r\n  * us-east4 62.4%\r\n  * northamerica-northeast1 43.4%\r\n  * us 3.5%\r\n  * us-east1 1.7%\r\n  * us-west1 1.2%\r\n  * us-central1 0.7%\r\n\r\n## G Suite\r\nThe impact on G Suite users was different from and generally lower than the impact on Google Cloud users due to differences in architecture and provisioning of these services. Please see the G Suite Status Dashboard (https://www.google.com/appsstatus) for details on affected G Suite services.\r\n\r\n\r\n# SLA CREDITS\r\nIf you believe your paid application experienced an SLA violation as a result of this incident, please populate the SLA credit request: https://support.google.com/cloud/contact/cloud_platform_sla\r\n\r\nA full list of all Google Cloud Platform Service Level Agreements can be found at https://cloud.google.com/terms/sla/.\r\n\r\nFor G Suite, please request an SLA credit through one of the Support channels: https://support.google.com/a/answer/104721\r\n\r\nG Suite Service Level Agreement can be found at https://gsuite.google.com/intl/en/terms/sla.html", "when": "2019-06-06T16:42:37Z"}, {"created": "2019-06-04T14:09:52Z", "modified": "2019-06-04T16:18:01Z", "text": "Additional information on this service disruption has been published in the Google Cloud Blog: https://cloud.google.com/blog/topics/inside-google-cloud/an-update-on-sundays-service-disruption\r\n\r\nA formal incident report is still forthcoming.", "when": "2019-06-04T14:09:51Z"}, {"created": "2019-06-02T23:56:28Z", "modified": "2019-06-02T23:56:28Z", "text": "The network congestion issue in eastern USA, affecting Google Cloud, G Suite, and YouTube has been resolved for all affected users as of 4:00pm US/Pacific. \r\n\r\nWe will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a detailed report of this incident once we have completed our internal investigation. This detailed report will contain information regarding SLA credits.", "when": "2019-06-02T23:56:28Z"}, {"created": "2019-06-02T23:04:27Z", "modified": "2019-06-02T23:04:27Z", "text": "The network congestion issue affecting Google Cloud, G Suite, and YouTube is resolved for the vast majority of users, and we expect a full resolution in the near future. We will provide another status update by Sunday, 2019-06-02 17:00 US/Pacific with current details.", "when": "2019-06-02T23:04:27Z"}, {"created": "2019-06-02T22:04:07Z", "modified": "2019-06-02T22:04:07Z", "text": "We continue to experience high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. Our engineering teams have completed the first phase of their mitigation work and are currently implementing the second phase, after which we expect to return to normal service.  We will provide an update by Sunday, 2019-06-02 16:00 US/Pacific.", "when": "2019-06-02T22:04:07Z"}, {"created": "2019-06-02T21:59:34Z", "modified": "2019-06-02T21:59:34Z", "text": "We continue to experience high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. Our engineering teams have completed the first phase of their mitigation work and are currently implementing the second phase, after which we expect to return to normal service.  We will provide an update at 16:00 US/Pacific.", "when": "2019-06-02T21:59:34Z"}, {"created": "2019-06-02T21:08:53Z", "modified": "2019-06-02T21:08:53Z", "text": "We are experiencing high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. We believe we have identified the root cause of the congestion and expect to return to normal service shortly. We will provide more information by Sunday, 2019-06-02 15:00 US/Pacific", "when": "2019-06-02T21:08:53Z"}, {"created": "2019-06-02T21:08:38Z", "modified": "2019-06-02T21:08:38Z", "text": "We are experiencing high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. We believe we have identified the root cause of the congestion and expect to return to normal service shortly. We will provide more information by Sunday, 2019-06-02 15:00 US/Pacific", "when": "2019-06-02T21:08:38Z"}, {"created": "2019-06-02T21:08:21Z", "modified": "2019-06-02T21:08:21Z", "text": "We are experiencing high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. We believe we have identified the root cause of the congestion and expect to return to normal service shortly. We will provide more information by Sunday, 2019-06-02 15:00 US/Pacific", "when": "2019-06-02T21:08:21Z"}, {"created": "2019-06-02T20:45:58Z", "modified": "2019-06-02T20:45:58Z", "text": "We are experiencing high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. We believe we have identified the root cause of the congestion and expect to return to normal service shortly.", "when": "2019-06-02T20:45:58Z"}, {"created": "2019-06-02T19:53:31Z", "modified": "2019-06-02T19:53:31Z", "text": "We are investigating an issue with Google Cloud Networking. We will provide more information by Sunday, 2019-06-02 13:30 US/Pacific.", "when": "2019-06-02T19:53:31Z"}], "uri": "/incident/cloud-networking/19009"}, {"begin": "2019-06-02T18:45:20Z", "created": "2019-06-02T19:25:31Z", "end": "2019-06-02T22:40:00Z", "external_desc": "We are experiencing a multi-region issue with Google Compute Engine", "modified": "2019-06-06T16:47:53Z", "most-recent-update": {"created": "2019-06-06T16:47:23Z", "modified": "2019-06-06T16:47:53Z", "text": "A detailed incident report has been posted on the Google\r\nCloud Status Dashboard: https://status.cloud.google.com/incident/cloud-networking/19009", "when": "2019-06-06T16:47:23Z"}, "number": 19003, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "high", "updates": [{"created": "2019-06-06T16:47:23Z", "modified": "2019-06-06T16:47:53Z", "text": "A detailed incident report has been posted on the Google\r\nCloud Status Dashboard: https://status.cloud.google.com/incident/cloud-networking/19009", "when": "2019-06-06T16:47:23Z"}, {"created": "2019-06-04T14:11:09Z", "modified": "2019-06-04T16:18:31Z", "text": "Additional information on this service disruption has been published in the Google Cloud Blog: https://cloud.google.com/blog/topics/inside-google-cloud/an-update-on-sundays-service-disruption\r\n\r\nA formal incident report is still forthcoming.", "when": "2019-06-04T14:11:09Z"}, {"created": "2019-06-03T00:09:24Z", "modified": "2019-06-03T00:09:24Z", "text": "The network congestion issue in eastern USA, affecting Google Cloud, G Suite, and YouTube has been resolved for all affected users as of 4:00pm US/Pacific. \r\n\r\nWe will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a detailed report of this incident once we have completed our internal investigation. This detailed report will contain information regarding SLA credits.", "when": "2019-06-03T00:09:24Z"}, {"created": "2019-06-02T21:58:07Z", "modified": "2019-06-02T21:58:07Z", "text": "We continue to experience high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. Our engineering teams have completed the first phase of their mitigation work and are currently implementing the second phase, after which we expect to return to normal service.  We will provide an update at 16:00 US/Pacific.", "when": "2019-06-02T21:58:07Z"}, {"created": "2019-06-02T20:36:03Z", "modified": "2019-06-02T20:36:03Z", "text": "This issue is still ongoing \r\n\r\nWe are experiencing high levels of network congestion in the eastern USA, affecting multiple service in Google Cloud, GSuite and YouTube. Users may see slow performance or intermittent errors. We believe we have identified the root cause of the congestion and expect to a return to normal service shortly.", "when": "2019-06-02T20:36:03Z"}, {"created": "2019-06-02T19:59:17Z", "modified": "2019-06-02T19:59:17Z", "text": "Issue is related to a larger network issue", "when": "2019-06-02T19:59:17Z"}, {"created": "2019-06-02T19:25:37Z", "modified": "2019-06-02T19:25:37Z", "text": "We are investigating an issue with Google Compute Engine. We will provide more information by Sunday, 2019-06-02 12:45 US/Pacific.", "when": "2019-06-02T19:25:37Z"}], "uri": "/incident/compute/19003"}, {"begin": "2019-05-21T05:40:35Z", "created": "2019-05-21T05:40:36Z", "end": "2019-05-21T08:24:01Z", "external_desc": "We are investigating an issue with Google Cloud SQL", "modified": "2019-05-21T08:24:02Z", "most-recent-update": {"created": "2019-05-21T08:24:02Z", "modified": "2019-05-21T08:24:02Z", "text": "The issue with CloudSQL caused by a recent issue with PubSub (https://status.cloud.google.com/incident/cloud-pubsub/19001) has been resolved for all affected projects as of Tuesday, 2019-05-21 01:22 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-21T08:24:02Z"}, "number": 19002, "public": true, "service_key": "cloud-sql", "service_name": "Google Cloud SQL", "severity": "high", "updates": [{"created": "2019-05-21T08:24:02Z", "modified": "2019-05-21T08:24:02Z", "text": "The issue with CloudSQL caused by a recent issue with PubSub (https://status.cloud.google.com/incident/cloud-pubsub/19001) has been resolved for all affected projects as of Tuesday, 2019-05-21 01:22 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-21T08:24:02Z"}, {"created": "2019-05-21T06:45:44Z", "modified": "2019-05-21T07:10:24Z", "text": "We are still seeing errors on Cloud PubSub which is the underlying cause of the Cloud SQL admin operations failures. Please follow https://status.cloud.google.com/incident/cloud-pubsub/19001 for additional information. We will provide an update by Tuesday, 2019-05-21 02:00 US/Pacific with current details", "when": "2019-05-21T06:45:44Z"}, {"created": "2019-05-21T05:56:51Z", "modified": "2019-05-21T05:56:51Z", "text": "Current data indicates that approximately 60% of the Cloud SQL admin operations globally, including instance creation, are affected by the recent Cloud PubSub outage. For everyone who is affected, we apologize for the disruption. We will provide an update by Monday, 2019-05-20 23:55 US/Pacific with current details.", "when": "2019-05-21T05:56:51Z"}, {"created": "2019-05-21T05:40:37Z", "modified": "2019-05-21T05:40:37Z", "text": "We are investigating an issue with Google Cloud SQL. We will provide more information by Monday, 2019-05-20 23:00 US/Pacific.", "when": "2019-05-21T05:40:37Z"}], "uri": "/incident/cloud-sql/19002"}, {"begin": "2019-05-21T03:54:16Z", "created": "2019-05-21T04:51:17Z", "end": "2019-05-21T07:24:07Z", "external_desc": "We are investigating elevated error rate on publish/pull and various admin operations (i.e. create/delete/get topics) with Google Cloud PubSub globally.", "modified": "2019-05-28T20:14:04Z", "most-recent-update": {"created": "2019-05-28T20:13:14Z", "modified": "2019-05-28T20:13:14Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Monday 20 May, 2019, Google Cloud Pub/Sub experienced publish error rates of 1.2%, increased publish latency by 1.7ms at the 50th percentile, and 8.3s increase at the 99th percentile for a duration of 3 hours, 30 minutes. Publish and Subscribe admin operations saw average error rates of 8.3% and 3.2% respectively for the same period. We apologize to our customers who were impacted by this service degradation.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Monday 20 May, 2019 from 20:54 to Tuesday 21 May, 2019 00:24 US/Pacific Google Cloud Pub/Sub experienced publish error rates of 1.2%, increased publish latency by 1.7ms at the 50th percentile, and 8.3s increase at the 99th percentile for a duration of 3 hours, 30 minutes. Publish (CreateTopic, GetTopic, UpdateTopic, DeleteTopic) and Subscribe (CreateSnapshot, CreateSubscription, UpdateSubscription) admin operations saw average error rates of 8.3% and 3.2% respectively for the same period. Customers affected by the incident may have seen errors containing messages like \u201cDEADLINE_EXCEEDED\u201d. \r\n\r\nCloud Pub/Sub\u2019s elevated error rates and increased latency indirectly impacted Cloud SQL, Cloud Filestore, and App Engine Task Queues globally. The incident caused elevated error rates in admin operations (including instance creation) for both Cloud SQL and Cloud Filestore, as well as increased latencies and timeout errors for App Engine Task Queues during the incident.\r\n\r\n# ROOT CAUSE\r\n\r\nThe incident was triggered by an internal user creating an unexpected surge of publish requests to Cloud Pub/Sub topics. These requests did not cache as expected and led to hotspotting on the underlying metadata storage system responsible for managing Cloud Pub/Sub\u2019s publish and subscribe operations. The hotspotting triggered overload protection mechanisms within the storage system which began to reject some incoming requests and delay the processing of others, both of which contributed to the elevated error rates and increased latencies experienced by Cloud Pub/Sub.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nOn Monday 20 May, 2019 at 21:16 US/Pacific Google engineers were automatically alerted to elevated error rates and immediately began their investigation. At 22:18, we determined the underlying storage system was responsible for the elevated error rates afflicting Cloud Pub/Sub and escalated the issue to the storage system\u2019s engineering team. At 22:48, Google engineers attempted to mitigate the issue by providing additional resources to the impacted storage system servers, however, this did not address the hotspots and error rates remained elevated. At 23:00, Google engineers disabled non-essential internal traffic to reduce load being sent to the storage system, this improved system behavior, but did not lead to a full recovery. On Tuesday 21 May, 2019 at 00:19 US/Pacific, Google engineers identified the source for the surge of requests and implemented a rate limit on the requests, which effectively mitigated the issue. Once the traffic had subsided, the storage system\u2019s automated mechanisms were able to successfully heal the service, leading to full resolution of the incident by 00:24.\r\n\r\nIn order to prevent a recurrence of the incident we are adding an additional layer of caching to further reduce load on the metadata storage system. We are preemptively increasing the number of storage servers to improve isolation, improve load distribution, and reduce the effect hotspotting may have. We are reviewing the schema of the storage system to improve load distribution. Finally we will be improving our playbooks with learnings from this incident, specifically improving sections around rate limiting, load shedding and hotspot detection.", "when": "2019-05-28T20:13:14Z"}, "number": 19001, "public": true, "service_key": "cloud-pubsub", "service_name": "Google Cloud Pub/Sub", "severity": "high", "updates": [{"created": "2019-05-28T20:13:14Z", "modified": "2019-05-28T20:13:14Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Monday 20 May, 2019, Google Cloud Pub/Sub experienced publish error rates of 1.2%, increased publish latency by 1.7ms at the 50th percentile, and 8.3s increase at the 99th percentile for a duration of 3 hours, 30 minutes. Publish and Subscribe admin operations saw average error rates of 8.3% and 3.2% respectively for the same period. We apologize to our customers who were impacted by this service degradation.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Monday 20 May, 2019 from 20:54 to Tuesday 21 May, 2019 00:24 US/Pacific Google Cloud Pub/Sub experienced publish error rates of 1.2%, increased publish latency by 1.7ms at the 50th percentile, and 8.3s increase at the 99th percentile for a duration of 3 hours, 30 minutes. Publish (CreateTopic, GetTopic, UpdateTopic, DeleteTopic) and Subscribe (CreateSnapshot, CreateSubscription, UpdateSubscription) admin operations saw average error rates of 8.3% and 3.2% respectively for the same period. Customers affected by the incident may have seen errors containing messages like \u201cDEADLINE_EXCEEDED\u201d. \r\n\r\nCloud Pub/Sub\u2019s elevated error rates and increased latency indirectly impacted Cloud SQL, Cloud Filestore, and App Engine Task Queues globally. The incident caused elevated error rates in admin operations (including instance creation) for both Cloud SQL and Cloud Filestore, as well as increased latencies and timeout errors for App Engine Task Queues during the incident.\r\n\r\n# ROOT CAUSE\r\n\r\nThe incident was triggered by an internal user creating an unexpected surge of publish requests to Cloud Pub/Sub topics. These requests did not cache as expected and led to hotspotting on the underlying metadata storage system responsible for managing Cloud Pub/Sub\u2019s publish and subscribe operations. The hotspotting triggered overload protection mechanisms within the storage system which began to reject some incoming requests and delay the processing of others, both of which contributed to the elevated error rates and increased latencies experienced by Cloud Pub/Sub.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nOn Monday 20 May, 2019 at 21:16 US/Pacific Google engineers were automatically alerted to elevated error rates and immediately began their investigation. At 22:18, we determined the underlying storage system was responsible for the elevated error rates afflicting Cloud Pub/Sub and escalated the issue to the storage system\u2019s engineering team. At 22:48, Google engineers attempted to mitigate the issue by providing additional resources to the impacted storage system servers, however, this did not address the hotspots and error rates remained elevated. At 23:00, Google engineers disabled non-essential internal traffic to reduce load being sent to the storage system, this improved system behavior, but did not lead to a full recovery. On Tuesday 21 May, 2019 at 00:19 US/Pacific, Google engineers identified the source for the surge of requests and implemented a rate limit on the requests, which effectively mitigated the issue. Once the traffic had subsided, the storage system\u2019s automated mechanisms were able to successfully heal the service, leading to full resolution of the incident by 00:24.\r\n\r\nIn order to prevent a recurrence of the incident we are adding an additional layer of caching to further reduce load on the metadata storage system. We are preemptively increasing the number of storage servers to improve isolation, improve load distribution, and reduce the effect hotspotting may have. We are reviewing the schema of the storage system to improve load distribution. Finally we will be improving our playbooks with learnings from this incident, specifically improving sections around rate limiting, load shedding and hotspot detection.", "when": "2019-05-28T20:13:14Z"}, {"created": "2019-05-21T08:13:08Z", "modified": "2019-05-21T08:13:08Z", "text": "The issue affecting PubSub and causing elevated error rate on multiple operations has been resolved for all affected users as of Tuesday, 2019-05-21 01:02 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-05-21T08:13:08Z"}, {"created": "2019-05-21T07:41:37Z", "modified": "2019-05-21T07:41:37Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-05-21 01:40 US/Pacific with current details.", "when": "2019-05-21T07:41:37Z"}, {"created": "2019-05-21T06:39:23Z", "modified": "2019-05-21T06:39:23Z", "text": "We are still seeing errors on publish/pull and admin operations on Cloud PubSub. Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-05-21 00:40 US/Pacific with current details.", "when": "2019-05-21T06:39:23Z"}, {"created": "2019-05-21T05:38:06Z", "modified": "2019-05-21T05:38:06Z", "text": "Further investigation indicates that approximately 3% pull and 5% publish operations globally are seeing 5xx and 499 errors. Various admin operations like CreateTopic and DeleteTopic are also seeing >50% errors. We will provide an update by Monday, 2019-05-20 23:40 US/Pacific with current details.", "when": "2019-05-21T05:38:06Z"}, {"created": "2019-05-21T05:16:23Z", "modified": "2019-05-21T05:16:23Z", "text": "We are experiencing an issue with Cloud PubSub beginning at 2019-05-20 21:35 US/Pacific. Current data indicates that approximately 1% of publish operations globally (15% publish operations in asia-northeast2) and approximately 20~25% of various types of admin operations (i.e. CreateTopic) are affected by this issue. For everyone who is affected, we apologize for the disruption. We will provide an update by Monday, 2019-05-20 22:45 US/Pacific with current details.", "when": "2019-05-21T05:16:23Z"}, {"created": "2019-05-21T04:51:19Z", "modified": "2019-05-21T04:51:19Z", "text": "We are investigating elevated error rate on create/delete/get topics with Google Cloud PubSub globally. Our Engineering Team is investigating possible causes. We will provide another status update by Monday, 2019-05-20 22:15 US/Pacific with current details.", "when": "2019-05-21T04:51:19Z"}, {"created": "2019-05-21T04:51:18Z", "modified": "2019-05-21T04:51:18Z", "text": "We are investigating elevated error rate on create/delete/get topics with Google Cloud PubSub globally.", "when": "2019-05-21T04:51:18Z"}], "uri": "/incident/cloud-pubsub/19001"}, {"begin": "2019-05-20T13:50:00Z", "created": "2019-05-20T14:10:37Z", "end": "2019-05-20T14:30:04Z", "external_desc": "We've received a report of an issue with accessing Google Cloud Support", "modified": "2019-05-20T14:51:33Z", "most-recent-update": {"created": "2019-05-20T14:51:05Z", "modified": "2019-05-20T14:51:05Z", "text": "The issue with Google Cloud Support cases being inaccessible has been resolved for all affected\r\nusers as of Monday, 2019-05-20 07:30 US/Pacific. We will conduct\r\nan internal investigation of this issue and make appropriate improvements to our\r\nsystems to help prevent or minimize future recurrence.", "when": "2019-05-20T14:51:04Z"}, "number": 19003, "public": true, "service_key": "support", "service_name": "Google Cloud Support", "severity": "high", "updates": [{"created": "2019-05-20T14:51:05Z", "modified": "2019-05-20T14:51:05Z", "text": "The issue with Google Cloud Support cases being inaccessible has been resolved for all affected\r\nusers as of Monday, 2019-05-20 07:30 US/Pacific. We will conduct\r\nan internal investigation of this issue and make appropriate improvements to our\r\nsystems to help prevent or minimize future recurrence.", "when": "2019-05-20T14:51:04Z"}, {"created": "2019-05-20T14:34:07Z", "modified": "2019-05-20T14:34:07Z", "text": "The issue with Google Cloud Support cases being inaccessible should be resolved for the majority of users and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-05-20 08:30 US/Pacific with current details.", "when": "2019-05-20T14:34:07Z"}, {"created": "2019-05-20T14:10:37Z", "modified": "2019-05-20T14:10:37Z", "text": "We are investigating an issue with Google Cloud Support. Support cases may be inaccessible. We will provide more information by Monday, 2019-05-20 07:45 US/Pacific.", "when": "2019-05-20T14:10:37Z"}], "uri": "/incident/support/19003"}, {"begin": "2019-05-17T17:08:00Z", "created": "2019-05-17T17:37:51Z", "end": "2019-05-17T21:07:21Z", "external_desc": "Issue with Google Cloud Support", "modified": "2019-05-31T21:57:10Z", "most-recent-update": {"created": "2019-05-31T21:55:25Z", "modified": "2019-05-31T21:55:25Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 17 May, 2019, Google Cloud Support experienced case creation and read issues for a duration of 4 hours. We sincerely apologize to our customers for the difficulties and delays in communicating with Google Cloud Support during this time.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 17 May, 2019 from 10:08 to 14:07 US/Pacific, both Google Cloud Support and our customers were unable to view, create, and modify Support cases, utilize Chat Support, or view known issues via the Google Cloud Support Center (GCSC) and the Cloud Console. Phone support was also unavailable from 10:08 to 14:26. \r\n\r\n\r\nROOT CAUSE\r\n\r\nA critical external dependency for Google Cloud Support encountered an issue resulting in service unavailability, which impacted both Google\u2019s and Google Cloud Platform customers\u2019 ability to use the service. This impacted case creation for chat, phone, and email cases, which all rely on the external dependency to reference customer data and route to the correct channel. Unfortunately, our tooling also did not allow for an automated mitigation.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle\u2019s engineers were alerted to the problem at 10:26 and escalated immediately. Once the nature and scope of the situation became clear, Cloud Support identified customer communication options through secondary channels. Customers were able to file cases through the backup \u201cContact Support\u201d mechanism that was automatically available in the Google Cloud Support Center (GCSC). All cases reported through the backup mechanism were routed internally to the appropriate Support Engineers. \r\n\r\nWe will refactor Cloud Support\u2019s Business Continuity Plan to account for issues of this nature, to ensure that:\r\n- Existing cases can continue to be viewed and updated\r\n- Chat Support will be available\r\n- Phone support will be available", "when": "2019-05-31T21:55:25Z"}, "number": 19002, "public": true, "service_key": "support", "service_name": "Google Cloud Support", "severity": "high", "updates": [{"created": "2019-05-31T21:55:25Z", "modified": "2019-05-31T21:55:25Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 17 May, 2019, Google Cloud Support experienced case creation and read issues for a duration of 4 hours. We sincerely apologize to our customers for the difficulties and delays in communicating with Google Cloud Support during this time.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 17 May, 2019 from 10:08 to 14:07 US/Pacific, both Google Cloud Support and our customers were unable to view, create, and modify Support cases, utilize Chat Support, or view known issues via the Google Cloud Support Center (GCSC) and the Cloud Console. Phone support was also unavailable from 10:08 to 14:26. \r\n\r\n\r\nROOT CAUSE\r\n\r\nA critical external dependency for Google Cloud Support encountered an issue resulting in service unavailability, which impacted both Google\u2019s and Google Cloud Platform customers\u2019 ability to use the service. This impacted case creation for chat, phone, and email cases, which all rely on the external dependency to reference customer data and route to the correct channel. Unfortunately, our tooling also did not allow for an automated mitigation.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle\u2019s engineers were alerted to the problem at 10:26 and escalated immediately. Once the nature and scope of the situation became clear, Cloud Support identified customer communication options through secondary channels. Customers were able to file cases through the backup \u201cContact Support\u201d mechanism that was automatically available in the Google Cloud Support Center (GCSC). All cases reported through the backup mechanism were routed internally to the appropriate Support Engineers. \r\n\r\nWe will refactor Cloud Support\u2019s Business Continuity Plan to account for issues of this nature, to ensure that:\r\n- Existing cases can continue to be viewed and updated\r\n- Chat Support will be available\r\n- Phone support will be available", "when": "2019-05-31T21:55:25Z"}, {"created": "2019-05-17T21:16:21Z", "modified": "2019-05-17T21:16:21Z", "text": "The issue with users being unable to create new cases has been resolved for all users. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence", "when": "2019-05-17T21:16:21Z"}, {"created": "2019-05-17T17:37:51Z", "modified": "2019-05-17T17:37:51Z", "text": "We are experiencing an issue with Google Cloud Support in which users are unable to create new support cases.", "when": "2019-05-17T17:37:51Z"}], "uri": "/incident/support/19002"}, {"begin": "2019-05-17T16:02:14Z", "created": "2019-05-17T16:02:14Z", "end": "2019-05-17T20:38:25Z", "external_desc": "Google BigQuery users experiencing elevated latency and error rates in US multi-region.", "modified": "2019-05-24T04:00:12Z", "most-recent-update": {"created": "2019-05-24T04:00:12Z", "modified": "2019-05-24T04:00:12Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday, May 17 2019, 83% of Google BigQuery insert jobs in the US multi-region failed for a duration of 27 minutes. Query jobs experienced an average error rate of 16.7% for a duration of 2 hours. BigQuery users in the US multi-region also observed elevated latency for a duration of 4 hours and 40 minutes. To our BigQuery customers whose business analytics were impacted during this outage, we sincerely apologize \u2013 this is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday May 17 2019, from 08:30 to 08:57 US/Pacific, 83% of Google BigQuery insert jobs failed for 27 minutes in the US multi-region. From 07:30 to 09:30 US/Pacific, query jobs in US multi-region returned an average error rate of 16.7%. Other jobs such as list, cancel, get, and getQueryResults in the US multi-region were also affected for 2 hours along with query jobs. Google BigQuery users observed elevated latencies for job completion from 07:30 to 12:10 US/Pacific. BigQuery jobs in regions outside of the US remained unaffected. \r\n\r\nROOT CAUSE\r\n\r\nThe incident was triggered by a sudden increase in queries in US multi-region leading to quota exhaustion in the storage system serving incoming requests. Detecting the sudden increase, BigQuery initiated its auto-defense mechanism and redirected user requests to a different location. The high load of requests triggered an issue in the scheduling system, causing delays in scheduling incoming queries. These delays resulted in errors for query, insert, list, cancel, get and getQueryResults BigQuery jobs and overall latency experienced by users. As a result of these high number of requests at 08:30 US/Pacific, the scheduling system\u2019s overload protection mechanism began rejecting further incoming requests, causing insert job failures for 27 minutes. \r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nBigQuery\u2019s defense mechanism began redirection at 07:50 US/Pacific. Google Engineers were automatically alerted at 07:54 US/Pacific and began investigation. The issue with the scheduler system began at 08:00 and our engineers were alerted again at 08:10. At 08:43, they restarted the scheduling system which mitigated the insert job failures by 08:57. Errors seen for insert, query, cancel, list, get and getQueryResults jobs  were mitigated by 09:30 when queries were redirected to different locations. Google engineers then successfully blocked the source of sudden incoming queries that helped reduce overall latency. The issue was fully resolved at 12:10 US/Pacific when all active and pending queries completed running.\r\n\r\nWe will resolve the issue that caused the scheduling system to delay scheduling of incoming queries. Although the overload protection mechanism prevented the incident from spreading globally, it did cause the failures for insert jobs. We will be improving this mechanism by lowering deadline for synchronous queries which will help prevent queries from piling up and overloading the scheduling system.  To prevent future recurrence of the issue we will also implement changes to improve BigQuery\u2019s quota exhaustion behaviour that would prevent the storage system to take on more load than it can handle. To reduce the duration of similar incidents, we will implement tools to quickly remediate backlogged queries.", "when": "2019-05-24T04:00:12Z"}, "number": 19003, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-05-24T04:00:12Z", "modified": "2019-05-24T04:00:12Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday, May 17 2019, 83% of Google BigQuery insert jobs in the US multi-region failed for a duration of 27 minutes. Query jobs experienced an average error rate of 16.7% for a duration of 2 hours. BigQuery users in the US multi-region also observed elevated latency for a duration of 4 hours and 40 minutes. To our BigQuery customers whose business analytics were impacted during this outage, we sincerely apologize \u2013 this is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday May 17 2019, from 08:30 to 08:57 US/Pacific, 83% of Google BigQuery insert jobs failed for 27 minutes in the US multi-region. From 07:30 to 09:30 US/Pacific, query jobs in US multi-region returned an average error rate of 16.7%. Other jobs such as list, cancel, get, and getQueryResults in the US multi-region were also affected for 2 hours along with query jobs. Google BigQuery users observed elevated latencies for job completion from 07:30 to 12:10 US/Pacific. BigQuery jobs in regions outside of the US remained unaffected. \r\n\r\nROOT CAUSE\r\n\r\nThe incident was triggered by a sudden increase in queries in US multi-region leading to quota exhaustion in the storage system serving incoming requests. Detecting the sudden increase, BigQuery initiated its auto-defense mechanism and redirected user requests to a different location. The high load of requests triggered an issue in the scheduling system, causing delays in scheduling incoming queries. These delays resulted in errors for query, insert, list, cancel, get and getQueryResults BigQuery jobs and overall latency experienced by users. As a result of these high number of requests at 08:30 US/Pacific, the scheduling system\u2019s overload protection mechanism began rejecting further incoming requests, causing insert job failures for 27 minutes. \r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nBigQuery\u2019s defense mechanism began redirection at 07:50 US/Pacific. Google Engineers were automatically alerted at 07:54 US/Pacific and began investigation. The issue with the scheduler system began at 08:00 and our engineers were alerted again at 08:10. At 08:43, they restarted the scheduling system which mitigated the insert job failures by 08:57. Errors seen for insert, query, cancel, list, get and getQueryResults jobs  were mitigated by 09:30 when queries were redirected to different locations. Google engineers then successfully blocked the source of sudden incoming queries that helped reduce overall latency. The issue was fully resolved at 12:10 US/Pacific when all active and pending queries completed running.\r\n\r\nWe will resolve the issue that caused the scheduling system to delay scheduling of incoming queries. Although the overload protection mechanism prevented the incident from spreading globally, it did cause the failures for insert jobs. We will be improving this mechanism by lowering deadline for synchronous queries which will help prevent queries from piling up and overloading the scheduling system.  To prevent future recurrence of the issue we will also implement changes to improve BigQuery\u2019s quota exhaustion behaviour that would prevent the storage system to take on more load than it can handle. To reduce the duration of similar incidents, we will implement tools to quickly remediate backlogged queries.", "when": "2019-05-24T04:00:12Z"}, {"created": "2019-05-17T20:38:27Z", "modified": "2019-05-17T20:38:27Z", "text": "The issue with Google BigQuery users experiencing latency and high error rates in US multi region has been resolved for all affected users as of Friday, 2019-05-17 13:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-05-17T20:38:27Z"}, {"created": "2019-05-17T19:14:45Z", "modified": "2019-05-17T19:14:45Z", "text": "Mitigation is underway and the rate of errors is decreasing. We will provide another status update by Friday, 2019-05-17 13:30 US/Pacific with current details.", "when": "2019-05-17T19:14:45Z"}, {"created": "2019-05-17T18:20:34Z", "modified": "2019-05-17T18:20:34Z", "text": "We believe the issue with Google BigQuery users experiencing latency and high error rates in US multi region is recurring. Our Engineering team is actively working towards mitigating the source of these errors. We sincerely apologize for the disruption caused. We will provide another status update by Friday, 2019-05-17 12:15 US/Pacific with current details", "when": "2019-05-17T18:20:34Z"}, {"created": "2019-05-17T17:35:36Z", "modified": "2019-05-17T17:35:36Z", "text": "The issue with Google BigQuery should be mitigated for the majority of users and we expect a full resolution in the near future. We will provide another status update by Friday, 2019-05-17 11:30 US/Pacific with current details.", "when": "2019-05-17T17:35:36Z"}, {"created": "2019-05-17T16:36:25Z", "modified": "2019-05-17T16:36:25Z", "text": "The rate of errors is decreasing. We will provide another status update by Friday, 2019-05-17 10:35 US/Pacific with current details.", "when": "2019-05-17T16:36:25Z"}, {"created": "2019-05-17T16:02:17Z", "modified": "2019-05-17T16:02:17Z", "text": "We've received a report of increased latency and errors for Google BigQuery users in the US. Mitigation work is currently underway by our Engineering Team. We will provide another status update by Friday, 2019-05-17 10:00 US/Pacific with current details.", "when": "2019-05-17T16:02:17Z"}, {"created": "2019-05-17T16:02:15Z", "modified": "2019-05-17T16:02:15Z", "text": "We've received a report of an issue with Google BigQuery.", "when": "2019-05-17T16:02:15Z"}], "uri": "/incident/bigquery/19003"}, {"begin": "2019-05-16T05:27:25Z", "created": "2019-05-16T05:27:30Z", "end": "2019-05-16T10:29:00Z", "external_desc": "The issue with Persistent Disk in us-central1-a zone has been resolved for all affected users.", "modified": "2019-05-16T10:56:21Z", "most-recent-update": {"created": "2019-05-16T10:56:21Z", "modified": "2019-05-16T10:56:21Z", "text": "The issue with Persistent Disk in us-central1-a zone has been resolved for all affected users", "when": "2019-05-16T10:29:00Z"}, "number": 19002, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-05-16T10:56:21Z", "modified": "2019-05-16T10:56:21Z", "text": "The issue with Persistent Disk in us-central1-a zone has been resolved for all affected users", "when": "2019-05-16T10:29:00Z"}, {"created": "2019-05-16T07:53:17Z", "modified": "2019-05-16T09:27:34Z", "text": "We are experiencing an issue with Persistent Disk in us-central1-a zone. All existing SSD devices operate normally. The HDD devices are not affected. Creation of a new SSD devices is temporarily disabled. The product team is actively investigating the root cause of the issue and we will provide an update by Thursday, 2019-05-16 03:00 US/Pacific with current details.", "when": "2019-05-16T07:53:17Z"}, {"created": "2019-05-16T06:25:55Z", "modified": "2019-05-16T06:25:55Z", "text": "We are experiencing an issue with Persistent Disk in us-central1-a zone. All existing SSD devices operate normally. The HDD devices are not affected. Creation of a new SSD devices is temporarily disabled. The product team is actively investigating the root cause of the issue and we will provide an update by Thursday, 2019-05-16 01:00 US/Pacific with current details.", "when": "2019-05-16T06:25:55Z"}, {"created": "2019-05-16T05:27:47Z", "modified": "2019-05-16T05:27:47Z", "text": "We are experiencing an issue with Persistent Disk in us-central-a zone. At the moment, all existing SSD devices expected to operate normally. Creation of a new SSD devices is temporarily disabled in this zone. HDD devices are not affected. We will provide an update by Wednesday, 2019-05-15 23:30 US/Pacific with current details.", "when": "2019-05-16T05:27:47Z"}, {"created": "2019-05-16T05:27:35Z", "modified": "2019-05-16T05:27:35Z", "text": "Persistent Disk SSD devices in us-central1-a experienced difficulties with write operations.", "when": "2019-05-16T05:27:35Z"}], "uri": "/incident/compute/19002"}, {"begin": "2019-05-11T15:52:33Z", "created": "2019-05-11T15:52:34Z", "end": "2019-05-11T18:04:17Z", "external_desc": "The Google Compute Engine is experiencing ZONE_RESOURCE_POOL_EXHAUSTED errors when creating new Compute instances in us-east4 region.", "modified": "2019-05-11T18:04:19Z", "most-recent-update": {"created": "2019-05-11T18:04:19Z", "modified": "2019-05-11T18:04:19Z", "text": "The Google Compute Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-05-11T18:04:19Z"}, "number": 19001, "public": true, "service_key": "compute", "service_name": "Google Compute Engine", "severity": "medium", "updates": [{"created": "2019-05-11T18:04:19Z", "modified": "2019-05-11T18:04:19Z", "text": "The Google Compute Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-05-11T18:04:19Z"}, {"created": "2019-05-11T16:12:06Z", "modified": "2019-05-11T16:12:06Z", "text": "The Google Compute Engine is experiencing ZONE_RESOURCE_POOL_EXHAUSTED errors when creating new Compute instances in us-east4 region. We will provide another status update by Saturday, 2019-05-11 11:00 US/Pacific with current details.", "when": "2019-05-11T16:12:06Z"}, {"created": "2019-05-11T15:52:35Z", "modified": "2019-05-11T15:52:35Z", "text": "We are investigating an issue with Google Compute Engine. We will provide more information by Saturday, 2019-05-11 11:00 US/Pacific.", "when": "2019-05-11T15:52:35Z"}], "uri": "/incident/compute/19001"}, {"begin": "2019-05-10T13:28:22Z", "created": "2019-05-10T13:28:28Z", "end": "2019-05-10T13:38:23Z", "external_desc": "Increased latency and occasional errors while accessing Google Cloud Console.", "modified": "2019-05-10T13:38:29Z", "most-recent-update": {"created": "2019-05-10T13:38:29Z", "modified": "2019-05-10T13:38:29Z", "text": "The issue with Google Cloud Console has been resolved for all affected users as of Friday, 2019-05-10 06:20 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-10T13:38:29Z"}, "number": 19004, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-05-10T13:38:29Z", "modified": "2019-05-10T13:38:29Z", "text": "The issue with Google Cloud Console has been resolved for all affected users as of Friday, 2019-05-10 06:20 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-10T13:38:29Z"}, {"created": "2019-05-10T13:28:36Z", "modified": "2019-05-10T13:28:36Z", "text": "The rate of errors is decreasing. We will provide another status update by Friday, 2019-05-10 07:15 US/Pacific with current details.", "when": "2019-05-10T13:28:36Z"}, {"created": "2019-05-10T13:28:28Z", "modified": "2019-05-10T13:28:28Z", "text": "Increased latency and occasional errors while accessing Google Cloud Console.", "when": "2019-05-10T13:28:28Z"}], "uri": "/incident/developers-console/19004"}, {"begin": "2019-05-08T00:53:26Z", "created": "2019-05-08T00:53:26Z", "end": "2019-05-08T02:28:28Z", "external_desc": "We are experiencing an issue with Google Cloud SQL data import and export operations in us-east1.", "modified": "2019-05-10T21:11:08Z", "most-recent-update": {"created": "2019-05-08T01:48:39Z", "modified": "2019-05-10T21:11:07Z", "text": "The issue with Google Cloud SQL data import and export operation failures in us-east1 has been resolved for all affected projects as of Tuesday, 2019-05-07 19:28 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-08T02:28:00Z"}, "number": 19001, "public": true, "service_key": "cloud-sql", "service_name": "Google Cloud SQL", "severity": "medium", "updates": [{"created": "2019-05-08T01:48:39Z", "modified": "2019-05-10T21:11:07Z", "text": "The issue with Google Cloud SQL data import and export operation failures in us-east1 has been resolved for all affected projects as of Tuesday, 2019-05-07 19:28 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-05-08T02:28:00Z"}, {"created": "2019-05-08T00:53:28Z", "modified": "2019-05-08T00:53:28Z", "text": "We are rolling back a configuration change to mitigate this issue. We will provide another status update by 2019-05-07 19:00 US/Pacific with current details.", "when": "2019-05-08T00:53:28Z"}, {"created": "2019-05-08T00:53:27Z", "modified": "2019-05-08T00:53:27Z", "text": "We are experiencing an issue with Google Cloud SQL data import and export operations in us-east1.", "when": "2019-05-08T00:53:27Z"}], "uri": "/incident/cloud-sql/19001"}, {"begin": "2019-05-02T15:33:00Z", "created": "2019-05-02T15:33:01Z", "end": "2019-05-02T15:33:43Z", "external_desc": "Unable to browse GCS storage buckets in Google Console", "modified": "2019-05-02T15:33:44Z", "most-recent-update": {"created": "2019-05-02T15:33:44Z", "modified": "2019-05-02T15:33:44Z", "text": "We are investigating errors with billing enabled projects causing console issues. Our Engineering Team is engaged and attempting mitigation against possible causes. For regular status updates, please follow: https://status.cloud.google.com/incident/developers-console/19003 where we will provide next update by Thursday, 2019-05-02 9:00 US/Pacific.", "when": "2019-05-02T15:33:44Z"}, "number": 19003, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-05-02T15:33:44Z", "modified": "2019-05-02T15:33:44Z", "text": "We are investigating errors with billing enabled projects causing console issues. Our Engineering Team is engaged and attempting mitigation against possible causes. For regular status updates, please follow: https://status.cloud.google.com/incident/developers-console/19003 where we will provide next update by Thursday, 2019-05-02 9:00 US/Pacific.", "when": "2019-05-02T15:33:44Z"}, {"created": "2019-05-02T15:33:02Z", "modified": "2019-05-02T15:33:02Z", "text": "We are investigating an issue with Google Cloud Storage. We will provide more information by Thursday, 2019-05-02 08:45 US/Pacific.", "when": "2019-05-02T15:33:02Z"}], "uri": "/incident/storage/19003"}, {"begin": "2019-05-02T15:32:58Z", "created": "2019-05-02T15:33:04Z", "end": "2019-05-02T15:33:00Z", "external_desc": "Unable to browse GCS storage buckets in Google Console", "modified": "2019-05-02T16:23:02Z", "most-recent-update": {"created": "2019-05-02T15:33:05Z", "modified": "2019-05-02T15:33:05Z", "text": "We are investigating an issue with Google Cloud Storage. We will provide more information by Thursday, 2019-05-02 08:45 US/Pacific.", "when": "2019-05-02T15:33:05Z"}, "number": 19004, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2019-05-02T15:33:05Z", "modified": "2019-05-02T15:33:05Z", "text": "We are investigating an issue with Google Cloud Storage. We will provide more information by Thursday, 2019-05-02 08:45 US/Pacific.", "when": "2019-05-02T15:33:05Z"}, {"created": "2019-05-02T16:23:01Z", "modified": "2019-05-02T16:23:01Z", "text": "We are investigating errors with billing enabled projects causing console issues. Our Engineering Team is engaged and attempting mitigation against possible causes. For regular status updates, please follow: https://status.cloud.google.com/incident/developers-console/19003 where we will provide next update by Thursday, 2019-05-02 9:00 US/Pacific.", "when": "2019-05-02T15:33:00Z"}], "uri": "/incident/storage/19004"}, {"begin": "2019-05-02T14:10:41Z", "created": "2019-05-02T15:17:42Z", "end": "2019-05-02T16:03:52Z", "external_desc": "Errors showing up in various areas in the Cloud Console", "modified": "2019-05-10T16:51:44Z", "most-recent-update": {"created": "2019-05-10T16:51:44Z", "modified": "2019-05-10T16:51:44Z", "text": "# ISSUE SUMMARY\r\nOn Thursday 2 May 2019, Google Cloud Console experienced a 40% error rate for all pageviews over a duration of 1 hour and 53 minutes. To all customers affected by this Cloud Console service degradation, we apologize. We are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Thursday 2 May 2019 from 07:10 to 09:03 US/Pacific the Google Cloud Console served 40% of all pageviews with a timeout error. Affected console sections include Compute Engine, Stackdriver, Kubernetes Engine, Cloud Storage, Firebase, App Engine, APIs, IAM, Cloud SQL, Dataflow, BigQuery and Billing.\r\n\r\n# ROOT CAUSE\r\nThe Google Cloud Console relies on many internal services to properly render individual user interface pages. The internal billing service is one of them, and is required to retrieve accurate state data for projects and accounts.\r\n\r\nAt 07:09 US/Pacific, a service unrelated to the Cloud Console began to send a large amount of traffic to the internal billing service. The additional load caused time-out and failure of individual requests including those from Google Cloud Console. This led to the Cloud Console serving timeout errors to customers when the underlying requests to the billing service failed.\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\nCloud Billing engineers were automatically alerted to the issue at 07:15 US/Pacific and Cloud Console engineers were alerted at 07:21. Both teams worked together to investigate the issue and once the root cause was identified, pursued two mitigation strategies. First, we increased the resources for the internal billing service in an attempt to handle the additional load. In parallel, we worked to identify the source of the extraneous traffic and then stop it from reaching the service. Once the traffic source was identified, mitigation was put in place and traffic to the internal billing service began to decrease at 08:40. The service fully recovered at 09:03.\r\n\r\nIn order to reduce the chance of recurrence we are taking the following actions. We will implement improved caching strategies in the Cloud Console to reduce unnecessary load and reliance on the internal billing service. The load shedding response of the billing service will be improved to better handle sudden spikes in load and to allow for quicker recovery should it be needed. Additionally, we will improve monitoring for the internal billing service to more precisely identify which part of the system is running into limits. Finally, we are reviewing dependencies in the serving path for all pages in the Cloud Console to ensure that necessary internal requests are handled gracefully in the event of failure.", "when": "2019-05-10T16:51:44Z"}, "number": 19003, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-05-10T16:51:44Z", "modified": "2019-05-10T16:51:44Z", "text": "# ISSUE SUMMARY\r\nOn Thursday 2 May 2019, Google Cloud Console experienced a 40% error rate for all pageviews over a duration of 1 hour and 53 minutes. To all customers affected by this Cloud Console service degradation, we apologize. We are taking immediate steps to improve the platform\u2019s performance and availability.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\nOn Thursday 2 May 2019 from 07:10 to 09:03 US/Pacific the Google Cloud Console served 40% of all pageviews with a timeout error. Affected console sections include Compute Engine, Stackdriver, Kubernetes Engine, Cloud Storage, Firebase, App Engine, APIs, IAM, Cloud SQL, Dataflow, BigQuery and Billing.\r\n\r\n# ROOT CAUSE\r\nThe Google Cloud Console relies on many internal services to properly render individual user interface pages. The internal billing service is one of them, and is required to retrieve accurate state data for projects and accounts.\r\n\r\nAt 07:09 US/Pacific, a service unrelated to the Cloud Console began to send a large amount of traffic to the internal billing service. The additional load caused time-out and failure of individual requests including those from Google Cloud Console. This led to the Cloud Console serving timeout errors to customers when the underlying requests to the billing service failed.\r\n\r\n\r\n# REMEDIATION AND PREVENTION\r\nCloud Billing engineers were automatically alerted to the issue at 07:15 US/Pacific and Cloud Console engineers were alerted at 07:21. Both teams worked together to investigate the issue and once the root cause was identified, pursued two mitigation strategies. First, we increased the resources for the internal billing service in an attempt to handle the additional load. In parallel, we worked to identify the source of the extraneous traffic and then stop it from reaching the service. Once the traffic source was identified, mitigation was put in place and traffic to the internal billing service began to decrease at 08:40. The service fully recovered at 09:03.\r\n\r\nIn order to reduce the chance of recurrence we are taking the following actions. We will implement improved caching strategies in the Cloud Console to reduce unnecessary load and reliance on the internal billing service. The load shedding response of the billing service will be improved to better handle sudden spikes in load and to allow for quicker recovery should it be needed. Additionally, we will improve monitoring for the internal billing service to more precisely identify which part of the system is running into limits. Finally, we are reviewing dependencies in the serving path for all pages in the Cloud Console to ensure that necessary internal requests are handled gracefully in the event of failure.", "when": "2019-05-10T16:51:44Z"}, {"created": "2019-05-02T16:41:53Z", "modified": "2019-05-02T16:41:53Z", "text": "The issue with Google Cloud Console has been resolved for all affected projects as of Thursday, 2019-05-02 8:58 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-05-02T16:41:53Z"}, {"created": "2019-05-02T15:57:44Z", "modified": "2019-05-02T15:57:44Z", "text": "We are experiencing an issue with Google Cloud Console where users are experiencing billing errors when trying to access products' dashboards beginning at 07:12 US/Pacific. An updated list of product dashboards that are affected is as follows; Google Compute Engine, Stackdriver, Google Kubernetes Engine, Google Cloud Storage, Firebase, Billing, App Engine, APIs, IAM, Cloud SQL, Dataflow, and Big Query. For everyone who is affected, we apologize for the disruption. We will provide an update by Thursday, 2019-05-02 10:30 US/Pacific with current details.", "when": "2019-05-02T15:57:44Z"}, {"created": "2019-05-02T15:17:44Z", "modified": "2019-05-02T15:17:44Z", "text": "The Google Cloud Console is experiencing errors when trying to access some dashboards within. Known dashboards include; Google Compute Engine, Stackdriver, Google Kubernetes Engine, Google Cloud Storage, and Firebase. Users will be experiencing billing errors when trying to access these pages. Gcloud can be used as a work around to interact with product APIs.  We will provide another status update by Thursday, 2019-05-02 09:00 US/Pacific", "when": "2019-05-02T15:17:44Z"}, {"created": "2019-05-02T15:17:42Z", "modified": "2019-05-02T15:17:42Z", "text": "Errors showing up in various areas in the Cloud Console", "when": "2019-05-02T15:17:42Z"}], "uri": "/incident/developers-console/19003"}, {"begin": "2019-04-26T21:57:55Z", "created": "2019-04-26T21:58:01Z", "end": "2019-04-27T00:26:58Z", "external_desc": "The Google Cloud Console service is experiencing an issue with elevated latency when starting Cloud Shell consoles.", "modified": "2019-04-27T00:26:59Z", "most-recent-update": {"created": "2019-04-27T00:26:59Z", "modified": "2019-04-27T00:26:59Z", "text": "The Google Cloud Console issue is believed to be affecting less than 1% of requests and our Engineering Team is working on it. If you have questions or feel you are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-04-27T00:26:59Z"}, "number": 19002, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "medium", "updates": [{"created": "2019-04-27T00:26:59Z", "modified": "2019-04-27T00:26:59Z", "text": "The Google Cloud Console issue is believed to be affecting less than 1% of requests and our Engineering Team is working on it. If you have questions or feel you are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-04-27T00:26:59Z"}, {"created": "2019-04-26T22:59:55Z", "modified": "2019-04-26T22:59:55Z", "text": "We are still seeing elevated latency with the Google Cloud Console service. Our Engineering Team is investigating possible causes. We will provide another status update by Friday, 2019-04-26 18:00 US/Pacific with current details.", "when": "2019-04-26T22:59:55Z"}, {"created": "2019-04-26T22:07:56Z", "modified": "2019-04-26T22:07:56Z", "text": "The Google Cloud Console service is experiencing an issue where ~30% of requests may experience elevated latency when starting Cloud Shell consoles. Most affected requests will succeed in under 30 seconds, however a small number of requests (approx. 5%) may fail. We will provide another status update by Friday, 2019-04-26 15:50 US/Pacific with current details.", "when": "2019-04-26T22:07:56Z"}, {"created": "2019-04-26T21:58:08Z", "modified": "2019-04-26T21:58:08Z", "text": "The Google Cloud Console service is experiencing an issue where ~30% of users may experience elevated latency when starting Cloud Shell consoles. A small number of users (approx. 5%) may experience instances where the Cloud Shell console fails to load at all.  We will provide another status update by Friday, 2019-04-26 15:30 US/Pacific with current details.", "when": "2019-04-26T21:58:08Z"}, {"created": "2019-04-26T21:58:02Z", "modified": "2019-04-26T21:58:02Z", "text": "The Google Cloud Console service is experiencing an issue with elevated latency when starting Cloud Shell consoles.", "when": "2019-04-26T21:58:02Z"}], "uri": "/incident/developers-console/19002"}, {"begin": "2019-04-26T13:00:00Z", "created": "2019-04-26T17:05:42Z", "end": "2019-04-26T16:00:00Z", "external_desc": "Google Stackdriver Alerts not firing reliably", "modified": "2019-04-26T17:13:38Z", "most-recent-update": {"created": "2019-04-26T17:05:42Z", "modified": "2019-04-26T17:05:42Z", "text": "We identified an issue with Google Stackdriver alerting starting at Friday, 2019-04-26 06:00 US/Pacific. The issue has been resolved for all affected projects as of 09:00 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-26T17:05:42Z"}, "number": 19005, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-04-26T17:05:42Z", "modified": "2019-04-26T17:05:42Z", "text": "We identified an issue with Google Stackdriver alerting starting at Friday, 2019-04-26 06:00 US/Pacific. The issue has been resolved for all affected projects as of 09:00 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-26T17:05:42Z"}], "uri": "/incident/google-stackdriver/19005"}, {"begin": "2019-04-19T16:27:30Z", "created": "2019-04-19T17:23:30Z", "end": "2019-04-19T20:11:10Z", "external_desc": "We are investigating an issue with a delay in Google Stackdriver Logging ingestion.", "modified": "2019-04-19T20:11:16Z", "most-recent-update": {"created": "2019-04-19T20:11:16Z", "modified": "2019-04-19T20:11:16Z", "text": "The issue with Google Stackdriver Logging ingestion has been resolved for all affected projects as of Friday, 2019-04-19 12:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-19T20:11:16Z"}, "number": 19004, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-04-19T20:11:16Z", "modified": "2019-04-19T20:11:16Z", "text": "The issue with Google Stackdriver Logging ingestion has been resolved for all affected projects as of Friday, 2019-04-19 12:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-19T20:11:16Z"}, {"created": "2019-04-19T18:55:41Z", "modified": "2019-04-19T18:55:41Z", "text": "The issue with delays in Google Stackdriver Logging ingestion should be resolved for some projects and we expect a full resolution in the near future. We will provide another status update by Friday, 2019-04-19 14:00 US/Pacific with current details.", "when": "2019-04-19T18:55:41Z"}, {"created": "2019-04-19T18:17:38Z", "modified": "2019-04-19T18:17:38Z", "text": "We are investigating an issue with a delay in Google Stackdriver Logging ingestion. Logs ingested after Friday, 2019-04-19 09:27 US/Pacific may be not show up in queries to the Stackdriver logging service. The service is currently processing the backlog of messages, logs ingested since 09:27 US/Pacific will begin to appear in results. We will provide more information by Friday, 2019-04-19 12:00 US/Pacific.", "when": "2019-04-19T18:17:38Z"}, {"created": "2019-04-19T17:23:32Z", "modified": "2019-04-19T17:31:50Z", "text": "We are investigating an issue with a delay in Google Stackdriver Logging ingestion. Logs ingested after Friday, 2019-04-19 09:27 US/Pacific are unable to be queried from the Stackdriver logging service. We currently working on a mitigation.  We will provide more information by Friday, 2019-04-19 11:15 US/Pacific.\r\n\r\nA workaround to access more recent logs would be to leverage log exports. Log exports are not currently experiencing the same delay as the query service.", "when": "2019-04-19T17:23:32Z"}, {"created": "2019-04-19T17:23:31Z", "modified": "2019-04-19T17:23:31Z", "text": "We are investigating an issue with a delay in Google Stackdriver Logging ingestion.", "when": "2019-04-19T17:23:31Z"}], "uri": "/incident/google-stackdriver/19004"}, {"begin": "2019-04-17T22:33:21Z", "created": "2019-04-17T22:33:21Z", "end": "2019-04-17T23:43:22Z", "external_desc": "OAuth login issues", "modified": "2019-04-18T00:07:05Z", "most-recent-update": {"created": "2019-04-17T23:43:26Z", "modified": "2019-04-17T23:43:26Z", "text": "The issue with OAuth login has been resolved for all affected projects as of Wednesday, 2019-04-17 16:37 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-17T23:43:26Z"}, "number": 19001, "public": true, "service_key": "cloud-iam", "service_name": "Identity and Access Management", "severity": "medium", "updates": [{"created": "2019-04-17T23:43:26Z", "modified": "2019-04-17T23:43:26Z", "text": "The issue with OAuth login has been resolved for all affected projects as of Wednesday, 2019-04-17 16:37 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-17T23:43:26Z"}, {"created": "2019-04-17T23:33:16Z", "modified": "2019-04-17T23:33:16Z", "text": "The issue with OAuth login should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Wednesday, 2019-04-17 17:00 US/Pacific with current details.", "when": "2019-04-17T23:33:16Z"}, {"created": "2019-04-17T22:55:12Z", "modified": "2019-04-17T22:55:12Z", "text": "The rate of OAuth login errors is decreasing. We will provide another status update by Wednesday, 2019-04-17 16:30 US/Pacific with current details.", "when": "2019-04-17T22:55:12Z"}, {"created": "2019-04-17T22:33:23Z", "modified": "2019-04-17T22:33:23Z", "text": "We are continuing to investigate an issue with OAuth login. We will provide an update by Wednesday, 2019-04-17 16:00 US/Pacific.", "when": "2019-04-17T22:33:23Z"}, {"created": "2019-04-17T22:33:22Z", "modified": "2019-04-17T22:33:22Z", "text": "OAuth login issues", "when": "2019-04-17T22:33:22Z"}], "uri": "/incident/cloud-iam/19001"}, {"begin": "2019-04-08T23:45:48Z", "created": "2019-04-19T07:11:48Z", "end": "2019-04-19T07:15:29Z", "external_desc": "We experiencing delays in showing logging.", "modified": "2019-04-25T16:55:16Z", "most-recent-update": {"created": "2019-04-25T16:55:16Z", "modified": "2019-04-25T16:55:16Z", "text": "Previously, this incident was reported as a service outage of Stackdriver Logging which was inaccurate.  Only logs generated between 2019-04-18 16:45 - 2019-04-19 00:15 US/Pacific were delayed by the incident, and became available once the incident was resolved.\r\n\r\nUpon evaluation of Google's incident scope and severity, this incident's severity has been adjusted from Outage to Disruption.", "when": "2019-04-25T16:54:00Z"}, "number": 19003, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-04-25T16:55:16Z", "modified": "2019-04-25T16:55:16Z", "text": "Previously, this incident was reported as a service outage of Stackdriver Logging which was inaccurate.  Only logs generated between 2019-04-18 16:45 - 2019-04-19 00:15 US/Pacific were delayed by the incident, and became available once the incident was resolved.\r\n\r\nUpon evaluation of Google's incident scope and severity, this incident's severity has been adjusted from Outage to Disruption.", "when": "2019-04-25T16:54:00Z"}, {"created": "2019-04-19T08:38:31Z", "modified": "2019-04-19T08:38:31Z", "text": "The issue with Google Stackdriver has been resolved for all affected users as of Friday, 2019-04-19 1:16 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-19T08:38:31Z"}, {"created": "2019-04-19T07:11:50Z", "modified": "2019-04-19T07:24:03Z", "text": "The mitigation is in place and service start to recover. We increased the speed of the backlog ingestion but the process is not completed yet. We will provide another status update by Friday, 2019-04-19 04:15 US/Pacific with current details.", "when": "2019-04-19T07:11:50Z"}, {"created": "2019-04-19T07:11:49Z", "modified": "2019-04-19T07:11:49Z", "text": "We experiencing delays in showing logging.", "when": "2019-04-19T07:11:49Z"}], "uri": "/incident/google-stackdriver/19003"}, {"begin": "2019-04-04T22:40:03Z", "created": "2019-04-04T23:07:04Z", "end": "2019-04-04T23:50:06Z", "external_desc": "Cloud Router issue in us-central1", "modified": "2019-04-10T22:12:30Z", "most-recent-update": {"created": "2019-04-10T22:11:48Z", "modified": "2019-04-10T22:11:48Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Thursday 4 April 2019, Cloud VPN  configurations with dynamic routes via Cloud Router, Cloud Dedicated Interconnect attachments, and Cloud Partner Interconnect attachments in us-central1 experienced a service disruption for a duration of 70 minutes.  We apologize to all our customers who were impacted by the incident. \r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Thursday 4 April 2019, from 15:40 to 16:50 US/Pacific, Google Cloud Routers and Cloud Interconnect experienced a service disruption in us-central1. Cloud Routers for Cloud Interconnect and Cloud VPN were unable to route traffic in us-central1 for the duration of the incident. This impacted Cloud Private Interconnect attachments and Cloud VPN tunnels using dynamic routing. Global routing and Cloud VPN tunnels utilizing static routes were not affected during the incident. \r\n\r\n# ROOT CAUSE\r\n\r\nThe Cloud Router control plane service assigns Cloud Router tasks to individual customers and creates routes between those tasks and customer VPCs. Individual Cloud Router tasks connected to the control plane service are responsible for establishing external BGP sessions and propagating routes to and from the service.\r\n \r\nThe disruption was caused by a rollout to the Cloud Router control plane service. One part of the control plane rollout process changed the version of the service which cloud router tasks connect to, performed through a leader election process. When the new version was elected leader, cloud router tasks encountered an issue while disassociating with the previous leader. This issue caused tasks to stay connected to the previous leader for an extended duration. The delay resulted in individual cloud router tasks losing state, requiring the system to be initialized from a \u201ccold\u201d state.\r\n\r\nChanges in the new version allowed the system to complete initialization without any intervention. During initialization, cloud router tasks were reassigned to customers and started to re-establish sessions. Until all customers\u2019 tasks were reassigned, routes learned from these Cloud Routers were not propagated and services dependent on Cloud Routers remained impacted in us-central1.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were alerted to the disruption at 15:41 US/Pacific on 4 April 2019 and began to investigate immediately. Once the root cause was determined, the rollout was paused and control plane tasks running the previous version were canceled to ensure that the previous version would not be elected leader. The leader task was then restarted to ensure that all cloud router tasks connected to the service running the new version. The service then recovered.\r\n\r\nThe actions we took, based on previous learnings,  greatly reduced the duration of this disruption; however, to further reduce and prevent recurrence, we are changing the logic in both the control plane service and cloud router tasks to ensure that when there is a leadership change, cloud router tasks connect to the new leader quickly and keep their state. \r\n\r\nShould a \u201ccold\u201d state initialization reoccur, we are optimizing the initialization logic to finish more quickly, reducing recovery time for this type of incident. Furthermore, we will review control planes across Google Cloud Platform and analyze how the systems perform under a \u201ccold\u201d start scenario to ensure they meet customer requirements.", "when": "2019-04-11T01:10:00Z"}, "number": 19007, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "high", "updates": [{"created": "2019-04-10T22:11:48Z", "modified": "2019-04-10T22:11:48Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Thursday 4 April 2019, Cloud VPN  configurations with dynamic routes via Cloud Router, Cloud Dedicated Interconnect attachments, and Cloud Partner Interconnect attachments in us-central1 experienced a service disruption for a duration of 70 minutes.  We apologize to all our customers who were impacted by the incident. \r\n\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Thursday 4 April 2019, from 15:40 to 16:50 US/Pacific, Google Cloud Routers and Cloud Interconnect experienced a service disruption in us-central1. Cloud Routers for Cloud Interconnect and Cloud VPN were unable to route traffic in us-central1 for the duration of the incident. This impacted Cloud Private Interconnect attachments and Cloud VPN tunnels using dynamic routing. Global routing and Cloud VPN tunnels utilizing static routes were not affected during the incident. \r\n\r\n# ROOT CAUSE\r\n\r\nThe Cloud Router control plane service assigns Cloud Router tasks to individual customers and creates routes between those tasks and customer VPCs. Individual Cloud Router tasks connected to the control plane service are responsible for establishing external BGP sessions and propagating routes to and from the service.\r\n \r\nThe disruption was caused by a rollout to the Cloud Router control plane service. One part of the control plane rollout process changed the version of the service which cloud router tasks connect to, performed through a leader election process. When the new version was elected leader, cloud router tasks encountered an issue while disassociating with the previous leader. This issue caused tasks to stay connected to the previous leader for an extended duration. The delay resulted in individual cloud router tasks losing state, requiring the system to be initialized from a \u201ccold\u201d state.\r\n\r\nChanges in the new version allowed the system to complete initialization without any intervention. During initialization, cloud router tasks were reassigned to customers and started to re-establish sessions. Until all customers\u2019 tasks were reassigned, routes learned from these Cloud Routers were not propagated and services dependent on Cloud Routers remained impacted in us-central1.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were alerted to the disruption at 15:41 US/Pacific on 4 April 2019 and began to investigate immediately. Once the root cause was determined, the rollout was paused and control plane tasks running the previous version were canceled to ensure that the previous version would not be elected leader. The leader task was then restarted to ensure that all cloud router tasks connected to the service running the new version. The service then recovered.\r\n\r\nThe actions we took, based on previous learnings,  greatly reduced the duration of this disruption; however, to further reduce and prevent recurrence, we are changing the logic in both the control plane service and cloud router tasks to ensure that when there is a leadership change, cloud router tasks connect to the new leader quickly and keep their state. \r\n\r\nShould a \u201ccold\u201d state initialization reoccur, we are optimizing the initialization logic to finish more quickly, reducing recovery time for this type of incident. Furthermore, we will review control planes across Google Cloud Platform and analyze how the systems perform under a \u201ccold\u201d start scenario to ensure they meet customer requirements.", "when": "2019-04-11T01:10:00Z"}, {"created": "2019-04-05T01:10:07Z", "modified": "2019-04-05T01:10:07Z", "text": "The issue with Cloud Router in us-central1 should be resolved for all affected projects. The engineering team is the process of making the appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-05T01:10:07Z"}, {"created": "2019-04-05T00:18:29Z", "modified": "2019-04-05T00:18:29Z", "text": "The issue with Cloud Router in us-central1 has been resolved for all affected projects as of Thursday, 2019-04-04 17:00 US/Pacific. Our engineering team is continuing to work on this issue to prevent the risk of a recurrence.\r\n\r\nWe will provide another update by Thursday, 2019-04-04 18:00 US/Pacific", "when": "2019-04-05T00:18:29Z"}, {"created": "2019-04-04T23:59:37Z", "modified": "2019-04-04T23:59:37Z", "text": "Cloud Router service is now working normally for most projects. Our engineering team continues work to mitigate for the remaining projects impacted. We will provide another status update by Thursday, 2019-04-04 17:30 US/Pacific with current details.", "when": "2019-04-04T23:59:37Z"}, {"created": "2019-04-04T23:30:52Z", "modified": "2019-04-04T23:30:52Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Thursday, 2019-04-04 17:00 US/Pacific with current details.", "when": "2019-04-04T23:30:52Z"}, {"created": "2019-04-04T23:07:06Z", "modified": "2019-04-04T23:07:06Z", "text": "We are investigating an issue with Cloud Router in us-central1. We will provide more information by Thursday, 2019-04-04 16:30 US/Pacific.", "when": "2019-04-04T23:07:06Z"}, {"created": "2019-04-04T23:07:05Z", "modified": "2019-04-04T23:07:05Z", "text": "Cloud Router issue in us-central1", "when": "2019-04-04T23:07:05Z"}], "uri": "/incident/cloud-networking/19007"}, {"begin": "2019-04-04T20:28:44Z", "created": "2019-04-04T21:38:45Z", "end": "2019-04-04T21:25:34Z", "external_desc": "Google Cloud Network unavailability in europe-west4-b", "modified": "2019-04-04T21:42:15Z", "most-recent-update": {"created": "2019-04-04T21:40:35Z", "modified": "2019-04-04T21:40:35Z", "text": "The issue with Google Cloud Network unavailability in europe-west4-b has been resolved for all affected project] as of Thursday, 2019-04-04 14:25 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-04T21:40:35Z"}, "number": 19006, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "high", "updates": [{"created": "2019-04-04T21:40:35Z", "modified": "2019-04-04T21:40:35Z", "text": "The issue with Google Cloud Network unavailability in europe-west4-b has been resolved for all affected project] as of Thursday, 2019-04-04 14:25 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-04-04T21:40:35Z"}, {"created": "2019-04-04T21:38:45Z", "modified": "2019-04-04T21:38:45Z", "text": "Google Cloud Networking experienced unavailability in europe-west4-b at 2019-04-04 13:28 - 14:25 US/Pacific.\r\n\r\nWe will provide the next update by Thursday, 2019-04-04 15:00 US/Pacific", "when": "2019-04-04T21:38:45Z"}], "uri": "/incident/cloud-networking/19006"}, {"begin": "2019-03-26T14:30:00Z", "created": "2019-03-26T16:04:09Z", "end": "2019-03-26T16:35:54Z", "external_desc": "Live Chat Unavailable and Case Responses Delayed for Some Users", "modified": "2019-03-26T16:56:38Z", "most-recent-update": {"created": "2019-03-26T16:35:54Z", "modified": "2019-03-26T16:56:37Z", "text": "The issue is believed not to have impacted users of Google Cloud Support. Our Engineering Team has resolved the issue as of Tuesday, March 26, 2019, 09:30 US/Pacific. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-26T16:35:54Z"}, "number": 19001, "public": true, "service_key": "support", "service_name": "Google Cloud Support", "severity": "medium", "updates": [{"created": "2019-03-26T16:35:54Z", "modified": "2019-03-26T16:56:37Z", "text": "The issue is believed not to have impacted users of Google Cloud Support. Our Engineering Team has resolved the issue as of Tuesday, March 26, 2019, 09:30 US/Pacific. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-26T16:35:54Z"}, {"created": "2019-03-26T16:26:32Z", "modified": "2019-03-26T16:26:32Z", "text": "Our Engineering Team believes they have identified the root cause of the latency and is working to mitigate. We will provide another status update by Tuesday, March 26, 2019, 10:30 US/Pacific with current details.", "when": "2019-03-26T16:26:32Z"}, {"created": "2019-03-26T16:04:10Z", "modified": "2019-03-26T16:04:10Z", "text": "We're currently investigating an issue with Google Cloud Support. Live chat may be unavailable for some users. Additionally, responses to support cases may be delayed. We will provide more information by Tuesday, March 26, 2019, 10:00 US/Pacific Time.", "when": "2019-03-26T16:04:10Z"}], "uri": "/incident/support/19001"}, {"begin": "2019-03-20T18:43:30Z", "created": "2019-03-20T18:43:31Z", "end": "2019-03-20T18:57:35Z", "external_desc": "Cloud Console Mobile App users seeing elevated error rates.", "modified": "2019-06-02T21:59:05Z", "most-recent-update": {"created": "2019-06-02T21:58:47Z", "modified": "2019-06-02T21:58:47Z", "text": "We continue to experience high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. Our engineering teams have completed the first phase of their mitigation work and are currently implementing the second phase, after which we expect to return to normal service.  We will provide an update at 16:00 US/Pacific.", "when": "2019-06-02T21:58:47Z"}, "number": 19008, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "high", "updates": [{"created": "2019-06-02T21:58:47Z", "modified": "2019-06-02T21:58:47Z", "text": "We continue to experience high levels of network congestion in the eastern USA, affecting multiple services in Google Cloud, G Suite and YouTube. Users may see slow performance or intermittent errors. Our engineering teams have completed the first phase of their mitigation work and are currently implementing the second phase, after which we expect to return to normal service.  We will provide an update at 16:00 US/Pacific.", "when": "2019-06-02T21:58:47Z"}, {"created": "2019-03-20T18:57:35Z", "modified": "2019-03-20T18:57:35Z", "text": "The Google App Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-20T18:57:35Z"}, {"created": "2019-03-20T18:43:34Z", "modified": "2019-03-20T19:12:44Z", "text": "We are experiencing an issue with Google App Engine and Cloud Console Mobile App users experiencing elevated error rates beginning Wednesday, 2019-03-20 10:45 US/Pacific. Early investigation indicate(s) that affected users are failing to load the home dashboard. Some portions of the app's navigation are also impacted if users decide to navigate away from the home dashboard.  Google App Engine applications may experience elevated 500/502 errors. For everyone who is affected, we apologize for the disruption. We will provide an update by Wednesday, 2019-03-20 12:40 US/Pacific with current details.", "when": "2019-03-20T18:43:34Z"}, {"created": "2019-03-20T18:43:32Z", "modified": "2019-03-20T18:43:32Z", "text": "Cloud Console Mobile App users seeing elevated error rates.", "when": "2019-03-20T18:43:32Z"}], "uri": "/incident/appengine/19008"}, {"begin": "2019-03-13T01:40:13Z", "created": "2019-03-13T03:31:13Z", "end": "2019-03-13T05:50:12Z", "external_desc": "Elevated error rate with Google Cloud Storage.", "modified": "2019-03-14T18:09:27Z", "most-recent-update": {"created": "2019-03-14T18:09:27Z", "modified": "2019-03-14T18:09:27Z", "text": "ISSUE SUMMARY\r\n\r\nOn Tuesday 12 March 2019, Google's internal blob storage service experienced a service disruption for a duration of 4 hours and 10 minutes. We apologize to customers whose service or application was impacted by this incident. We know that our customers depend on Google Cloud Platform services and we are taking immediate steps to improve our availability and prevent outages of this type from recurring.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 12 March 2019 from 18:40 to 22:50 PDT, Google's internal blob (large data object) storage service experienced elevated error rates, averaging 20% error rates with a short peak of 31% errors during the incident. User-visible Google services including Gmail, Photos, and Google Drive, which make use of the blob storage service also saw elevated error rates, although (as was the case with GCS) the user impact was greatly reduced by caching and redundancy built into those services. There will be a separate incident report for non-GCP services affected by this incident.\r\n\r\nThe Google Cloud Platform services that experienced the most significant customer impact were the following:\r\n\r\nGoogle Cloud Storage experienced elevated long tail latency and an average error rate of 4.8%. All bucket locations and storage classes were impacted. GCP services that depend on Cloud Storage were also impacted.\r\n\r\nStackdriver Monitoring experienced up to 5% errors retrieving historical time series data. Recent time series data was available. Alerting was not impacted.\r\n\r\nApp Engine's Blobstore API experienced elevated latency and an error rate that peaked at 21% for fetching blob data. App Engine deployments experienced elevated errors that peaked at 90%. Serving of static files from App Engine also experienced elevated errors.\r\n\r\nROOT CAUSE\r\n\r\nOn Monday 11 March 2019, Google SREs were alerted to a significant increase in storage resources for metadata used by the internal blob service. On Tuesday 12 March, to reduce resource usage, SREs made a configuration change which had a side effect of overloading a key part of the system for looking up the location of blob data. The increased load eventually lead to a cascading failure.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nSREs were alerted to the service disruption at 18:56 PDT and immediately stopped the job that was making configuration changes. In order to recover from the cascading failure, SREs manually reduced traffic levels to the blob service to allow tasks to start up without crashing due to high load. \r\n\r\nIn order to prevent service disruptions of this type, we will be improving the isolation between regions of the storage service so that failures are less likely to have global impact. We will be improving our ability to more quickly provision resources in order to recover from a cascading failure triggered by high load. We will make software measures to prevent any configuration changes that cause overloading of key parts of the system. We will improve load shedding behavior of the metadata storage system so that it degrades gracefully under overload.", "when": "2019-03-14T18:09:27Z"}, "number": 19002, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "high", "updates": [{"created": "2019-03-14T18:09:27Z", "modified": "2019-03-14T18:09:27Z", "text": "ISSUE SUMMARY\r\n\r\nOn Tuesday 12 March 2019, Google's internal blob storage service experienced a service disruption for a duration of 4 hours and 10 minutes. We apologize to customers whose service or application was impacted by this incident. We know that our customers depend on Google Cloud Platform services and we are taking immediate steps to improve our availability and prevent outages of this type from recurring.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 12 March 2019 from 18:40 to 22:50 PDT, Google's internal blob (large data object) storage service experienced elevated error rates, averaging 20% error rates with a short peak of 31% errors during the incident. User-visible Google services including Gmail, Photos, and Google Drive, which make use of the blob storage service also saw elevated error rates, although (as was the case with GCS) the user impact was greatly reduced by caching and redundancy built into those services. There will be a separate incident report for non-GCP services affected by this incident.\r\n\r\nThe Google Cloud Platform services that experienced the most significant customer impact were the following:\r\n\r\nGoogle Cloud Storage experienced elevated long tail latency and an average error rate of 4.8%. All bucket locations and storage classes were impacted. GCP services that depend on Cloud Storage were also impacted.\r\n\r\nStackdriver Monitoring experienced up to 5% errors retrieving historical time series data. Recent time series data was available. Alerting was not impacted.\r\n\r\nApp Engine's Blobstore API experienced elevated latency and an error rate that peaked at 21% for fetching blob data. App Engine deployments experienced elevated errors that peaked at 90%. Serving of static files from App Engine also experienced elevated errors.\r\n\r\nROOT CAUSE\r\n\r\nOn Monday 11 March 2019, Google SREs were alerted to a significant increase in storage resources for metadata used by the internal blob service. On Tuesday 12 March, to reduce resource usage, SREs made a configuration change which had a side effect of overloading a key part of the system for looking up the location of blob data. The increased load eventually lead to a cascading failure.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nSREs were alerted to the service disruption at 18:56 PDT and immediately stopped the job that was making configuration changes. In order to recover from the cascading failure, SREs manually reduced traffic levels to the blob service to allow tasks to start up without crashing due to high load. \r\n\r\nIn order to prevent service disruptions of this type, we will be improving the isolation between regions of the storage service so that failures are less likely to have global impact. We will be improving our ability to more quickly provision resources in order to recover from a cascading failure triggered by high load. We will make software measures to prevent any configuration changes that cause overloading of key parts of the system. We will improve load shedding behavior of the metadata storage system so that it degrades gracefully under overload.", "when": "2019-03-14T18:09:27Z"}, {"created": "2019-03-13T07:05:45Z", "modified": "2019-03-13T07:05:45Z", "text": "We did a preliminary analysis about the impact of this issue.  We confirmed that the error rate to Cloud Storage has been less than 6% during the incident.", "when": "2019-03-13T07:05:45Z"}, {"created": "2019-03-13T06:43:13Z", "modified": "2019-03-13T06:43:13Z", "text": "The issue with Google Cloud Storage has been resolved for all affected projects as of Tuesday, 2019-03-12 23:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.  We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-03-13T06:43:13Z"}, {"created": "2019-03-13T06:15:41Z", "modified": "2019-03-13T06:15:41Z", "text": "The issue with Cloud Storage should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Tuesday, 2019-03-12 23:45 US/Pacific with current details.", "when": "2019-03-13T06:15:41Z"}, {"created": "2019-03-13T05:51:46Z", "modified": "2019-03-13T05:51:46Z", "text": "The underlying storage infrastructure of Cloud Storage is gradually recovering . We will provide another status update by Tuesday, 2019-03-12 23:15 US/Pacific with current details.", "when": "2019-03-13T05:51:46Z"}, {"created": "2019-03-13T05:15:15Z", "modified": "2019-03-13T05:15:15Z", "text": "We still have an issue with Google Cloud Storage.  Our Engineering team understands the root cause and is working to implement the solution.  We will provide another status update by Tuesday, 2019-03-12 22:45 US/Pacific with current details.", "when": "2019-03-13T05:15:15Z"}, {"created": "2019-03-13T04:47:19Z", "modified": "2019-03-13T04:47:19Z", "text": "We are still working to address the root cause of the issue. We will provide another status update by Tuesday, 2019-03-12 22:15 US/Pacific with current details.", "when": "2019-03-13T04:47:19Z"}, {"created": "2019-03-13T04:15:52Z", "modified": "2019-03-13T04:15:52Z", "text": "Mitigation work is still underway by our Engineering Team. We will provide another status update by Tuesday, 2019-03-12 21:45 US/Pacific with current details.", "when": "2019-03-13T04:15:52Z"}, {"created": "2019-03-13T03:46:05Z", "modified": "2019-03-13T03:46:05Z", "text": "We confirmed that the issue affects customers in all regions.  Also our Engineering Team believes they have identified the potential root causes of the errors and is still working to mitigate. We will provide another status update by Tuesday, 2019-03-12 21:15 US/Pacific with current details.", "when": "2019-03-13T03:46:05Z"}, {"created": "2019-03-13T03:31:15Z", "modified": "2019-03-13T03:31:15Z", "text": "We are still seeing the increased error rate with Google Cloud Storage. Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-03-12 20:45 US/Pacific with current details.", "when": "2019-03-13T03:31:15Z"}, {"created": "2019-03-13T03:31:14Z", "modified": "2019-03-13T03:31:14Z", "text": "Elevated error rate with Google Cloud Storage", "when": "2019-03-13T03:31:14Z"}], "uri": "/incident/storage/19002"}, {"begin": "2019-03-13T01:30:02Z", "created": "2019-03-13T02:49:02Z", "end": "2019-03-13T05:50:19Z", "external_desc": "Elevated error rate with Google App Engine Blobstore API and App Engine Version Deployment", "modified": "2019-03-14T18:11:06Z", "most-recent-update": {"created": "2019-03-14T18:11:06Z", "modified": "2019-03-14T18:11:06Z", "text": "ISSUE SUMMARY\r\n\r\nOn Tuesday 12 March 2019, Google's internal blob storage service experienced a service disruption for a duration of 4 hours and 10 minutes. We apologize to customers whose service or application was impacted by this incident. We know that our customers depend on Google Cloud Platform services and we are taking immediate steps to improve our availability and prevent outages of this type from recurring.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 12 March 2019 from 18:40 to 22:50 PDT, Google's internal blob (large data object) storage service experienced elevated error rates, averaging 20% error rates with a short peak of 31% errors during the incident. User-visible Google services including Gmail, Photos, and Google Drive, which make use of the blob storage service also saw elevated error rates, although (as was the case with GCS) the user impact was greatly reduced by caching and redundancy built into those services. There will be a separate incident report for non-GCP services affected by this incident.\r\n\r\nThe Google Cloud Platform services that experienced the most significant customer impact were the following:\r\n\r\nGoogle Cloud Storage experienced elevated long tail latency and an average error rate of 4.8%. All bucket locations and storage classes were impacted. GCP services that depend on Cloud Storage were also impacted.\r\n\r\nStackdriver Monitoring experienced up to 5% errors retrieving historical time series data. Recent time series data was available. Alerting was not impacted.\r\n\r\nApp Engine's Blobstore API experienced elevated latency and an error rate that peaked at 21% for fetching blob data. App Engine deployments experienced elevated errors that peaked at 90%. Serving of static files from App Engine also experienced elevated errors.\r\n\r\nROOT CAUSE\r\n\r\nOn Monday 11 March 2019, Google SREs were alerted to a significant increase in storage resources for metadata used by the internal blob service. On Tuesday 12 March, to reduce resource usage, SREs made a configuration change which had a side effect of overloading a key part of the system for looking up the location of blob data. The increased load eventually lead to a cascading failure.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nSREs were alerted to the service disruption at 18:56 PDT and immediately stopped the job that was making configuration changes. In order to recover from the cascading failure, SREs manually reduced traffic levels to the blob service to allow tasks to start up without crashing due to high load. \r\n\r\nIn order to prevent service disruptions of this type, we will be improving the isolation between regions of the storage service so that failures are less likely to have global impact. We will be improving our ability to more quickly provision resources in order to recover from a cascading failure triggered by high load. We will make software measures to prevent any configuration changes that cause overloading of key parts of the system. We will improve load shedding behavior of the metadata storage system so that it degrades gracefully under overload.", "when": "2019-03-14T18:11:05Z"}, "number": 19007, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "high", "updates": [{"created": "2019-03-14T18:11:06Z", "modified": "2019-03-14T18:11:06Z", "text": "ISSUE SUMMARY\r\n\r\nOn Tuesday 12 March 2019, Google's internal blob storage service experienced a service disruption for a duration of 4 hours and 10 minutes. We apologize to customers whose service or application was impacted by this incident. We know that our customers depend on Google Cloud Platform services and we are taking immediate steps to improve our availability and prevent outages of this type from recurring.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Tuesday 12 March 2019 from 18:40 to 22:50 PDT, Google's internal blob (large data object) storage service experienced elevated error rates, averaging 20% error rates with a short peak of 31% errors during the incident. User-visible Google services including Gmail, Photos, and Google Drive, which make use of the blob storage service also saw elevated error rates, although (as was the case with GCS) the user impact was greatly reduced by caching and redundancy built into those services. There will be a separate incident report for non-GCP services affected by this incident.\r\n\r\nThe Google Cloud Platform services that experienced the most significant customer impact were the following:\r\n\r\nGoogle Cloud Storage experienced elevated long tail latency and an average error rate of 4.8%. All bucket locations and storage classes were impacted. GCP services that depend on Cloud Storage were also impacted.\r\n\r\nStackdriver Monitoring experienced up to 5% errors retrieving historical time series data. Recent time series data was available. Alerting was not impacted.\r\n\r\nApp Engine's Blobstore API experienced elevated latency and an error rate that peaked at 21% for fetching blob data. App Engine deployments experienced elevated errors that peaked at 90%. Serving of static files from App Engine also experienced elevated errors.\r\n\r\nROOT CAUSE\r\n\r\nOn Monday 11 March 2019, Google SREs were alerted to a significant increase in storage resources for metadata used by the internal blob service. On Tuesday 12 March, to reduce resource usage, SREs made a configuration change which had a side effect of overloading a key part of the system for looking up the location of blob data. The increased load eventually lead to a cascading failure.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nSREs were alerted to the service disruption at 18:56 PDT and immediately stopped the job that was making configuration changes. In order to recover from the cascading failure, SREs manually reduced traffic levels to the blob service to allow tasks to start up without crashing due to high load. \r\n\r\nIn order to prevent service disruptions of this type, we will be improving the isolation between regions of the storage service so that failures are less likely to have global impact. We will be improving our ability to more quickly provision resources in order to recover from a cascading failure triggered by high load. We will make software measures to prevent any configuration changes that cause overloading of key parts of the system. We will improve load shedding behavior of the metadata storage system so that it degrades gracefully under overload.", "when": "2019-03-14T18:11:05Z"}, {"created": "2019-03-13T06:31:21Z", "modified": "2019-03-13T06:31:21Z", "text": "The issue with App Engine Blobstore API has been resolved for all affected projects as of Tuesday, 2019-03-12 23:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.  We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-03-13T06:31:21Z"}, {"created": "2019-03-13T06:18:11Z", "modified": "2019-03-13T06:18:11Z", "text": "The issue with App Engine Deployment should be resolved as of Tuesday, 2019-03-12 23:11 US/Pacific, and the issue with underlying storage of Blobstore API should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Tuesday, 2019-03-12 23:45 US/Pacific with current details.", "when": "2019-03-13T06:18:11Z"}, {"created": "2019-03-13T05:52:58Z", "modified": "2019-03-13T05:52:58Z", "text": "The underlying storage infrastructure is gradually recovering. We will provide another status update by Tuesday, 2019-03-12 23:15 US/Pacific with current details.", "when": "2019-03-13T05:52:58Z"}, {"created": "2019-03-13T05:16:54Z", "modified": "2019-03-13T05:16:54Z", "text": "We still have an issue with App Engine Blobstore API and Version Deployment.  Our Engineering team understands the root cause and is working to implement the solution.  We will provide another status update by Tuesday, 2019-03-12 22:45 US/Pacific with current details.", "when": "2019-03-13T05:16:54Z"}, {"created": "2019-03-13T04:56:11Z", "modified": "2019-03-13T04:56:11Z", "text": "The issue with App Engine Version Deployment should be resolved for some runtimes, including nodejs, python37, php72, go111, but we are still seeing the issue with other runtimes. We will provide another status update by Tuesday, 2019-03-12 22:15 US/Pacific with current details.", "when": "2019-03-13T04:56:11Z"}, {"created": "2019-03-13T04:47:35Z", "modified": "2019-03-13T04:47:35Z", "text": "We are still working to address the root cause of the issue. We will provide another status update by Tuesday, 2019-03-12 22:15 US/Pacific with current details.", "when": "2019-03-13T04:47:35Z"}, {"created": "2019-03-13T04:17:50Z", "modified": "2019-03-13T04:17:50Z", "text": "Mitigation work with the underlying storage infrastructure is still underway by our Engineering Team. We will provide another status update by Tuesday, 2019-03-12 21:45 US/Pacific with current details.", "when": "2019-03-13T04:17:50Z"}, {"created": "2019-03-13T03:50:51Z", "modified": "2019-03-13T03:50:51Z", "text": "We are still working on the issue with Google App Engine Blobstore API and App Engine Version Deployment. Our Engineering Team believes they have identified the potential root causes of the errors and is working to mitigate. We will provide another status update by Tuesday, 2019-03-12 21:15 US/Pacific with current details.", "when": "2019-03-13T03:50:51Z"}, {"created": "2019-03-13T03:14:45Z", "modified": "2019-03-13T03:14:45Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-03-12 20:45 US/Pacific with current details.", "when": "2019-03-13T03:14:45Z"}, {"created": "2019-03-13T02:58:30Z", "modified": "2019-03-13T02:58:30Z", "text": "We are still seeing the increased error rate with Google App Engine Blobstore API.  Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-03-12 20:30 US/Pacific US/Pacific with current details.", "when": "2019-03-13T02:58:30Z"}, {"created": "2019-03-13T02:49:03Z", "modified": "2019-03-13T02:49:03Z", "text": "We are investigating an issue with Google App Engine. We will provide more information by Tuesday, 2019-03-12 20:00 US/Pacific.", "when": "2019-03-13T02:49:03Z"}], "uri": "/incident/appengine/19007"}, {"begin": "2019-03-11T22:02:37Z", "created": "2019-03-11T22:02:38Z", "end": "2019-03-11T23:19:37Z", "external_desc": "We've received a report of an issue with Google Cloud Functions deployments", "modified": "2019-03-11T23:19:38Z", "most-recent-update": {"created": "2019-03-11T23:19:38Z", "modified": "2019-03-11T23:19:38Z", "text": "The issue with  Google Cloud Functions deployments experiencing an elevated error has been resolved for all affected projects as of Monday, 2019-03-11 15:45 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-11T23:19:38Z"}, "number": 19002, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-03-11T23:19:38Z", "modified": "2019-03-11T23:19:38Z", "text": "The issue with  Google Cloud Functions deployments experiencing an elevated error has been resolved for all affected projects as of Monday, 2019-03-11 15:45 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-11T23:19:38Z"}, {"created": "2019-03-11T22:52:45Z", "modified": "2019-03-11T22:52:45Z", "text": "The issue with Google Cloud Functions deployments experiencing an elevated error rate should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 17:00 US/Pacific with current details.", "when": "2019-03-11T22:52:45Z"}, {"created": "2019-03-11T22:02:40Z", "modified": "2019-03-11T22:02:40Z", "text": "We've received a report of an issue with Google Cloud Functions deployments seeing increased errors as of Monday, 2019-03-11 14:34 US/Pacific. We are creating a rollback now to attempt to mitigate the issue. We will provide more information by Monday, 2019-03-11 16:30 US/Pacific.", "when": "2019-03-11T22:02:40Z"}, {"created": "2019-03-11T22:02:39Z", "modified": "2019-03-11T22:02:39Z", "text": "We've received a report of an issue with Google Cloud Functions deployments", "when": "2019-03-11T22:02:38Z"}], "uri": "/incident/cloud-functions/19002"}, {"begin": "2019-03-11T17:33:35Z", "created": "2019-03-11T17:33:37Z", "end": "2019-03-12T13:13:05Z", "external_desc": "We are experiencing an issue with increased system lag in some Google Cloud Dataflow jobs.", "modified": "2019-03-12T13:13:07Z", "most-recent-update": {"created": "2019-03-12T13:13:07Z", "modified": "2019-03-12T13:13:07Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag has been resolved for all affected projects as of Tuesday, 2019-03-12 04:49 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-12T13:13:07Z"}, "number": 19001, "public": true, "service_key": "cloud-dataflow", "service_name": "Google Cloud Dataflow", "severity": "medium", "updates": [{"created": "2019-03-12T13:13:07Z", "modified": "2019-03-12T13:13:07Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag has been resolved for all affected projects as of Tuesday, 2019-03-12 04:49 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-12T13:13:07Z"}, {"created": "2019-03-12T12:00:14Z", "modified": "2019-03-12T12:00:14Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects. We are still monitoring the system to confirm that the issue has been completely mitigated. We will provide another status update by Tuesday, 2019-03-12 06:30 US/Pacific with current details.", "when": "2019-03-12T12:00:14Z"}, {"created": "2019-03-12T09:57:45Z", "modified": "2019-03-12T09:57:45Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the vast majority of projects and we expect it to be resolved in near future. We will provide another status update by Tuesday, 2019-03-12 05:00 US/Pacific with current details.", "when": "2019-03-12T09:57:45Z"}, {"created": "2019-03-12T06:29:57Z", "modified": "2019-03-12T06:29:57Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects. Our mitigation efforts are effective and we expect a full resolution in the near future. We will provide another status update by Tuesday, 2019-03-12 03:00 US/Pacific with current details.", "when": "2019-03-12T06:29:57Z"}, {"created": "2019-03-12T04:56:03Z", "modified": "2019-03-12T04:56:03Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects. Our mitigation efforts are effective and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 23:00 US/Pacific with current details.", "when": "2019-03-12T04:56:03Z"}, {"created": "2019-03-12T02:59:12Z", "modified": "2019-03-12T02:59:12Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 22:00 US/Pacific with current details.", "when": "2019-03-12T02:59:12Z"}, {"created": "2019-03-12T01:00:38Z", "modified": "2019-03-12T01:00:38Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 20:00 US/Pacific with current details.", "when": "2019-03-12T01:00:38Z"}, {"created": "2019-03-11T23:44:19Z", "modified": "2019-03-11T23:44:19Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 18:00 US/Pacific with current details.", "when": "2019-03-11T23:44:19Z"}, {"created": "2019-03-11T22:59:32Z", "modified": "2019-03-11T22:59:32Z", "text": "The issue with Google Cloud Dataflow jobs experiencing system lag should be resolved for the majority of projects and we expect a full resolution in the near future. A very small number of jobs are still impacted and Google Engineers are very close to a full resolution. We will provide another status update by Monday, 2019-03-11 16:45 US/Pacific with current details.", "when": "2019-03-11T22:59:32Z"}, {"created": "2019-03-11T21:02:16Z", "modified": "2019-03-11T21:02:16Z", "text": "The issue with stuck Dataflow jobs has been resolved for some users and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 16:00 US/Pacific with current details.", "when": "2019-03-11T21:02:16Z"}, {"created": "2019-03-11T19:45:40Z", "modified": "2019-03-11T19:45:40Z", "text": "Our Engineering Team continues to mitigate the issue. We will provide another status update by Monday, 2019-03-11 14:00 US/Pacific with current details.", "when": "2019-03-11T19:45:40Z"}, {"created": "2019-03-11T18:43:09Z", "modified": "2019-03-11T18:43:09Z", "text": "Our Engineering Team believes they have identified the root cause of the stuck Dataflow jobs and is working to mitigate. We will provide another status update by Monday, 2019-03-11 12:45 US/Pacific with current details.\r\n\r\nAs a reminder, Dataflow jobs using newly-created Google Cloud Pub-Sub topics are unaffected.", "when": "2019-03-11T18:43:09Z"}, {"created": "2019-03-11T18:40:05Z", "modified": "2019-03-11T18:40:05Z", "text": "Our Engineering Team believes they have identified the root cause of the stuck Dataflow jobs and is working to mitigate. We will provide another status update by Monday, 2019-03-11 12:45 US/Pacific with current details.", "when": "2019-03-11T18:40:05Z"}, {"created": "2019-03-11T17:33:39Z", "modified": "2019-03-11T17:33:39Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Monday, 2019-03-11 11:45 US/Pacific with current details.", "when": "2019-03-11T17:33:39Z"}, {"created": "2019-03-11T17:33:37Z", "modified": "2019-03-11T17:33:37Z", "text": "We've received a report of an issue with increased system lag in some Google Cloud Dataflow jobs.", "when": "2019-03-11T17:33:37Z"}], "uri": "/incident/cloud-dataflow/19001"}, {"begin": "2019-03-11T16:58:00Z", "created": "2019-03-11T16:58:01Z", "end": "2019-03-11T23:31:13Z", "external_desc": "We've received a report of an issue with Google Cloud Console.", "modified": "2019-03-14T23:55:15Z", "most-recent-update": {"created": "2019-03-14T23:55:15Z", "modified": "2019-03-14T23:55:15Z", "text": "ISSUE SUMMARY\r\n\r\nOn Monday, 11 March 2019, Google Cloud Console was unavailable for a duration of 3 hours and 54 minutes. Although, Google Cloud Platform resources remained unaffected, we understand that a majority of our customers rely on Cloud Console to manage their cloud resources and we sincerely apologize to everyone who was affected by the incident. The issue also affected Firebase console and IAM service account activations. \r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Monday, 11 March 2019, from 09:26 to 13:20 US/Pacific, Cloud Console was unavailable. Users were unable to access and manage their GCP resources using Cloud Console. All Google Cloud Platform resources continued to function and were accessible using the gcloud CLI, and the Cloud Console iOS and Android apps. From 14:10 to 15:37 US/Pacific, for a duration of 1 hour 27 minutes, Firebase Console and IAM service account activation were also unavailable to users. \r\n\r\nROOT CAUSE\r\n\r\nMost Google services use a quota system for rate limiting user requests. The quota system implements a variant of the classic token bucket algorithm [1]. \r\n\r\nThe issue was triggered when a code change in the most recent release of the quota system introduced a bug, causing a fallback to significantly smaller, default quota limits,  resulting in user requests being denied. \r\n\r\nWhile the Cloud Console team mitigated the issue at 13:20 US/Pacifc, the underlying issue with the quota system started affecting Firebase Console and IAM service account activation beginning 14:10 US/Pacific until it was mitigated at 15:37 US/Pacific. \r\n \r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nCloud Console engineers were alerted at 09:31 US/Pacific and began investigation shortly after. The issue was mitigated at 13:20 US/Pacific when quota server engineers granted additional quota to Cloud Console while they continued to investigate the root cause. The issue was permanently mitigated when the offending change was rolled back. \r\n\r\nIn addition to fixing the underlying bug, we will be fixing the error in our default quota configuration. We will also be improving our automated alerts system to cover obviously erroneous quota denials.\r\n\r\nWe apologize again for the inconvenience caused by this issue to our customers. \r\n\r\n[1] https://en.wikipedia.org/wiki/Token_bucket", "when": "2019-03-14T23:55:15Z"}, "number": 19001, "public": true, "service_key": "developers-console", "service_name": "Google Cloud Console", "severity": "high", "updates": [{"created": "2019-03-14T23:55:15Z", "modified": "2019-03-14T23:55:15Z", "text": "ISSUE SUMMARY\r\n\r\nOn Monday, 11 March 2019, Google Cloud Console was unavailable for a duration of 3 hours and 54 minutes. Although, Google Cloud Platform resources remained unaffected, we understand that a majority of our customers rely on Cloud Console to manage their cloud resources and we sincerely apologize to everyone who was affected by the incident. The issue also affected Firebase console and IAM service account activations. \r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Monday, 11 March 2019, from 09:26 to 13:20 US/Pacific, Cloud Console was unavailable. Users were unable to access and manage their GCP resources using Cloud Console. All Google Cloud Platform resources continued to function and were accessible using the gcloud CLI, and the Cloud Console iOS and Android apps. From 14:10 to 15:37 US/Pacific, for a duration of 1 hour 27 minutes, Firebase Console and IAM service account activation were also unavailable to users. \r\n\r\nROOT CAUSE\r\n\r\nMost Google services use a quota system for rate limiting user requests. The quota system implements a variant of the classic token bucket algorithm [1]. \r\n\r\nThe issue was triggered when a code change in the most recent release of the quota system introduced a bug, causing a fallback to significantly smaller, default quota limits,  resulting in user requests being denied. \r\n\r\nWhile the Cloud Console team mitigated the issue at 13:20 US/Pacifc, the underlying issue with the quota system started affecting Firebase Console and IAM service account activation beginning 14:10 US/Pacific until it was mitigated at 15:37 US/Pacific. \r\n \r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nCloud Console engineers were alerted at 09:31 US/Pacific and began investigation shortly after. The issue was mitigated at 13:20 US/Pacific when quota server engineers granted additional quota to Cloud Console while they continued to investigate the root cause. The issue was permanently mitigated when the offending change was rolled back. \r\n\r\nIn addition to fixing the underlying bug, we will be fixing the error in our default quota configuration. We will also be improving our automated alerts system to cover obviously erroneous quota denials.\r\n\r\nWe apologize again for the inconvenience caused by this issue to our customers. \r\n\r\n[1] https://en.wikipedia.org/wiki/Token_bucket", "when": "2019-03-14T23:55:15Z"}, {"created": "2019-03-11T23:31:19Z", "modified": "2019-03-11T23:31:19Z", "text": "The issue with  Google Cloud Console has been resolved for all affected projects as of Monday, 2019-03-11 16:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-03-11T23:31:19Z"}, {"created": "2019-03-11T22:49:26Z", "modified": "2019-03-11T22:49:26Z", "text": "The issue with Google Cloud Console should be resolved for the majority of projects as of 15:41 US/Pacific and we expect a full resolution in the near future. We will provide another status update by Monday, 2019-03-11 17:00 US/Pacific with current details.", "when": "2019-03-11T22:49:26Z"}, {"created": "2019-03-11T22:37:32Z", "modified": "2019-03-11T22:37:32Z", "text": "The rate of errors is decreasing. We will provide another status update by Monday, 2019-03-11 16:45 US/Pacific with current details.", "when": "2019-03-11T22:37:32Z"}, {"created": "2019-03-11T21:51:33Z", "modified": "2019-03-11T21:51:33Z", "text": "Our Engineering Team continues to pursue a complete resolution. We will provide another status update by Monday, 2019-03-11 17:00 US/Pacific with current details.", "when": "2019-03-11T21:51:33Z"}, {"created": "2019-03-11T20:52:03Z", "modified": "2019-03-11T20:52:03Z", "text": "The issue with Google Cloud Console should be partially resolved for the majority of users and we expect a full resolution in the near future. However, users may still have trouble listing project permissions from the Google Cloud Console. We will provide another status update by Monday, 2019-03-11 14:50 US/Pacific with current details.", "when": "2019-03-11T20:52:03Z"}, {"created": "2019-03-11T20:16:05Z", "modified": "2019-03-11T20:16:05Z", "text": "Our Engineering Team continues to mitigate the issue. We will provide another status update by Monday, 2019-03-11 14:15 US/Pacific with current details.", "when": "2019-03-11T20:16:05Z"}, {"created": "2019-03-11T19:11:39Z", "modified": "2019-03-11T19:11:39Z", "text": "Our Engineering Team continues to mitigate the issue. We will provide another status update by Monday, 2019-03-11 13:15 US/Pacific with the current details.", "when": "2019-03-11T19:11:39Z"}, {"created": "2019-03-11T17:58:37Z", "modified": "2019-03-11T17:58:37Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Monday, 2019-03-11 12:15 US/Pacific with current details.\r\n\r\nAffected users may receive a \"failed to load\" error message when attempting to list resources like Compute Engine instances, billing accounts, GKE clusters, and Google Cloud Functions quotas.\r\n\r\nAs a workaround, the gcloud SDK can be used instead of the Cloud Console.", "when": "2019-03-11T17:58:37Z"}, {"created": "2019-03-11T16:58:03Z", "modified": "2019-03-11T16:58:03Z", "text": "We've received a report of an issue with Google Cloud Console as of Monday, 2019-03-11 09:38 US/Pacific. We will provide more information by Monday, 2019-03-11 11:00 US/Pacific.", "when": "2019-03-11T16:58:03Z"}, {"created": "2019-03-11T16:58:01Z", "modified": "2019-03-11T16:58:01Z", "text": "We've received a report of an issue with Google Cloud Console.", "when": "2019-03-11T16:58:01Z"}], "uri": "/incident/developers-console/19001"}, {"begin": "2019-03-08T08:45:50Z", "created": "2019-03-08T09:23:51Z", "end": "2019-03-08T09:30:29Z", "external_desc": "High error rate on multiple Google BigQuery APIs in the US region", "modified": "2019-03-18T18:25:12Z", "most-recent-update": {"created": "2019-03-18T18:25:12Z", "modified": "2019-03-18T18:25:12Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 8 March 2019, Google BigQuery\u2019s jobs.insert API in the US regions experienced an average elevated error rate of 51.21% for a duration of 45 minutes. BigQuery\u2019s Streaming API was unaffected during this period. We understand how important BigQuery\u2019s availability is to our customers\u2019 business analytics and we sincerely apologize for the impact caused by this incident. We are taking immediate steps detailed below to prevent this situation from happening again.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 8 March 2019 from 00:45 - 01:30 US/Pacific, BigQuery\u2019s jobs.insert [1] API (responsible for import/export, query, and copy jobs) in the US region experienced an average error rate of 51.21%. Affected customers received error responses such as \u201cError encountered during Execution, retrying may solve the problem\u201d and \u201cRead timed out\u201d when sending requests to BigQuery. BigQuery\u2019s Streaming API was not impacted by this incident.\r\n\r\nThe following is a breakdown of the errors experienced during the incident: \r\n\r\n- 64.01% of jobs.insert API requests to BigQuery (US) received HTTP 503 errors\r\n\r\n- The jobs.insert API experienced an average error rate of 51.21% and a peak error rate of 75.96% percent at 01:21 US/Pacific\r\n\r\n- 17.93% of BigQuery projects in the region were impacted\r\n\r\n\r\nROOT CAUSE\r\n\r\nA recent change to BigQuery\u2019s shuffle scheduling service [2] introduced the potential for the service to enter a state where it was unable to process shuffle jobs. A new canary release was deployed to fix the potential issue. However, this release contained an unrelated issue which placed an overly restrictive rate limit on the shuffle service preventing it from operating nominally. This strict rate limit created a large job backlog for the BigQuery Job Server, which resulted in BigQuery returning errors such as \u201cError encountered during Execution, retrying may solve the problem\u201d and \u201cRead timed out\u201d to users.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle Engineers were automatically alerted at 00:47 and immediately began their investigation. The root cause was discovered at 01:23, and our engineers worked quickly to mitigate the issue by redirecting traffic away from the impacted datacenter at 01:27. The incident was fully resolved by 01:30.\r\n\r\nWe are taking immediate action to prevent recurrence. First, we have implemented a fix to prevent the shuffle service from potentially entering a state where it is unable to process jobs. Second, we are allocating additional capacity to BigQuery\u2019s US region to reduce the impact of traffic redirections on adjacent datacenters running the service. Additionally, we are increasing the precision of our monitoring to enable more swift and accurate diagnosing of BigQuery issues going forward.\r\n\r\n\r\n[1] https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/insert\r\n[2] https://cloud.google.com/blog/products/gcp/in-memory-query-execution-in-google-bigquery", "when": "2019-03-18T18:25:12Z"}, "number": 19002, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-03-18T18:25:12Z", "modified": "2019-03-18T18:25:12Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 8 March 2019, Google BigQuery\u2019s jobs.insert API in the US regions experienced an average elevated error rate of 51.21% for a duration of 45 minutes. BigQuery\u2019s Streaming API was unaffected during this period. We understand how important BigQuery\u2019s availability is to our customers\u2019 business analytics and we sincerely apologize for the impact caused by this incident. We are taking immediate steps detailed below to prevent this situation from happening again.\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 8 March 2019 from 00:45 - 01:30 US/Pacific, BigQuery\u2019s jobs.insert [1] API (responsible for import/export, query, and copy jobs) in the US region experienced an average error rate of 51.21%. Affected customers received error responses such as \u201cError encountered during Execution, retrying may solve the problem\u201d and \u201cRead timed out\u201d when sending requests to BigQuery. BigQuery\u2019s Streaming API was not impacted by this incident.\r\n\r\nThe following is a breakdown of the errors experienced during the incident: \r\n\r\n- 64.01% of jobs.insert API requests to BigQuery (US) received HTTP 503 errors\r\n\r\n- The jobs.insert API experienced an average error rate of 51.21% and a peak error rate of 75.96% percent at 01:21 US/Pacific\r\n\r\n- 17.93% of BigQuery projects in the region were impacted\r\n\r\n\r\nROOT CAUSE\r\n\r\nA recent change to BigQuery\u2019s shuffle scheduling service [2] introduced the potential for the service to enter a state where it was unable to process shuffle jobs. A new canary release was deployed to fix the potential issue. However, this release contained an unrelated issue which placed an overly restrictive rate limit on the shuffle service preventing it from operating nominally. This strict rate limit created a large job backlog for the BigQuery Job Server, which resulted in BigQuery returning errors such as \u201cError encountered during Execution, retrying may solve the problem\u201d and \u201cRead timed out\u201d to users.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle Engineers were automatically alerted at 00:47 and immediately began their investigation. The root cause was discovered at 01:23, and our engineers worked quickly to mitigate the issue by redirecting traffic away from the impacted datacenter at 01:27. The incident was fully resolved by 01:30.\r\n\r\nWe are taking immediate action to prevent recurrence. First, we have implemented a fix to prevent the shuffle service from potentially entering a state where it is unable to process jobs. Second, we are allocating additional capacity to BigQuery\u2019s US region to reduce the impact of traffic redirections on adjacent datacenters running the service. Additionally, we are increasing the precision of our monitoring to enable more swift and accurate diagnosing of BigQuery issues going forward.\r\n\r\n\r\n[1] https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/insert\r\n[2] https://cloud.google.com/blog/products/gcp/in-memory-query-execution-in-google-bigquery", "when": "2019-03-18T18:25:12Z"}, {"created": "2019-03-08T10:51:30Z", "modified": "2019-03-08T10:51:30Z", "text": "The issue with Google BigQuery API returning 503 errors has been resolved for all affected projects as of 1:30 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-08T10:51:29Z"}, {"created": "2019-03-08T10:30:29Z", "modified": "2019-03-08T10:30:29Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Friday, 2019-03-08 03:30 US/Pacific with current details.", "when": "2019-03-08T10:30:29Z"}, {"created": "2019-03-08T09:24:09Z", "modified": "2019-03-08T09:24:09Z", "text": "We are investigating an issue with Google BigQuery. We will provide more information by Friday, 2019-03-08 02:30 US/Pacific.", "when": "2019-03-08T09:24:09Z"}, {"created": "2019-03-08T09:24:06Z", "modified": "2019-03-08T09:24:06Z", "text": "We are investigation an issue with Google BigQuery.", "when": "2019-03-08T09:24:06Z"}], "uri": "/incident/bigquery/19002"}, {"begin": "2019-03-08T04:30:00Z", "created": "2019-03-08T05:32:12Z", "end": "2019-03-08T12:02:20Z", "external_desc": "We are investigating an issue with Google Cloud Dialogflow - customers will experience 502 error messages.", "modified": "2019-03-08T12:02:20Z", "most-recent-update": {"created": "2019-03-08T12:02:20Z", "modified": "2019-03-08T12:02:20Z", "text": "The global issue with Google Cloud Dialogflow has been fully mitigated at Friday, 2019-03-08 03:21 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-08T12:02:20Z"}, "number": 19001, "public": true, "service_key": "cloud-ml", "service_name": "Cloud Machine Learning", "severity": "medium", "updates": [{"created": "2019-03-08T12:02:20Z", "modified": "2019-03-08T12:02:20Z", "text": "The global issue with Google Cloud Dialogflow has been fully mitigated at Friday, 2019-03-08 03:21 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-08T12:02:20Z"}, {"created": "2019-03-08T10:59:43Z", "modified": "2019-03-08T10:59:43Z", "text": "The global issue with Google Cloud Dialogflow has been mitigated for majority of projects We will provide another update by Friday, 2019-03-08 04:00 US/Pacific.", "when": "2019-03-08T10:59:43Z"}, {"created": "2019-03-08T08:55:31Z", "modified": "2019-03-08T09:01:15Z", "text": "We are still investigating global issue with Google Cloud Dialogflow started at Thursday, 2019-03-07 20:30 US/Pacific. Customers will experience 502 error messages. There is no workaround at this time. The product team is actively investigating possible causes. We will provide the next update at Friday, 2019-03-08 03:00 US/Pacific.", "when": "2019-03-08T09:00:00Z"}, {"created": "2019-03-08T07:24:05Z", "modified": "2019-03-08T07:42:46Z", "text": "We are still investigating global issue with Google Cloud Dialogflow started at Thursday, 2019-03-07 20:30 US/Pacific. Customers will experience 502 error messages. There is no workaround at this time. The product team is actively investigating possible causes.", "when": "2019-03-08T07:00:00Z"}, {"created": "2019-03-08T05:32:12Z", "modified": "2019-03-08T05:32:12Z", "text": "We are investigating an issue with Google Cloud Dialogflow - customers will experience 502 error messages.", "when": "2019-03-08T05:32:12Z"}], "uri": "/incident/cloud-ml/19001"}, {"begin": "2019-03-08T00:44:35Z", "created": "2019-03-08T00:44:35Z", "end": "2019-03-08T00:48:02Z", "external_desc": "We are investigating an issue with Google Stackdriver Monitoring.", "modified": "2019-03-08T00:48:03Z", "most-recent-update": {"created": "2019-03-08T00:48:03Z", "modified": "2019-03-08T00:48:03Z", "text": "The Google Stackdriver issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-08T00:48:03Z"}, "number": 19001, "public": true, "service_key": "google-stackdriver", "service_name": "Google Stackdriver", "severity": "medium", "updates": [{"created": "2019-03-08T00:48:03Z", "modified": "2019-03-08T00:48:03Z", "text": "The Google Stackdriver issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-08T00:48:03Z"}, {"created": "2019-03-08T00:44:36Z", "modified": "2019-03-08T00:44:36Z", "text": "We are investigating an issue with Google Stackdriver Monitoring. We will provide more information by Thursday, 2019-03-07 17:30 US/Pacific.", "when": "2019-03-08T00:44:36Z"}], "uri": "/incident/google-stackdriver/19001"}, {"begin": "2019-03-07T13:49:56Z", "created": "2019-03-07T13:49:57Z", "end": "2019-03-07T15:24:30Z", "external_desc": "GKE API unavailable in region europe-west4", "modified": "2019-03-07T15:24:31Z", "most-recent-update": {"created": "2019-03-07T15:24:31Z", "modified": "2019-03-07T15:24:31Z", "text": "The issue with Google Kubernetes Engine API in europe-west4 has been resolved for all affected projects as of Thursday, 2019-03-07 7:09 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-07T15:24:31Z"}, "number": 19005, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-03-07T15:24:31Z", "modified": "2019-03-07T15:24:31Z", "text": "The issue with Google Kubernetes Engine API in europe-west4 has been resolved for all affected projects as of Thursday, 2019-03-07 7:09 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-03-07T15:24:31Z"}, {"created": "2019-03-07T14:44:14Z", "modified": "2019-03-07T14:44:14Z", "text": "Our Engineering Team believes they have identified the potential root cause of the issue and is working to mitigate. We will provide another status update by Thursday, 2019-03-07 07:45 US/Pacific with current details.", "when": "2019-03-07T14:44:14Z"}, {"created": "2019-03-07T14:15:24Z", "modified": "2019-03-07T14:15:24Z", "text": "Our Engineering Team believes they have identified the potential root cause of the issue and is working to mitigate. We will provide another status update by Thursday, 2019-03-07 06:45 US/Pacific with current details.", "when": "2019-03-07T14:15:24Z"}, {"created": "2019-03-07T13:49:59Z", "modified": "2019-03-07T13:49:59Z", "text": "We are experiencing an issue with GKE API beginning at Thursday, 2019-03-07 5:41 US/Pacific. Current data indicate that all GKE API requests in region europe-west4 are failing. For everyone who is affected, we apologize for the disruption. We will provide an update by Thursday, 2019-03-07 06:15 US/Pacific with current details.", "when": "2019-03-07T13:49:59Z"}, {"created": "2019-03-07T13:49:57Z", "modified": "2019-03-07T13:49:57Z", "text": "GKE API unavailable in region europe-west4", "when": "2019-03-07T13:49:57Z"}], "uri": "/incident/container-engine/19005"}, {"begin": "2019-03-07T07:37:48Z", "created": "2019-03-07T07:37:49Z", "end": "2019-03-07T16:14:01Z", "external_desc": "Google Cloud Networking issue with Cloud Routers in us-east4", "modified": "2019-03-12T20:43:07Z", "most-recent-update": {"created": "2019-03-12T20:43:07Z", "modified": "2019-03-12T20:43:07Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Wednesday 6 March 2019, Google Cloud Router and Cloud Interconnect experienced a service disruption in the us-east4 region for a duration of 8 hours and 34 minutes. Cloud VPN configurations with dynamic routes via Cloud Router were impacted during this time. We apologize to our customers who were impacted by this outage.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 6 March 2019 from 20:17 to Thursday 7 March 04:51 US/Pacific, Cloud Router and Cloud Interconnect experienced a service disruption in us-east4. Customers utilizing us-east4 were unable to advertise routes to their Google Compute Engine (GCE) instances or learn routes from GCE. \r\n\r\nCloud VPN traffic with dynamic routes over Cloud Router and Cloud Interconnect in us-east4 was impacted by this service disruption. Cloud VPN traffic over pre-configured static routes was unaffected and continued to function without disruption during this time.\r\n\r\n# ROOT CAUSE\r\n\r\nThe Cloud Router control plane service assigns Cloud Router tasks to individual customers and creates routes between those tasks and customer VPCs. Individual Cloud Router tasks establish external BGP sessions and propagate routes to and from the control plane service.\r\n\r\nA disruption occurred during the rollout of a new version of the control plane service in us-east4. This required the control plane to restart from a \u201ccold\u201d state requiring it to validate all assignments of the Cloud Router tasks. The control plane service did not successfully initialize and it was unable to assign individual Cloud Router tasks in order to propagate routes between those tasks and customer VPCs. Cloud Router tasks became temporarily disassociated with customers and BGP sessions were terminated. As a result, Cloud VPN and Cloud Interconnect configurations that were dependent on Cloud Router in us-east4 were unavailable during this time.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted at 20:30 PST on 6 March 2019 and immediately began an investigation. A fix for the control plane service was tested, integrated, and rolled out on 7 March 2019 at 04:33 US/Pacific. The control plane service fully recovered by 05:16 US/Pacific.\r\n\r\nWe are taking immediate steps to prevent recurrence. The issue that prevented the control plane from restarting has been resolved. In order to ensure faster incident detection, we are improving control plane service testing, the instrumentation of Cloud Router tasks, and the control plane service instrumentation.", "when": "2019-03-12T20:43:07Z"}, "number": 19005, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-03-12T20:43:07Z", "modified": "2019-03-12T20:43:07Z", "text": "# ISSUE SUMMARY\r\n\r\nOn Wednesday 6 March 2019, Google Cloud Router and Cloud Interconnect experienced a service disruption in the us-east4 region for a duration of 8 hours and 34 minutes. Cloud VPN configurations with dynamic routes via Cloud Router were impacted during this time. We apologize to our customers who were impacted by this outage.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 6 March 2019 from 20:17 to Thursday 7 March 04:51 US/Pacific, Cloud Router and Cloud Interconnect experienced a service disruption in us-east4. Customers utilizing us-east4 were unable to advertise routes to their Google Compute Engine (GCE) instances or learn routes from GCE. \r\n\r\nCloud VPN traffic with dynamic routes over Cloud Router and Cloud Interconnect in us-east4 was impacted by this service disruption. Cloud VPN traffic over pre-configured static routes was unaffected and continued to function without disruption during this time.\r\n\r\n# ROOT CAUSE\r\n\r\nThe Cloud Router control plane service assigns Cloud Router tasks to individual customers and creates routes between those tasks and customer VPCs. Individual Cloud Router tasks establish external BGP sessions and propagate routes to and from the control plane service.\r\n\r\nA disruption occurred during the rollout of a new version of the control plane service in us-east4. This required the control plane to restart from a \u201ccold\u201d state requiring it to validate all assignments of the Cloud Router tasks. The control plane service did not successfully initialize and it was unable to assign individual Cloud Router tasks in order to propagate routes between those tasks and customer VPCs. Cloud Router tasks became temporarily disassociated with customers and BGP sessions were terminated. As a result, Cloud VPN and Cloud Interconnect configurations that were dependent on Cloud Router in us-east4 were unavailable during this time.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted at 20:30 PST on 6 March 2019 and immediately began an investigation. A fix for the control plane service was tested, integrated, and rolled out on 7 March 2019 at 04:33 US/Pacific. The control plane service fully recovered by 05:16 US/Pacific.\r\n\r\nWe are taking immediate steps to prevent recurrence. The issue that prevented the control plane from restarting has been resolved. In order to ensure faster incident detection, we are improving control plane service testing, the instrumentation of Cloud Router tasks, and the control plane service instrumentation.", "when": "2019-03-12T20:43:07Z"}, {"created": "2019-03-07T16:14:11Z", "modified": "2019-03-07T16:14:11Z", "text": "The issue with Google Cloud Routers in us-east4 has been resolved for all affected projects as of Thursday, 2019-03-07 7:55 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2019-03-07T16:14:11Z"}, {"created": "2019-03-07T14:27:21Z", "modified": "2019-03-07T14:27:21Z", "text": "Our Engineering Team believes they have identified the root cause of the errors and provided mitigation. The issue with Cloud Routers in us-east4 should be resolved for majority of our customers as of Thursday, 2019-03-07 06:08 US/Pacific. And we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-03-07 09:00 US/Pacific with current details.", "when": "2019-03-07T14:27:21Z"}, {"created": "2019-03-07T13:08:38Z", "modified": "2019-03-07T13:08:38Z", "text": "Our Engineering Team believes they have identified the root cause of the errors and provided mitigation. The issue with Cloud Routers in us-east4 should be resolved for majority of our customers as of Thursday, 2019-03-07 05:00 US/Pacific. And we expect a full resolution in the near future. We will provide another status update by Thursday, 2019-03-07 06:30 US/Pacific with current details.", "when": "2019-03-07T13:08:38Z"}, {"created": "2019-03-07T12:31:30Z", "modified": "2019-03-07T12:31:30Z", "text": "We are experiencing an issue with Google Cloud Networking beginning at Wednesday, 2019-03-06 21:15 US/Pacific. Current investigation indicates that approximately 100% of Cloud Router in us-east4 region are affected by this issue. Users will experience BGP sessions down on all of their Cloud Router enabled VPN tunnels and Cloud Interconnect VLAN Attachments in us-east4 region. Further us-east4 subnets might be not redistributed to other regions as part of VPC Global routing mode, thus making this region unreachable over Interconnect. As a workaround customers can setup a Cloud VPN without Cloud Router betwen us-east4 and their on-premise network.  Cloud Console might be timing out for getting Cloud Router related status information, please use gcloud instead. Other regions are not affected. The engineering team is investigating the issue and we will provide another status update by Thursday, 2019-03-07 05:30 US/Pacific with current details.", "when": "2019-03-07T12:31:30Z"}, {"created": "2019-03-07T11:31:48Z", "modified": "2019-03-07T11:31:48Z", "text": "We are experiencing an issue with Google Cloud Networking beginning at Wednesday, 2019-03-06 21:15 US/Pacific. Current investigation indicates that approximately 100% of Cloud Router in us-east4 region are affected by this issue. Users will experience BGP sessions down on all of their Cloud Router enabled VPN tunnels and Cloud Interconnect links in us-east4 region. Further us-east4 subnets might be not redistributed to other regions as part of VPC Global routing mode, thus making this region unreachable over Interconnect. As a workaround customers can setup a Cloud VPN without Cloud Router betwen us-east4 and their on-premise network.\r\nOther regions are not affected. The engineering team is investigating the issue and we will provide another status update by Thursday, 2019-03-07 04:30 US/Pacific with current details.", "when": "2019-03-07T11:31:48Z"}, {"created": "2019-03-07T10:31:15Z", "modified": "2019-03-07T10:31:15Z", "text": "We are experiencing an issue with Google Cloud Networking beginning at Wednesday, 2019-03-06 21:15 US/Pacific. Current investigation indicates that approximately 100% of Cloud Router in us-east4 region are affected by this issue. Users will experience BGP sessions down on all of their Cloud Router enabled VPN tunnels and Cloud Interconnect links in us-east4 region. Other regions are not affected. The engineering team is investigating the issue and we will provide another status update by Thursday, 2019-03-07 03:30 US/Pacific with current details.", "when": "2019-03-07T10:31:15Z"}, {"created": "2019-03-07T08:40:42Z", "modified": "2019-03-07T08:40:42Z", "text": "We are experiencing an issue with Google Cloud Networking beginning at Wednesday, 2019-03-06 21:15 US/Pacific. Current investigation indicates that approximately 100% of Cloud Router users in us-east4 region are affected by this issue. Users will experience BGP sessions down on all of their Cloud Router enabled VPN tunnels and Cloud Interconnect links in us-east4 region. Other regions are not affected. We will provide another status update by Thursday, 2019-03-07 02:30 US/Pacific with current details.", "when": "2019-03-07T08:40:42Z"}, {"created": "2019-03-07T07:42:50Z", "modified": "2019-03-07T07:42:50Z", "text": "We are experiencing an issue with Google Cloud Networking beginning at Wednesday, 2019-03-06 21:15 US/Pacific. Current investigation indicates that approximately 100% of Cloud Router users in us-east4 region are affected by this issue. Users will experience BGP sessions down on all of their Cloud Router enabled VPN tunnels and Cloud Interconnect links in us-east4 region. Other regions are not affected. We will provide another status update by Thursday, 2019-03-07 00:40 US/Pacific with current details", "when": "2019-03-07T07:42:49Z"}, {"created": "2019-03-07T07:37:51Z", "modified": "2019-03-07T07:37:51Z", "text": "We are still seeing errors on the services responsible for the Cloud Router BGP issue in us-east4 region. Our Engineering team is still working on the mitigation at the moment. We will provide another status update by Thursday, 2019-03-07 00:40 US/Pacific with current details.", "when": "2019-03-07T07:37:51Z"}, {"created": "2019-03-07T07:37:50Z", "modified": "2019-03-07T07:37:50Z", "text": "We've received a report of an issue with Google Cloud Networking", "when": "2019-03-07T07:37:50Z"}], "uri": "/incident/cloud-networking/19005"}, {"begin": "2019-03-05T18:38:48Z", "created": "2019-03-05T20:38:48Z", "end": "2019-03-05T22:52:39Z", "external_desc": "Instance connectivity issues in us-west1, us-central1, asia-east1 and europe-west1.", "modified": "2019-03-05T22:52:40Z", "most-recent-update": {"created": "2019-03-05T22:52:40Z", "modified": "2019-03-05T22:52:40Z", "text": "The Google Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-05T22:52:40Z"}, "number": 19004, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-03-05T22:52:40Z", "modified": "2019-03-05T22:52:40Z", "text": "The Google Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-05T22:52:40Z"}, {"created": "2019-03-05T22:52:38Z", "modified": "2019-03-05T22:52:38Z", "text": "The Google Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-05T22:52:38Z"}, {"created": "2019-03-05T22:31:12Z", "modified": "2019-03-05T22:31:12Z", "text": "The Google Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-05T22:31:12Z"}, {"created": "2019-03-05T22:27:21Z", "modified": "2019-03-05T22:27:21Z", "text": "The Google Cloud Networking issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-03-05T22:27:21Z"}, {"created": "2019-03-05T21:58:49Z", "modified": "2019-03-05T21:58:49Z", "text": "The issue with Google Cloud Networking instance connectivity should be resolved for some of users and we expect a full resolution in the near future. We will provide another status update by Tuesday, 2019-03-05 15:00 US/Pacific with current details.", "when": "2019-03-05T21:58:49Z"}, {"created": "2019-03-05T21:06:26Z", "modified": "2019-03-05T21:06:26Z", "text": "Mitigation work is currently underway by our Engineering Team. We have reports of this effecting instances in us-west1, us-central1, asia-east1 and europe-west1. We will provide another status update by Tuesday, 2019-03-05 14:00 US/Pacific with current details.", "when": "2019-03-05T21:06:26Z"}, {"created": "2019-03-05T20:38:53Z", "modified": "2019-03-05T20:38:53Z", "text": "We are investigating instance connectivity issues with Google Cloud Networking. We have had reports in us-west1, us-central1 and asia-east1. Our Engineering Team is investigating possible causes. We will provide another status update by Tuesday, 2019-03-05 13:30 US/Pacific with current details.", "when": "2019-03-05T20:38:53Z"}, {"created": "2019-03-05T20:38:50Z", "modified": "2019-03-05T20:38:50Z", "text": "Instance connectivity issues in us-west1, us-central1 and asia-east1.", "when": "2019-03-05T20:38:50Z"}], "uri": "/incident/cloud-networking/19004"}, {"begin": "2019-02-26T23:30:28Z", "created": "2019-02-26T23:30:46Z", "end": "2019-02-27T04:53:04Z", "external_desc": "Issues with Google Kubernetes Engine GetClusters API endpoint in europe-west3 and us-east1.", "modified": "2019-02-27T04:53:23Z", "most-recent-update": {"created": "2019-02-27T04:53:23Z", "modified": "2019-02-27T04:53:23Z", "text": "The issue with Google Kubernetes Engine GetClusters API endpoint in europe-west3 and us-east1 has been resolved for all affected users as of 2019-02-26 20:50  US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-27T04:53:23Z"}, "number": 19004, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-02-27T04:53:23Z", "modified": "2019-02-27T04:53:23Z", "text": "The issue with Google Kubernetes Engine GetClusters API endpoint in europe-west3 and us-east1 has been resolved for all affected users as of 2019-02-26 20:50  US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-27T04:53:23Z"}, {"created": "2019-02-27T03:59:07Z", "modified": "2019-02-27T03:59:07Z", "text": "Our engineering team is continuing to work on the issues. We will provide another status update by Tuesday, 2019-02-26 21:00 US/Pacific with current details.", "when": "2019-02-27T03:59:07Z"}, {"created": "2019-02-27T02:58:46Z", "modified": "2019-02-27T02:58:46Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-02-26 20:00 US/Pacific with current details.", "when": "2019-02-27T02:58:46Z"}, {"created": "2019-02-27T01:59:37Z", "modified": "2019-02-27T01:59:37Z", "text": "Further to this, the issue persists despite the previously published actions and we are pursuing alternative solutions. We will provide another status update by Tuesday, 2019-02-26 19:00 US/Pacific with current details.", "when": "2019-02-27T01:59:37Z"}, {"created": "2019-02-27T00:28:52Z", "modified": "2019-02-27T00:28:52Z", "text": "Mitigation work is currently underway by our Engineering Team. We are currently rolling back a change to mitigate the issue in the affected regions.  We will provide another status update by Tuesday, 2019-02-26 18:00 US/Pacific with current details.", "when": "2019-02-27T00:28:52Z"}, {"created": "2019-02-26T23:30:49Z", "modified": "2019-02-26T23:30:49Z", "text": "We are experiencing an issue with Google Kubernetes Engine GetClusters API endpoint beginning Tuesday, 2019-02-26 14:52 US/Pacific. Current data indicate(s) that this issue affects europe-west3 and us-east1. The issue partially affected europe-west2-a and us-east1-d but these zones have recovered as of Tuesday, 2019-02-26 15:10 US/Pacific. \r\n\r\nUsers affected by this issue may receive HTTP 5XX errors when listing clusters with the GetClusters API Endpoint. Users will also be unable to resize, upgrade or repair their clusters in the affected regions. \r\n\r\nFor everyone who is affected, we apologize for the disruption. We will provide an update by Tuesday, 2019-02-26 16:30 US/Pacific with current details.", "when": "2019-02-26T23:30:49Z"}, {"created": "2019-02-26T23:30:47Z", "modified": "2019-02-26T23:30:47Z", "text": "Issues with Google Kubernetes Engine GetClusters API endpoint in europe-west3 and us-east1.", "when": "2019-02-26T23:30:47Z"}], "uri": "/incident/container-engine/19004"}, {"begin": "2019-02-26T21:00:00Z", "created": "2019-02-26T21:58:04Z", "end": "2019-02-26T23:21:25Z", "external_desc": "We've received a report of an issue with updating or deploying Google Cloud Functions", "modified": "2019-02-26T23:21:26Z", "most-recent-update": {"created": "2019-02-26T23:21:26Z", "modified": "2019-02-26T23:21:26Z", "text": "The issue with Google Cloud Function deployments seeing increased errors has been resolved for all affected users as of Tuesday, 2019-02-26 15:15 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-26T23:21:25Z"}, "number": 19001, "public": true, "service_key": "cloud-functions", "service_name": "Google Cloud Functions", "severity": "medium", "updates": [{"created": "2019-02-26T23:21:26Z", "modified": "2019-02-26T23:21:26Z", "text": "The issue with Google Cloud Function deployments seeing increased errors has been resolved for all affected users as of Tuesday, 2019-02-26 15:15 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-26T23:21:25Z"}, {"created": "2019-02-26T22:55:50Z", "modified": "2019-02-26T22:55:50Z", "text": "The issue with Google Cloud Function deployments seeing increased errors should be resolved for the majority of users and we expect a full resolution in the near future. We will provide another status update by Tuesday, 2019-02-26 16:00 US/Pacific with current details.", "when": "2019-02-26T22:55:50Z"}, {"created": "2019-02-26T22:22:10Z", "modified": "2019-02-26T22:22:10Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Tuesday, 2019-02-26 15:00 US/Pacific with current details.", "when": "2019-02-26T22:22:10Z"}, {"created": "2019-02-26T21:58:04Z", "modified": "2019-02-26T21:58:04Z", "text": "We've received a report of an issue with Google Cloud Function deployments seeing increased errors as of Tuesday, 2019-02-26 13:00 US/Pacific. We will provide more information by Tuesday, 2019-02-26 14:45 US/Pacific.", "when": "2019-02-26T21:58:04Z"}], "uri": "/incident/cloud-functions/19001"}, {"begin": "2019-02-26T11:02:00Z", "created": "2019-02-26T16:06:42Z", "end": "2019-02-26T16:32:26Z", "external_desc": "Seeing increased write error rates with Google Cloud Datastore in europe-west1 due to timeouts.", "modified": "2019-02-26T17:04:43Z", "most-recent-update": {"created": "2019-02-26T17:04:26Z", "modified": "2019-02-26T17:04:26Z", "text": "The issue with with Google Cloud Datastore experiencing increased write errors in europe-west1 has been resolved for all affected projects as of 08:32 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-26T17:04:26Z"}, "number": 19002, "public": true, "service_key": "cloud-datastore", "service_name": "Google Cloud Datastore", "severity": "medium", "updates": [{"created": "2019-02-26T17:04:26Z", "modified": "2019-02-26T17:04:26Z", "text": "The issue with with Google Cloud Datastore experiencing increased write errors in europe-west1 has been resolved for all affected projects as of 08:32 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-26T17:04:26Z"}, {"created": "2019-02-26T16:06:43Z", "modified": "2019-02-26T16:06:43Z", "text": "We are investigating an issue with Google Cloud Datastore experiencing increased write errors in europe-west1 starting at 03:02 US/Pacific. We have engaged additional teams to help investigate the root cause and mitigate the issue. We will provide more information by Tuesday, 2019-02-26 09:30 US/Pacific.", "when": "2019-02-26T16:06:43Z"}], "uri": "/incident/cloud-datastore/19002"}, {"begin": "2019-02-22T17:19:55Z", "created": "2019-02-22T17:20:11Z", "end": "2019-02-22T18:31:09Z", "external_desc": "Elevated error rates with Google BigQuery Streaming Inserts in the US region beginning at Friday, 2019-02-22 7:33 US/Pacific", "modified": "2019-02-22T18:31:11Z", "most-recent-update": {"created": "2019-02-22T18:31:11Z", "modified": "2019-02-22T18:31:11Z", "text": "The issue with Google BigQuery Streaming Insert errors in the US has been resolved for all affected projects as of Friday, 2019-02-22 10:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-22T18:31:10Z"}, "number": 19001, "public": true, "service_key": "bigquery", "service_name": "Google BigQuery", "severity": "medium", "updates": [{"created": "2019-02-22T18:31:11Z", "modified": "2019-02-22T18:31:11Z", "text": "The issue with Google BigQuery Streaming Insert errors in the US has been resolved for all affected projects as of Friday, 2019-02-22 10:18 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-22T18:31:10Z"}, {"created": "2019-02-22T18:07:28Z", "modified": "2019-02-22T18:07:28Z", "text": "The issue with Google BigQuery Streaming Inserts in the US region beginning at Friday, 2019-02-22 7:33 US/Pacific should be resolved for the majority of projects and we expect a full resolution in the near future. We will provide another status update by Friday, 2019-02-22 10:45 US/Pacific with current details.", "when": "2019-02-22T18:07:28Z"}, {"created": "2019-02-22T17:51:34Z", "modified": "2019-02-22T17:51:34Z", "text": "Our Engineering Team believes they have identified the root cause of the errors and is working to mitigate. We will provide another status update by Friday, 2019-02-22 10:45 US/Pacific with current details.", "when": "2019-02-22T17:51:34Z"}, {"created": "2019-02-22T17:24:43Z", "modified": "2019-02-22T17:24:43Z", "text": "Mitigation is currently underway by our Engineering Team. We will provide another status update by Friday, 2019-02-22 10:00 US/Pacific with current details.", "when": "2019-02-22T17:24:43Z"}, {"created": "2019-02-22T17:20:13Z", "modified": "2019-02-22T17:20:13Z", "text": "We are experiencing an elevated error rate with Google BigQuery Streaming Inserts in the US region beginning at Friday, 2019-02-22 7:33 US/Pacific. Affected users will see 500 errors returned for their streaming inserts. For everyone who is affected, we apologize for the disruption. We will provide an update by Friday, 2019-02-22 10:00 US/Pacific with current details.", "when": "2019-02-22T17:20:13Z"}, {"created": "2019-02-22T17:20:12Z", "modified": "2019-02-22T17:20:12Z", "text": "We are experiencing an elevated error rate with Google BigQuery Streaming Inserts in the US region beginning at Friday, 2019-02-22 7:33 US/Pacific.", "when": "2019-02-22T17:20:12Z"}], "uri": "/incident/bigquery/19001"}, {"begin": "2019-02-20T08:45:23Z", "created": "2019-02-20T08:45:55Z", "end": "2019-02-20T12:29:00Z", "external_desc": "The URL Fetch API for Google App Engine service is failing for some of our customers.", "modified": "2019-02-20T12:33:58Z", "most-recent-update": {"created": "2019-02-20T12:33:58Z", "modified": "2019-02-20T12:33:58Z", "text": "The issue with URL Fetch API for Google App Engine service has been resolved for all affected customers as of 2019-02-20 03:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-20T12:29:00Z"}, "number": 19003, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-02-20T12:33:58Z", "modified": "2019-02-20T12:33:58Z", "text": "The issue with URL Fetch API for Google App Engine service has been resolved for all affected customers as of 2019-02-20 03:27 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-20T12:29:00Z"}, {"created": "2019-02-20T11:54:09Z", "modified": "2019-02-20T11:54:09Z", "text": "Mitigation work is currently underway by our Engineering Team and the rate of errors is decreasing. We will provide another status update by Wednesday, 2019-02-20 05:00 US/Pacific with current details.", "when": "2019-02-20T10:58:00Z"}, {"created": "2019-02-20T11:53:36Z", "modified": "2019-02-20T11:53:36Z", "text": "The URL Fetch API for Google App Engine service is failing for some of our customers. If you see MalformedURLExceptions type errors when using the URL Fetch API then you are affected by this issue. Currently there is no work around for this issue. Mitigation work is currently underway by our Engineering Team.", "when": "2019-02-20T09:00:00Z"}], "uri": "/incident/appengine/19003"}, {"begin": "2019-02-14T01:06:21Z", "created": "2019-02-14T01:06:30Z", "end": "2019-02-14T01:13:21Z", "external_desc": "Google Kubernetes Engine may clear certain add-ons from the configuration UI after a cluster upgrade.", "modified": "2019-02-14T01:13:43Z", "most-recent-update": {"created": "2019-02-14T01:13:43Z", "modified": "2019-02-14T01:13:43Z", "text": "The Google Kubernetes Engine issue is believed to be affecting less than 1% of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-02-14T01:13:43Z"}, "number": 19003, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-02-14T01:13:43Z", "modified": "2019-02-14T01:13:43Z", "text": "The Google Kubernetes Engine issue is believed to be affecting less than 1% of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-02-14T01:13:43Z"}, {"created": "2019-02-14T01:06:42Z", "modified": "2019-02-14T01:06:42Z", "text": "We've received a report of an issue with Google Kubernetes Engine cluster upgrades as of Wednesday, 2019-02-13 16:04 US/Pacific. We will provide more information by Wednesday, 2019-02-13 19:00 US/Pacific.", "when": "2019-02-14T01:06:42Z"}, {"created": "2019-02-14T01:06:34Z", "modified": "2019-02-14T01:06:34Z", "text": "Google Kubernetes Engine may clear certain add-ons from the configuration UI after a cluster upgrade.", "when": "2019-02-14T01:06:34Z"}], "uri": "/incident/container-engine/19003"}, {"begin": "2019-02-13T15:37:01Z", "created": "2019-02-13T16:11:46Z", "end": "2019-02-13T15:46:18Z", "external_desc": "The issue with Google Cloud Networking in us-central1 has been resolved.", "modified": "2019-02-13T16:15:01Z", "most-recent-update": {"created": "2019-02-13T16:13:11Z", "modified": "2019-02-13T16:13:11Z", "text": "The issue with Google Cloud Networking in us-central1-b, us-central1-c or us-central1-f has been resolved for all affected users as of 07:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-13T16:13:10Z"}, "number": 19003, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-02-13T16:13:11Z", "modified": "2019-02-13T16:13:11Z", "text": "The issue with Google Cloud Networking in us-central1-b, us-central1-c or us-central1-f has been resolved for all affected users as of 07:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-13T16:13:10Z"}, {"created": "2019-02-13T16:12:17Z", "modified": "2019-02-13T16:15:01Z", "text": "We are investigating an issue with Google Cloud Networking in us-central1. Instances in us-central1-b, us-central1-c or us-central1-f may have seen increased packet loss between other regions and to the internet from 07:37 to 07:46 US/Pacific. We will provide more information by Wednesday, 2019-02-13 08:15 US/Pacific.", "when": "2019-02-13T15:58:17Z"}], "uri": "/incident/cloud-networking/19003"}, {"begin": "2019-02-05T17:21:00Z", "created": "2019-02-21T14:43:32Z", "end": "2019-02-05T18:17:00Z", "external_desc": "Elevated error rates in Google Cloud Storage in European multi-region", "modified": "2019-02-21T14:43:32Z", "most-recent-update": {"created": "2019-02-21T14:43:32Z", "modified": "2019-02-21T14:43:32Z", "text": "ISSUE SUMMARY\r\n\r\nGoogle Cloud Storage experienced elevated error rates averaging 10% across the European multi-region for GET, PUT, and DELETE requests. Google Engineering fully mitigated this incident at 10:17 on Feb 5, 2019.  We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-21T14:43:32Z"}, "number": 19001, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "high", "updates": [{"created": "2019-02-21T14:43:32Z", "modified": "2019-02-21T14:43:32Z", "text": "ISSUE SUMMARY\r\n\r\nGoogle Cloud Storage experienced elevated error rates averaging 10% across the European multi-region for GET, PUT, and DELETE requests. Google Engineering fully mitigated this incident at 10:17 on Feb 5, 2019.  We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-02-21T14:43:32Z"}], "uri": "/incident/storage/19001"}, {"begin": "2019-01-23T19:56:17Z", "created": "2019-01-23T19:56:39Z", "end": "2019-01-23T22:43:38Z", "external_desc": "Google Kubernetes Engine (GKE) is experiencing intermittent timeout errors during GKE cluster creation and deletion in all regions.", "modified": "2019-02-19T15:32:07Z", "most-recent-update": {"created": "2019-02-15T17:57:40Z", "modified": "2019-02-15T17:57:40Z", "text": "The Google Kubernetes Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-02-15T17:57:40Z"}, "number": 19001, "public": true, "service_key": "container-engine", "service_name": "Google Kubernetes Engine", "severity": "medium", "updates": [{"created": "2019-02-15T17:57:40Z", "modified": "2019-02-15T17:57:40Z", "text": "The Google Kubernetes Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-02-15T17:57:40Z"}, {"created": "2019-02-15T17:57:34Z", "modified": "2019-02-15T17:57:34Z", "text": "The Google Kubernetes Engine issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-02-15T17:57:34Z"}, {"created": "2019-01-23T22:43:27Z", "modified": "2019-01-23T22:43:27Z", "text": "The issue with Google Kubernetes Engine (GKE) experiencing intermittent timeout errors during GKE cluster creation and deletion in all regions has been resolved for all affected projects as of Wednesday, 2019-01-23 13:42 US/Pacific. We will continue monitoring GKE for any potential recurrence of the issue, but no further updates will be provided at this time. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-23T22:43:27Z"}, {"created": "2019-01-23T21:42:19Z", "modified": "2019-01-23T21:42:19Z", "text": "Our Engineering Team believes they have identified the potential root causes of the creation & deletion timeouts and is working to mitigate. We will provide another status update by Wednesday, 2019-01-23 14:45 US/Pacific with current details.", "when": "2019-01-23T21:42:19Z"}, {"created": "2019-01-23T20:43:08Z", "modified": "2019-01-23T20:43:08Z", "text": "We are experiencing an intermittent issue with Google Kubernetes Engine (GKE) cluster creation & deletion operations timing out in all regions beginning at Wednesday, 2019-01-23 11:29 US/Pacific. Google Engineers are currently investigating the root cause and attempting to mitigate the issue. Existing GKE clusters remain unaffected. For everyone who is affected, we apologize for the disruption. We will provide an update by Wednesday, 2019-01-23 13:45 US/Pacific with current details.", "when": "2019-01-23T20:43:08Z"}, {"created": "2019-01-23T19:56:43Z", "modified": "2019-01-23T19:56:43Z", "text": "The Google Cloud Kubernetes Engine (GKE) service is experiencing an elevated error rate upon GKE cluster creation and deletion in all regions. Affected customers may experience timeout errors upon cluster creation and deletion. Existing GKE clusters should remain unaffected. Google Engineers are currently investigating the issue. We will provide another status update by Wednesday, 2019-01-23 12:45 US/Pacific with current details.", "when": "2019-01-23T19:56:43Z"}, {"created": "2019-01-23T19:56:41Z", "modified": "2019-01-23T19:56:41Z", "text": "Google Cloud Kubernetes Engine (GKE) is experiencing experience timeout errors during GKE cluster creation and deletion in all regions.", "when": "2019-01-23T19:56:41Z"}], "uri": "/incident/container-engine/19001"}, {"begin": "2019-01-18T17:50:00Z", "created": "2019-01-18T17:50:00Z", "end": "2019-01-18T18:19:07Z", "external_desc": "Google Cloud Networking is currently experiencing an issue with packet loss in us-central1-[b,c,f]", "modified": "2019-01-18T18:19:08Z", "most-recent-update": {"created": "2019-01-18T18:19:08Z", "modified": "2019-01-18T18:19:08Z", "text": "The issue with Google Cloud Networking experiencing packet loss in us-central1-[b,c,f] is believed to be affecting less than 1% of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-01-18T18:19:08Z"}, "number": 19002, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-01-18T18:19:08Z", "modified": "2019-01-18T18:19:08Z", "text": "The issue with Google Cloud Networking experiencing packet loss in us-central1-[b,c,f] is believed to be affecting less than 1% of customers and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2019-01-18T18:19:08Z"}, {"created": "2019-01-18T18:09:10Z", "modified": "2019-01-18T18:09:10Z", "text": "The issue with Google Cloud Networking experiencing an issue with packet loss in us-central1-[b,c,f] should be resolved for the majority of customers and we expect a full resolution in the near future. We will provide another status update by Friday, 2019-01-18 11:00 US/Pacific with current details.", "when": "2019-01-18T18:09:10Z"}, {"created": "2019-01-18T17:50:03Z", "modified": "2019-01-18T17:50:03Z", "text": "Google Cloud Networking is currently experiencing an issue with packet loss in us-central1-[b,c,f] and mitigation work is currently underway by our Engineering Team. We will provide another status update by Friday, 2019-01-18 10:30 US/Pacific with current details.", "when": "2019-01-18T17:50:02Z"}, {"created": "2019-01-18T17:50:01Z", "modified": "2019-01-18T17:50:01Z", "text": "Google Cloud Networking is currently experiencing an issue with packet loss in us-central1-[b,c,f]", "when": "2019-01-18T17:50:01Z"}], "uri": "/incident/cloud-networking/19002"}, {"begin": "2019-01-18T03:37:18Z", "created": "2019-01-18T03:37:24Z", "end": "2019-01-18T04:36:58Z", "external_desc": "Google App Engine Flex deployments are failing in all regions.", "modified": "2019-01-18T04:37:00Z", "most-recent-update": {"created": "2019-01-18T04:37:00Z", "modified": "2019-01-18T04:37:00Z", "text": "The issue with Google App Engine Flex deployment failures in all regions has been resolved as of Thursday, 2019-01-17 20:25 US/Pacific. The affected customer deployments may need to be retried. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-18T04:37:00Z"}, "number": 19002, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "medium", "updates": [{"created": "2019-01-18T04:37:00Z", "modified": "2019-01-18T04:37:00Z", "text": "The issue with Google App Engine Flex deployment failures in all regions has been resolved as of Thursday, 2019-01-17 20:25 US/Pacific. The affected customer deployments may need to be retried. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-18T04:37:00Z"}, {"created": "2019-01-18T03:37:31Z", "modified": "2019-01-18T03:37:31Z", "text": "Google App Engine Flex deployments are failing in all regions as of Thursday, 2019-01-17 19:13 US/Pacific. We will provide more information by Thursday, 2019-01-17 20:45 US/Pacific.", "when": "2019-01-18T03:37:31Z"}, {"created": "2019-01-18T03:37:30Z", "modified": "2019-01-18T03:37:30Z", "text": "Google App Engine Flex deployments are failing in all regions.", "when": "2019-01-18T03:37:30Z"}], "uri": "/incident/appengine/19002"}, {"begin": "2019-01-17T16:26:34Z", "created": "2019-01-17T16:56:37Z", "end": "2019-01-17T16:46:53Z", "external_desc": "The issue with Google Cloud Networking in the us-east1 has been resolved.", "modified": "2019-01-17T17:06:36Z", "most-recent-update": {"created": "2019-01-17T17:01:56Z", "modified": "2019-01-17T17:01:56Z", "text": "The issue with Google Cloud Networking experiencing increased packet loss to instances in the us-east1 region has been resolved for all affected users as of 08:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-17T17:01:56Z"}, "number": 19001, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "medium", "updates": [{"created": "2019-01-17T17:01:56Z", "modified": "2019-01-17T17:01:56Z", "text": "The issue with Google Cloud Networking experiencing increased packet loss to instances in the us-east1 region has been resolved for all affected users as of 08:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-17T17:01:56Z"}, {"created": "2019-01-17T16:56:40Z", "modified": "2019-01-17T16:56:40Z", "text": "We are investigating an issue with Google Cloud Networking in the us-east1 region starting at 08:26 US/Pacific. We will provide more information by Thursday, 2019-01-17 09:30 US/Pacific", "when": "2019-01-17T16:56:40Z"}, {"created": "2019-01-17T16:56:39Z", "modified": "2019-01-17T16:56:39Z", "text": "We are investigating an issue with Google Cloud Networking in the us-east1", "when": "2019-01-17T16:56:39Z"}], "uri": "/incident/cloud-networking/19001"}, {"begin": "2019-01-02T22:40:09Z", "created": "2019-01-02T23:38:15Z", "end": "2019-01-03T02:20:36Z", "external_desc": "We are currently investigating an issue with Google App Engine app creation, Cloud Function deployments, and Project Creation in Cloud Console.", "modified": "2019-01-08T01:28:10Z", "most-recent-update": {"created": "2019-01-08T01:25:07Z", "modified": "2019-01-08T01:25:07Z", "text": "ISSUE SUMMARY\r\n\r\nOn Wednesday 2 January, 2019, application creation in Google App Engine (App Engine), first-time deployment of Google Cloud Functions (Cloud Functions) per region, and project creation & API management in Cloud Console experienced elevated error rates ranging from 71% to 100% for a duration of 3 hours, 40 minutes starting at 14:40 PST. Workloads already running on App Engine and Cloud Functions, including deployment of new versions of applications and functions, as well as ongoing use of existing projects and activated APIs, were not impacted.\r\n\r\nWe know that many customers depend on the ability to create new Cloud projects, applications & functions, and apologize for our failure to provide this to you during this time. The root cause of the incident is fully understood and engineering efforts are underway to ensure the issue is not at risk of recurrence.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 2 January, 2019 from 14:40 PST to 18:20 PST, application creation in App Engine, first-time deployments of Cloud Functions, and project creation & API auto-enablement in Cloud Console experienced elevated error rates in all regions due to a recently deployed configuration update to the underlying control plane for all impacted services.\r\n\r\nFirst-time deployments of new Cloud Functions failed. Redeploying existing deployments of Cloud Functions were not impacted. Workloads on already deployed Cloud Functions were not impacted.\r\n\r\nApp Engine app creation experienced an error rate of 98%. Workloads for deployed App Engine applications were not impacted.\r\n\r\nCloud API enable requests experienced a 97% average error rate while disable requests had a 71% average error rate. Affected users observed these errors when attempting to enable an API via the Cloud Console and API Console. \r\n\r\n\r\nROOT CAUSE\r\n\r\nThe control plane responsible for managing new app creations in App Engine, new function deployments in Cloud Functions, project creation & API management in Cloud Console utilizes a metadata store. This metadata store is responsible for persisting and processing new project creations, function deployments, App Engine applications, and API enablements.\r\n\r\nGoogle engineers began rolling out a new feature designed to improve the fault-tolerance of the metadata store. The rollout had been successful in test environments, but triggered an issue in production due to an unexpected difference in configuration, which triggered a bug. The bug caused writes to the metadata store to fail.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted of the elevated error rate within 3 minutes of the incident start and immediately began their investigation. \r\n\r\nAt 15:17, an issue with our metadata store was identified as the root cause, and mitigation work began. An initial mitigation was applied, but automation intentionally slowed the rollout of this mitigation to minimize risks to production. To reduce time to resolution, Google engineers developed and implemented a new mitigation. The metadata store became fully available at 18:20.\r\n\r\nTo prevent a recurrence, we will implement additional validation to the metadata store\u2019s schemas and ensure that test validation of metadata store configuration matches production.\r\n\r\nTo improve time to resolution for such issues, we are increasing the robustness of our emergency rollback procedures for the metadata store, and creating engineering runbooks for such scenarios.", "when": "2019-01-08T01:25:07Z"}, "number": 19001, "public": true, "service_key": "appengine", "service_name": "Google App Engine", "severity": "high", "updates": [{"created": "2019-01-08T01:25:07Z", "modified": "2019-01-08T01:25:07Z", "text": "ISSUE SUMMARY\r\n\r\nOn Wednesday 2 January, 2019, application creation in Google App Engine (App Engine), first-time deployment of Google Cloud Functions (Cloud Functions) per region, and project creation & API management in Cloud Console experienced elevated error rates ranging from 71% to 100% for a duration of 3 hours, 40 minutes starting at 14:40 PST. Workloads already running on App Engine and Cloud Functions, including deployment of new versions of applications and functions, as well as ongoing use of existing projects and activated APIs, were not impacted.\r\n\r\nWe know that many customers depend on the ability to create new Cloud projects, applications & functions, and apologize for our failure to provide this to you during this time. The root cause of the incident is fully understood and engineering efforts are underway to ensure the issue is not at risk of recurrence.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 2 January, 2019 from 14:40 PST to 18:20 PST, application creation in App Engine, first-time deployments of Cloud Functions, and project creation & API auto-enablement in Cloud Console experienced elevated error rates in all regions due to a recently deployed configuration update to the underlying control plane for all impacted services.\r\n\r\nFirst-time deployments of new Cloud Functions failed. Redeploying existing deployments of Cloud Functions were not impacted. Workloads on already deployed Cloud Functions were not impacted.\r\n\r\nApp Engine app creation experienced an error rate of 98%. Workloads for deployed App Engine applications were not impacted.\r\n\r\nCloud API enable requests experienced a 97% average error rate while disable requests had a 71% average error rate. Affected users observed these errors when attempting to enable an API via the Cloud Console and API Console. \r\n\r\n\r\nROOT CAUSE\r\n\r\nThe control plane responsible for managing new app creations in App Engine, new function deployments in Cloud Functions, project creation & API management in Cloud Console utilizes a metadata store. This metadata store is responsible for persisting and processing new project creations, function deployments, App Engine applications, and API enablements.\r\n\r\nGoogle engineers began rolling out a new feature designed to improve the fault-tolerance of the metadata store. The rollout had been successful in test environments, but triggered an issue in production due to an unexpected difference in configuration, which triggered a bug. The bug caused writes to the metadata store to fail.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted of the elevated error rate within 3 minutes of the incident start and immediately began their investigation. \r\n\r\nAt 15:17, an issue with our metadata store was identified as the root cause, and mitigation work began. An initial mitigation was applied, but automation intentionally slowed the rollout of this mitigation to minimize risks to production. To reduce time to resolution, Google engineers developed and implemented a new mitigation. The metadata store became fully available at 18:20.\r\n\r\nTo prevent a recurrence, we will implement additional validation to the metadata store\u2019s schemas and ensure that test validation of metadata store configuration matches production.\r\n\r\nTo improve time to resolution for such issues, we are increasing the robustness of our emergency rollback procedures for the metadata store, and creating engineering runbooks for such scenarios.", "when": "2019-01-08T01:25:07Z"}, {"created": "2019-01-03T02:38:37Z", "modified": "2019-01-03T02:38:37Z", "text": "The issue regarding Google App Engine application creation, GCP Project Creation, Cloud Function deployments, and GenerateUploadUrl calls has been resolved for all affected users as of Wednesday, 2019-01-02 18:35 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence.", "when": "2019-01-03T02:38:37Z"}, {"created": "2019-01-03T02:16:19Z", "modified": "2019-01-03T02:16:19Z", "text": "Mitigation work is currently underway by our Engineering Team. As the work progresses, the error rates observed from various affected systems continue to drop. We will provide another status update by Wednesday, 2019-01-02 19:15 US/Pacific with current details.", "when": "2019-01-03T02:16:19Z"}, {"created": "2019-01-03T01:13:07Z", "modified": "2019-01-03T01:13:07Z", "text": "Error rates have started to subside. Mitigation work is continuing by our Engineering Team. We will provide another status update by Wednesday, 2019-01-02 18:15 US/Pacific with current details.", "when": "2019-01-03T01:13:07Z"}, {"created": "2019-01-03T00:31:13Z", "modified": "2019-01-03T00:31:13Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Wednesday, 2019-01-02 17:15 US/Pacific with current details.", "when": "2019-01-03T00:31:13Z"}, {"created": "2019-01-03T00:04:48Z", "modified": "2019-01-03T00:04:48Z", "text": "We are currently investigating an issue with Google App Engine app creation, Cloud Function deployments, and Project Creation in Cloud Console as of Wednesday, 2019-01-02 15:07 US/Pacific. \r\n\r\nAffected customers may see elevated errors when creating Google App Engine applications in all regions.\r\n\r\nAffected Customers may also experience an issue with GCP Project Creation, Cloud Function deployments, and GenerateUploadUrl calls may also fail. Existing App Engine applications are unaffected. \r\n\r\nWe have identified the root cause and are working towards a mitigation. We will provide another status update by Wednesday, 2019-01-02 16:45 US/Pacific with current details.", "when": "2019-01-03T00:04:48Z"}, {"created": "2019-01-02T23:38:18Z", "modified": "2019-01-02T23:38:18Z", "text": "We are investigating elevated errors when creating Google App Engine applications in all regions as of Wednesday, 2019-01-02 15:07 US/Pacific. Existing App Engine applications are unaffected. We have identified the root cause and are working towards a mitigation. We will provide another status update by Wednesday, 2019-01-02 16:30 US/Pacific with current details.", "when": "2019-01-02T23:38:18Z"}, {"created": "2019-01-02T23:38:16Z", "modified": "2019-01-02T23:38:16Z", "text": "Google App Engine application creation is experiencing elevated error rates in all regions.", "when": "2019-01-02T23:38:16Z"}], "uri": "/incident/appengine/19001"}, {"begin": "2018-12-29T10:00:07Z", "created": "2018-12-29T14:29:08Z", "end": "2018-12-29T16:00:25Z", "external_desc": "We are investigating an issue with Google Cloud Storage.", "modified": "2018-12-29T17:03:00Z", "most-recent-update": {"created": "2018-12-29T16:58:26Z", "modified": "2018-12-29T16:58:26Z", "text": "The Google Cloud Storage issue is believed to be affecting less than 0.1% of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2018-12-29T16:58:26Z"}, "number": 18007, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2018-12-29T16:58:26Z", "modified": "2018-12-29T16:58:26Z", "text": "The Google Cloud Storage issue is believed to be affecting less than 0.1% of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2018-12-29T16:58:26Z"}, {"created": "2018-12-29T16:01:21Z", "modified": "2018-12-29T16:01:21Z", "text": "Customers do not need to make any changes at this time. We will provide another status update by Saturday, 2018-12-29 09:00 US/Pacific with current details.", "when": "2018-12-29T16:01:21Z"}, {"created": "2018-12-29T15:29:53Z", "modified": "2018-12-29T15:29:53Z", "text": "Mitigation work is currently underway by our Engineering Team. We will provide another status update by Saturday, 2018-12-29 08:00 US/Pacific US/Pacific with current details.", "when": "2018-12-29T15:29:53Z"}, {"created": "2018-12-29T14:59:05Z", "modified": "2018-12-29T14:59:05Z", "text": "Our Engineering Team believes they have identified the potential root cause of the errors and is working to mitigate. We will provide another status update by Saturday, 2018-12-29 07:30 US/Pacific with current details.", "when": "2018-12-29T14:59:05Z"}, {"created": "2018-12-29T14:29:10Z", "modified": "2018-12-29T14:29:10Z", "text": "We are experiencing an issue with Google Cloud Storage manifesting as increased error rate in europe-west-1 beginning at Saturday, 2018-12-29 02:00 US/Pacific. Current data indicates that approximately 0.1% of requests in region are affected by this issue. For everyone who is affected, we apologize for the disruption. We will provide an update by Saturday, 2018-12-29 07:00 US/Pacific with current details.", "when": "2018-12-29T14:29:10Z"}, {"created": "2018-12-29T14:29:08Z", "modified": "2018-12-29T14:29:08Z", "text": "We are investigating an issue with Google Cloud Storage.", "when": "2018-12-29T14:29:08Z"}], "uri": "/incident/storage/18007"}, {"begin": "2018-12-21T16:01:40Z", "created": "2018-12-21T16:51:40Z", "end": "2018-12-21T19:43:51Z", "external_desc": "We are currently investigating an issue with Google Cloud Storage and App Engine.  Google Cloud Build and Cloud Functions services are restored", "modified": "2018-12-28T17:53:42Z", "most-recent-update": {"created": "2018-12-28T17:53:42Z", "modified": "2018-12-28T17:53:42Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 21 December 2018, customers deploying App Engine apps, deploying in Cloud Functions, reading from Google Cloud Storage (GCS), or using Cloud Build experienced increased latency and elevated error rates ranging from 1.6% to 18% for a period of 3 hours, 41 minutes.  \r\n\r\nWe understand that these services are critical to our customers and sincerely apologize for the disruption caused by this incident; this is not the level of quality and reliability that we strive to offer you. We have several engineering efforts now under way to prevent a recurrence of this sort of problem; they are described in detail below.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 21 December 2018, from 08:01 to 11:43 PST, Google Cloud Storage reads, App Engine deployments, Cloud Functions deployments, and Cloud Build experienced a disruption due to increased latency and 5xx errors while reading from Google Cloud Storage. The peak error rate for GCS reads was 1.6% in US multi-region. Writes were not impacted, as the impacted metadata store is not utilized on writes.\r\n\r\nElevated deployment errors for App Engine Apps in all regions averaged 8% during the incident period. In Cloud Build, a 14% INTERNAL_ERROR rate and 18% TIMEOUT error rate occurred at peak. The aggregated average deployment failure rate of 4.6% for Cloud Functions occurred in us-central1, us-east1, europe-west1, and asia-northeast1.\r\n\r\n\r\nROOT CAUSE\r\n\r\nImpact began when increased load on one of GCS's metadata stores resulted in request queuing, which in turn created an uneven distribution of service load. \r\n\r\nThe additional load was created by a partially-deployed new feature. A routine maintenance operation in combination with this new feature resulted in an unexpected increase in the load on the metadata store. This increase in load affected read workloads due to increased request latency to the metadata store.  \r\n\r\nIn some cases, this latency exceeded the timeout threshold, causing an average of 0.6% of requests to fail in the US multi-region for the duration of the incident.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted to the increased error rate at 08:22 PST. Since the issue involved multiple backend systems, multiple teams at Google were involved in the investigation and narrowed down the issue to the newly-deployed feature. The latency and error rate began to subside as Google Engineers initiated the rollback of the new feature. The issue was fully mitigated at 11:43 PST when the rollback finished, at which point the impacted GCP services recovered completely.\r\n\r\nIn addition to updating the impacting feature to prevent this type of increased load, we will update the rollout workflow to stress feature limits before rollout.  To improve time to resolution of issues in the metadata store, we are implementing additional instrumentation to the requests made of the subsystem.", "when": "2018-12-28T17:53:00Z"}, "number": 18005, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2018-12-28T17:53:42Z", "modified": "2018-12-28T17:53:42Z", "text": "ISSUE SUMMARY\r\n\r\nOn Friday 21 December 2018, customers deploying App Engine apps, deploying in Cloud Functions, reading from Google Cloud Storage (GCS), or using Cloud Build experienced increased latency and elevated error rates ranging from 1.6% to 18% for a period of 3 hours, 41 minutes.  \r\n\r\nWe understand that these services are critical to our customers and sincerely apologize for the disruption caused by this incident; this is not the level of quality and reliability that we strive to offer you. We have several engineering efforts now under way to prevent a recurrence of this sort of problem; they are described in detail below.\r\n\r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Friday 21 December 2018, from 08:01 to 11:43 PST, Google Cloud Storage reads, App Engine deployments, Cloud Functions deployments, and Cloud Build experienced a disruption due to increased latency and 5xx errors while reading from Google Cloud Storage. The peak error rate for GCS reads was 1.6% in US multi-region. Writes were not impacted, as the impacted metadata store is not utilized on writes.\r\n\r\nElevated deployment errors for App Engine Apps in all regions averaged 8% during the incident period. In Cloud Build, a 14% INTERNAL_ERROR rate and 18% TIMEOUT error rate occurred at peak. The aggregated average deployment failure rate of 4.6% for Cloud Functions occurred in us-central1, us-east1, europe-west1, and asia-northeast1.\r\n\r\n\r\nROOT CAUSE\r\n\r\nImpact began when increased load on one of GCS's metadata stores resulted in request queuing, which in turn created an uneven distribution of service load. \r\n\r\nThe additional load was created by a partially-deployed new feature. A routine maintenance operation in combination with this new feature resulted in an unexpected increase in the load on the metadata store. This increase in load affected read workloads due to increased request latency to the metadata store.  \r\n\r\nIn some cases, this latency exceeded the timeout threshold, causing an average of 0.6% of requests to fail in the US multi-region for the duration of the incident.\r\n\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nGoogle engineers were automatically alerted to the increased error rate at 08:22 PST. Since the issue involved multiple backend systems, multiple teams at Google were involved in the investigation and narrowed down the issue to the newly-deployed feature. The latency and error rate began to subside as Google Engineers initiated the rollback of the new feature. The issue was fully mitigated at 11:43 PST when the rollback finished, at which point the impacted GCP services recovered completely.\r\n\r\nIn addition to updating the impacting feature to prevent this type of increased load, we will update the rollout workflow to stress feature limits before rollout.  To improve time to resolution of issues in the metadata store, we are implementing additional instrumentation to the requests made of the subsystem.", "when": "2018-12-28T17:53:00Z"}, {"created": "2018-12-21T20:10:51Z", "modified": "2018-12-21T20:10:51Z", "text": "The issue with Google Cloud Storage, App Engine, and Cloud Functions has been resolved for all affected projects as of Friday, 2018-12-21 11:46 US/Pacific. We will conduct an internal investigation of this issue and make appropriate improvements to our systems to help prevent or minimize future recurrence. We will provide a more detailed analysis of this incident once we have completed our internal investigation.", "when": "2018-12-21T20:10:51Z"}, {"created": "2018-12-21T19:46:46Z", "modified": "2018-12-21T19:46:46Z", "text": "The rollout for the potential fix is continuing its progress.  The Google Cloud Storage error rate has improved and is currently 0.1% for US multi-region.  Google App Engine App deployments and Google Cloud Function deployments remain affected. We will provide another status update by Friday, 2018-12-21 12:30 US/Pacific with current details.", "when": "2018-12-21T19:46:46Z"}, {"created": "2018-12-21T18:53:59Z", "modified": "2018-12-21T18:53:59Z", "text": "We are rolling out a potential fix to mitigate this issue. This currently also impacts Google App Engine App deployments and Google Cloud Function deployments. We will provide another status update by Friday, 2018-12-21 12:00 US/Pacific with current details.", "when": "2018-12-21T18:53:59Z"}, {"created": "2018-12-21T18:30:11Z", "modified": "2018-12-21T18:30:11Z", "text": "A proximate root cause has been identified and mitigation work is currently underway by our Engineering Team. We will provide another status update by Friday, 2018-12-21 12:30 US/Pacific with current details.", "when": "2018-12-21T18:30:11Z"}, {"created": "2018-12-21T17:59:01Z", "modified": "2018-12-21T17:59:01Z", "text": "We are experiencing an issue with Google Cloud Storage service returning elevated error rates for requests in the US multi-region, starting at Friday, 2018-12-21 08:06 US/Pacific. This currently also impacts Google App Engine. The issue for Google Cloud Build and Google Cloud Functions has been resolved as of Friday, 2018-12-21 09:38 US/Pacific. \r\n\r\nMitigation work is currently underway by our engineering team and we expect a full resolution in the near future. \r\n\r\nGoogle Cloud Storage service is reporting a 1% error rate for all requests.\r\nAffected Google Cloud Functions customers may seen their deployments time out. \r\nAffected customers of Google Cloud Build were observable as \"Build failed (internal error)\"\r\n\r\nWe will provide another status update by Friday, 2018-12-21 11:00 US/Pacific with current details.", "when": "2018-12-21T17:59:01Z"}, {"created": "2018-12-21T17:18:56Z", "modified": "2018-12-21T17:18:56Z", "text": "The Google Cloud Storage service issue is correlated to issues in Google App Engine, Google Cloud Build and Google Cloud Functions in US multi-region. We will provide another status update by Friday, 2018-12-21 10:00 US/Pacific with current details.", "when": "2018-12-21T17:18:56Z"}, {"created": "2018-12-21T17:14:19Z", "modified": "2018-12-21T17:14:19Z", "text": "The Google Cloud Storage service issue is correlated to issues in Google Cloud Build and Google Cloud Functions in US Region. We will provide another status update by Friday, 2018-12-21 10:00 US/Pacific with current details.", "when": "2018-12-21T17:14:19Z"}, {"created": "2018-12-21T16:51:42Z", "modified": "2018-12-21T16:51:42Z", "text": "The Google Cloud Storage service is reporting an error rate increase in US region on requests. We will provide another status update by Friday, 2018-12-21 09:15 US/Pacific with current details.", "when": "2018-12-21T16:51:42Z"}, {"created": "2018-12-21T16:51:41Z", "modified": "2018-12-21T16:51:41Z", "text": "We are investigating an issue with Google Cloud Storage.", "when": "2018-12-21T16:51:41Z"}], "uri": "/incident/storage/18005"}, {"begin": "2018-12-19T19:09:37Z", "created": "2018-12-19T19:09:38Z", "end": "2018-12-19T19:43:42Z", "external_desc": "We've received a report of an issue with Google Cloud Storage.", "modified": "2018-12-19T19:43:48Z", "most-recent-update": {"created": "2018-12-19T19:43:48Z", "modified": "2018-12-19T19:43:48Z", "text": "The Google Cloud Storage issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2018-12-19T19:43:48Z"}, "number": 18004, "public": true, "service_key": "storage", "service_name": "Google Cloud Storage", "severity": "medium", "updates": [{"created": "2018-12-19T19:43:48Z", "modified": "2018-12-19T19:43:48Z", "text": "The Google Cloud Storage issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here.", "when": "2018-12-19T19:43:48Z"}, {"created": "2018-12-19T19:09:50Z", "modified": "2018-12-19T19:09:50Z", "text": "The issue with Google Cloud Storage error rates in Asia region should be resolved for the majority of users and we expect a full resolution in the near future. We will provide another status update by Wednesday, 2018-12-19 11:45 US/Pacific with current details.", "when": "2018-12-19T19:09:50Z"}, {"created": "2018-12-19T19:09:38Z", "modified": "2018-12-19T19:09:38Z", "text": "We've received a report of an issue with Google Cloud Storage.", "when": "2018-12-19T19:09:38Z"}], "uri": "/incident/storage/18004"}, {"begin": "2018-12-19T14:00:11Z", "created": "2018-12-19T14:29:17Z", "end": "2018-12-19T14:39:12Z", "external_desc": "We are investigating an issue with Google Cloud Networking in the zone europe-west1-b. We will provide more information by Wednesday, 2018-12-19 07:00 US/Pacific.", "modified": "2018-12-21T22:49:33Z", "most-recent-update": {"created": "2018-12-21T22:49:33Z", "modified": "2018-12-21T22:49:33Z", "text": "ISSUE SUMMARY\r\n\r\nOn Wednesday 19 December 2018 multiple GCP services in europe-west1-b experienced a disruption for a duration of 34 minutes. Several GCP services were impacted: GCE, Monitoring, Cloud Console, GAE Admin API, Task Queues, Cloud Spanner, Cloud SQL, GKE, Cloud Bigtable, and Redis. GCP services in all other zones remained unaffected.\r\n\r\nThis service disruption was caused by an erroneous trigger leading to a switch re-installation during upgrades to two control plane network (CPN) switches impacting a portion of europe-west1-b. Most impacted GCP services in the zone recovered within a few minutes after the issue was mitigated. \r\n\r\nWe understand that these services are critical to our customers and sincerely apologize for the disruption caused by this incident. To prevent the issue from recurring we are fixing our repair workflows to catch such errors before serving traffic. \r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 19 December 2018 from 05:53 to 06:27 US/Pacific, multiple GCP services in europe-west1-b experienced disruption due to a network outage in one of Google\u2019s data centers. \r\n\r\nThe following Google Cloud Services in europe-west1-b were impacted: GCE instance creation, GCE networking, Cloud VPN, Cloud Interconnect,  Stackdriver Monitoring API, Cloud Console, App Engine Admin API, App Engine Task Queues, Cloud Spanner, Cloud SQL, GKE, Cloud Bigtable, and Cloud Memorystore for Redis. Most of these services suffered a brief disruption during the duration of the incident and recovered when the issue was mitigated. \r\n\r\n  - Stackdriver: Around 1% of customers accessing Stackdriver Monitoring API directly received 5xx\r\n    errors.\r\n  - Cloud Console: Affected customers may not have been able to view graphs and API usage\r\n    statistics. Impacted dashboards include: /apis/dashboard, /home/dashboard, /google/maps-api/api \r\n    list.             \r\n  - Redis: After the network outage ended, ~50 standard Redis instances in europe-west1 remained\r\n    unavailable until 07:55 US/Pacific due to a failover bug triggered by the outage. \r\n\r\nROOT CAUSE \r\n\r\nAs part of a program to upgrade network switches in control plane networks across Google\u2019s data center, two control plane network (CPN) switches supporting a single CPN were scheduled to undergo upgrades. On December 17, the first switch was upgraded and was back online the same day. The issue triggered on December 19 when the second switch was due to be upgraded. During the upgrade of the second switch, a reinstallation was erroneously triggered on the first switch, causing it to go offline for a short period of time. Having both switches down partitioned the network supporting a portion of europe-west1-b. Due to this isolation, the zone was left partially functional.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nThe issue was mitigated at 06:27 US/Pacific when reinstallation of the first switch in the CPN completed.  \r\n\r\nTo prevent the issue from recurring we are changing the switch upgrade workflow to prevent erroneous triggers. The trigger inadvertently caused the switch to re-install before any CPN switch is deemed healthy to serve traffic. We are also adding additional checks to make sure upgraded devices are in full functional state before they are deemed healthy to start serving. We will also be improving our automation to catch offline peer devices sooner and help prevent related issues.", "when": "2018-12-21T22:49:33Z"}, "number": 18019, "public": true, "service_key": "cloud-networking", "service_name": "Google Cloud Networking", "severity": "high", "updates": [{"created": "2018-12-21T22:49:33Z", "modified": "2018-12-21T22:49:33Z", "text": "ISSUE SUMMARY\r\n\r\nOn Wednesday 19 December 2018 multiple GCP services in europe-west1-b experienced a disruption for a duration of 34 minutes. Several GCP services were impacted: GCE, Monitoring, Cloud Console, GAE Admin API, Task Queues, Cloud Spanner, Cloud SQL, GKE, Cloud Bigtable, and Redis. GCP services in all other zones remained unaffected.\r\n\r\nThis service disruption was caused by an erroneous trigger leading to a switch re-installation during upgrades to two control plane network (CPN) switches impacting a portion of europe-west1-b. Most impacted GCP services in the zone recovered within a few minutes after the issue was mitigated. \r\n\r\nWe understand that these services are critical to our customers and sincerely apologize for the disruption caused by this incident. To prevent the issue from recurring we are fixing our repair workflows to catch such errors before serving traffic. \r\n\r\nDETAILED DESCRIPTION OF IMPACT\r\n\r\nOn Wednesday 19 December 2018 from 05:53 to 06:27 US/Pacific, multiple GCP services in europe-west1-b experienced disruption due to a network outage in one of Google\u2019s data centers. \r\n\r\nThe following Google Cloud Services in europe-west1-b were impacted: GCE instance creation, GCE networking, Cloud VPN, Cloud Interconnect,  Stackdriver Monitoring API, Cloud Console, App Engine Admin API, App Engine Task Queues, Cloud Spanner, Cloud SQL, GKE, Cloud Bigtable, and Cloud Memorystore for Redis. Most of these services suffered a brief disruption during the duration of the incident and recovered when the issue was mitigated. \r\n\r\n  - Stackdriver: Around 1% of customers accessing Stackdriver Monitoring API directly received 5xx\r\n    errors.\r\n  - Cloud Console: Affected customers may not have been able to view graphs and API usage\r\n    statistics. Impacted dashboards include: /apis/dashboard, /home/dashboard, /google/maps-api/api \r\n    list.             \r\n  - Redis: After the network outage ended, ~50 standard Redis instances in europe-west1 remained\r\n    unavailable until 07:55 US/Pacific due to a failover bug triggered by the outage. \r\n\r\nROOT CAUSE \r\n\r\nAs part of a program to upgrade network switches in control plane networks across Google\u2019s data center, two control plane network (CPN) switches supporting a single CPN were scheduled to undergo upgrades. On December 17, the first switch was upgraded and was back online the same day. The issue triggered on December 19 when the second switch was due to be upgraded. During the upgrade of the second switch, a reinstallation was erroneously triggered on the first switch, causing it to go offline for a short period of time. Having both switches down partitioned the network supporting a portion of europe-west1-b. Due to this isolation, the zone was left partially functional.\r\n\r\nREMEDIATION AND PREVENTION\r\n\r\nThe issue was mitigated at 06:27 US/Pacific when reinstallation of the first switch in the CPN completed.  \r\n\r\nTo prevent the issue from recurring we are changing the switch upgrade workflow to prevent erroneous triggers. The trigger inadvertently caused the switch to re-install before any CPN switch is deemed healthy to serve traffic. We are also adding additional checks to make sure upgraded devices are in full functional state before they are deemed healthy to start serving. We will also be improving our automation to catch offline peer devices sooner and help prevent related issues.", "when": "2018-12-21T22:49:33Z"}, {"created": "2018-12-19T14:39:15Z", "modified": "2018-12-19T14:39:15Z", "text": "The Google Cloud Networking issue in zone europe-west1-b has been resolved. No further updates will be provided here.", "when": "2018-12-19T14:39:15Z"}, {"created": "2018-12-19T14:29:26Z", "modified": "2018-12-19T14:29:26Z", "text": "We are investigating an issue with Google Cloud Networking in the zone europe-west1-b. We will provide more information by Wednesday, 2018-12-19 07:00 US/Pacific.", "when": "2018-12-19T14:29:26Z"}, {"created": "2018-12-19T14:29:18Z", "modified": "2018-12-19T14:29:18Z", "text": "We are investigating an issue with Google Cloud Networking in the zone europe-west1-b. We will provide more information by Wednesday, 2018-12-19 07:00 US/Pacific.", "when": "2018-12-19T14:29:18Z"}], "uri": "/incident/cloud-networking/18019"}, {"begin": "2018-12-19T08:00:00Z", "created": "2019-02-13T19:36:18Z", "end": "2019-01-19T08:00:00Z", "external_desc": "We investigated an issue with Google Cloud Datastore", "modified": "2019-02-14T22:01:16Z", "most-recent-update": {"created": "2019-02-13T19:36:18Z", "modified": "2019-02-14T22:01:16Z", "text": "# ISSUE SUMMARY\r\n\r\nCloud Datastore experienced a low rate of errors associated with a small subset of high write-rate databases. We have separately notified the customers who may have been potentially impacted by these errors.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nBeginning Wednesday December 19, 2018 and ending January 19 2019, Cloud Datastore experienced a low rate of errors associated with a small subset of high write-rate databases. Less than 0.1% of databases may have experienced these errors when called from the us-central1 region. We have separately notified the customers who may have been potentially impacted by these errors.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nWe have identified and remediated the issue. As part of the remediation, we reverted to an earlier rollout ending the event. After verifying the issue was resolved, Google is taking steps to improve our existing testing scenarios to help prevent future recurrence. \r\n", "when": "2019-02-14T22:00:18Z"}, "number": 19001, "public": true, "service_key": "cloud-datastore", "service_name": "Google Cloud Datastore", "severity": "medium", "updates": [{"created": "2019-02-13T19:36:18Z", "modified": "2019-02-14T22:01:16Z", "text": "# ISSUE SUMMARY\r\n\r\nCloud Datastore experienced a low rate of errors associated with a small subset of high write-rate databases. We have separately notified the customers who may have been potentially impacted by these errors.\r\n\r\n# DETAILED DESCRIPTION OF IMPACT\r\n\r\nBeginning Wednesday December 19, 2018 and ending January 19 2019, Cloud Datastore experienced a low rate of errors associated with a small subset of high write-rate databases. Less than 0.1% of databases may have experienced these errors when called from the us-central1 region. We have separately notified the customers who may have been potentially impacted by these errors.\r\n\r\n# REMEDIATION AND PREVENTION\r\n\r\nWe have identified and remediated the issue. As part of the remediation, we reverted to an earlier rollout ending the event. After verifying the issue was resolved, Google is taking steps to improve our existing testing scenarios to help prevent future recurrence. \r\n", "when": "2019-02-14T22:00:18Z"}], "uri": "/incident/cloud-datastore/19001"}]